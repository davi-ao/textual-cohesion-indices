Recent research has shown the ability of convolutional neural networks (CNN) to deal with complex machine vision problems: unprecedented results were achieved in tasks such as classification (Krizhevsky, Sutskever, Hinton, 2012, Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich, 2015), segmentation, and object detection (Sermanet, Eigen, Zhang, Mathieu, Fergus, LeCun, 2013, Szegedy, Toshev, Erhan, 2013), often outperforming human accuracy (He et al., 2015). CNNs have the ability of learning a hierarchical representation of the input data without requiring any effort to design handcrafted features (LeCun et al., 2015). Different layers of the network are capable of different levels of abstraction and capture different amount of structure from the patterns present in the image (Zeiler and Fergus, 2013). Due to the complexity of the tasks and the very large number of network parameters that need to be learned during training, CNNs require a massive amount of annotated training images in order to deliver competitive results. As a consequence, significant performance increase can be achieved as soon as faster hardware and higher amount of training data become available (Krizhevsky et al., 2012).
In this work we investigate the applicability of convolutional neural networks to medical image analysis. Our goal is to perform segmentation of single and multiple anatomic regions in volumetric clinical images from various modalities. To this end, we perform a large study on parameter variations and network architectures, while proposing a novel segmentation framework based on Hough voting and patch-wise back-projection of a multi-atlas. We demonstrate the performance of our approach on brain MRI scans and 3D freehand ultrasound (US) volumes of the deep brain regions (Fig. 1).
The paradigm-shifting results delivered by CNNs in computer vision were in part accomplished with the help of extremely large training datasets and significant computational resources. Both of which may be often unrealistic in clinical environments, due to the absence of large annotated dataset and to data protection policies which often do not allow computation outsourcing. Therefore, in this study, we perform all training and testing of CNN networks on clinically realistic dataset sizes, using a high-performance, but stand-alone PC workstation.
Segmentation of brain structures in US and MRI has widespread clinical relevance, but it is challenging in both modalities.
In MRI, the segmentation of basal ganglia is a relevant task for diagnosis, treatment and clinical research. A concrete application is pre-operative planning of Deep Brain Stimulation (DBS) neurosurgery in which basal ganglia, like the sub-thalamic nucleus (STN) and globus pallidus internal (GPi), are targeted for treatment of symptoms of Parkinson’s disease (PD) and dystonia, respectively (D’Haese et al., 2012). Accurate localisation and outlining of these nuclei can be challenging, even when performed manually, due to their weak contrast in MRI data. Moreover, fully manual labelling of individual MRIs into multiple regions in 3D is extremely time-consuming and therefore prohibitive. For this reason, both in research (D’Haese, Pallavaram, Li, Remple, Kao, Neimat, Konrad, Dawant, 2012, DAlbis, Haegelen, Essert, Fernndez-Vidal, Lalys, Jannin, 2015) and in clinical practice (Barbe et al., 2014), segmentation through atlas-based approaches is widely used.
Transcranial ultrasound (TCUS) can be used to scan deep brain regions non-invasively through the temporal bone window. Using TCUS, hyper-echogenicities of the Substantia Nigra (SN) can be analysed, gaining valuable information to perform differential (Walter et al., 2007) and early (Berg et al., 2011) diagnosis of Parkinson’s Disease (PD). A crucial step towards computer assisted diagnosis of PD is midbrain segmentation (Ahmadi, Baust, Karamalis, Plate, Bötzel, Klein, Navab, 2011, Milletari, Ahmadi, Kroll, Hennersperger, Tombari, Shah, Plate, Boetzel, Navab, 2015). This task is reportedly challenging even for human observers (Plate et al., 2012). In order to penetrate the skull, low frequencies need to be applied resulting in an overall reduction of the resolution and in the presence of large incoherent speckle patterns. Scanning through the bone, moreover, attenuates a large part of the ultrasound energy, leading to overall reduction of the signal-to-noise ratio, as well as low contrast and largely missing contours at anatomic boundaries. Additionally, the higher speed of sound in the bone leads to phase aberration (Ivancevich et al., 2006) and de-focussing of the ultrasound beam which causes further lowering of the image quality. A variety of image TCUS quality, anatomical visibility and 3D ultrasound fan geometry can be seen in Fig. 3. Registration methods, in particular non-linear registration, are very difficult under these conditions. Therefore, atlas-building and atlas-based segmentation methods tend to fail in ultrasound.
In this work we evaluate the performance of our approach using an ultrasound dataset of manually annotated TCUS volumes depicting the midbrain, and an MRI dataset, depicting 26 regions including basal ganglia, annotated in a computer-assisted manner. Our method is fully automatic, registration-free and highly robust towards the presence of artefacts. Through our patch-based voting strategy, our approach can localise and segment structures that are only partially visible or whose appearances are corrupted by artefacts. To the best of our knowledge, this is the first work employing CNNs to perform ultrasound segmentation.
Our work features several contributions:
We propose Hough-CNN, a novel segmentation approach based on a voting strategy similar to Milletari et al. (2015). We show that the method is multi-modal, multi-region, robust and implicitly encoding priors on anatomical shape and appearance. Hough-CNN delivers results comparable or superior to other state-of-the-art approaches while being entirely registration-free. In particular, it outperforms methods based on voxel-wise classification.
We propose and evaluate several different CNN architectures, with varying numbers of layers and convolutional kernels per layer. In this way we acquire insights on how different network architectures cope with the amount of variability present in medical volumes and image modalities.
Each network is trained with different amounts of data in order to evaluate the impact of the number of annotated training examples on the final segmentation result. In particular, we show how complex networks with higher parameter number cope with relatively small training datasets.
We adapted the Caffe framework (Jia et al., 2014) to perform convolutions of volumetric data, preserving its third dimension across the whole network. We compare CNN performance using 3D convolution to the more common 2D convolution, as well as to a recent 2.5D approach (Roth et al., 2014).
In this section we give an overview of existing approaches that employ CNNs to solve problems from both computer vision and medical imaging domain.
In the last few years CNNs became very popular tools among the computer vision community. Classification problems such as image categorisation (Krizhevsky, Sutskever, Hinton, 2012, Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, Rabinovich, 2015), object detection (Girshick et al., 2014) and face recognition (Farfade et al., 2015) as well as regression problems such as human pose estimation (Belagiannis et al., 2015), and depth prediction from RGB data (Eigen et al., 2014) have been addressed using CNNs and unprecedented results have been reported. In order to cope with the challenges present in natural images, such as scale changes, occlusions, deformations different illumination settings and viewpoint changes, these methods needed to be trained on very large annotated datasets and required several weeks to be built even when powerful GPUs were employed. In medical imaging, however, it is difficult to obtain even a fraction of this amount of resources, both in terms of computational means and amount of annotated training data.
Many works applying deep learning to medical problems relayed only on a few dozen of training images (e.g. de Brébisson, Montana, 2015, Ciresan, Giusti, Gambardella, Schmidhuber, 2012, Cirean, Giusti, Gambardella, Schmidhuber, 2013, Havaei, Davy, Warde-Farley, Biard, Courville, Bengio, Pal, Jodoin, Larochelle, 2015, Ngo, Carneiro, 2013, Prasoon, Petersen, Igel, Lauze, Dam, Nielsen, 2013). Most networks were applied to tasks that could be solved by interpreting the images patch-wise in a sliding window fashion. In this case, several thousands of annotated training examples could be obtained from just a few images. Dataset augmentation techniques, such as random patch rotation and mirroring, were also applied if the objects of interest were invariant to these transformations (Ciresan, Giusti, Gambardella, Schmidhuber, 2012, Cirean, Giusti, Gambardella, Schmidhuber, 2013, Havaei, Davy, Warde-Farley, Biard, Courville, Bengio, Pal, Jodoin, Larochelle, 2015, Roth, Lu, Seff, Cherry, Hoffman, Wang, Liu, Turkbey, Summers, 2014). This is the case for cell nuclei, lymph nodes and tumor regions, but not for anatomic structures with regular size and local context, such as regions of the brain or abdomen. Another way to deal with little training data is to embed CNNs as core components into previously successful methods from the community. A deep variational model is proposed in Ranftl and Pock (2014). Their CNN is embedded into a global inference model, i.e. the CNN outputs are treated as unary potentials on a graph and the segmentation is solved via minimum s-t cuts on the predicted graph. In Turaga et al. (2010) the CNN performs 3D regression to predict an affinity graph, which can be solved via graph partitioning techniques or connected components in order to segment neuron boundaries. Active shape models are realised with CNNs in Liang et al. (2015) via regression of multi-template contributions and object location. Variational Deep Learning was realised in Ngo and Carneiro (2013) by combining shape-regularised levelset methods with Deep Belief Networks (DBN) for left ventricle segmentation in cardiac MRI.
In this work, we propose a novel Hough-CNN detection and segmentation approach. Our method utilises CNNs at its core to efficiently process medical volumes in a patch-wise fashion. It obtains voxel-wise classifications along with high level features – used to retrieve votes – that are descriptive of the object of interest. Generalised Hough voting has been proposed in the past to address problems related with object detection (Leibe et al., 2004) and tracking (Godec et al., 2013). A notable extension to generalized Hough voting was extended by introducing implicit shape models (ISM) (Leibe, Leonardis, Schiele, 2004, Leibe, Leonardis, Schiele, 2008), which combined recognition and segmentation into a probabilistic framework. The concept of additional object segmentation in ISM by back-projection of codebook patches can be considered as a precursor for our proposed work. The ISM recognition was further refined with weighted voting for optimized object center localization (Maji and Malik, 2009). Later, principled implicit shape models (PRISM) (Lehmann et al., 2009) demonstrated the ability to perform fast nearest-neighbor searches of test patches to trained codebook patches for object detection at recognition time. More recent works such as Riegler et al. (2013) and Xie et al. (2015) performed Hough voting using a CNN. Their respective aim is to obtain head poses and cell locations in 2D by using the network to perform simultaneous classification and vote regression. In this work we propose a more flexible voting mechanism based on neighborhood relationships in feature space. On the one hand, this allows us to cast a variable amount of votes for each patch, which can be associated with information such as segmentation patches. Additionally, therapeutic indications or diagnostic information can be added or modified at any time without requiring re-training. On the other hand, instead of relying on regression, our method uses votes collected from annotated training images. Thus, it does not experience unpredictable behaviour of the votes when the network is presented with unusual data that produces unexpected feature values and mis-classifications.
Compared to computer vision which performs Deep Learning mostly on 2D images, medical images often deal with volumes acquired through scanners such as MRI or CT. In our literature review, most approaches have continued working in 2D by approaching 3D scans in a slice-by-slice fashion (e.g. de Brébisson, Montana, 2015, Ciresan, Giusti, Gambardella, Schmidhuber, 2012, Cirean, Giusti, Gambardella, Schmidhuber, 2013, Havaei, Davy, Warde-Farley, Biard, Courville, Bengio, Pal, Jodoin, Larochelle, 2015, Kim, Wu, Shen, 2013, Lee, Laine, Klein, 2011, Ngo, Carneiro, 2013, Prasoon, Petersen, Igel, Lauze, Dam, Nielsen, 2013, Song, Zhang, Chen, Ni, Lei, Wang, 2015). The advantage is high speed, low memory consumption and the ability to utilise pre-trained nets such as AlexNet (Krizhevsky et al., 2012), either directly or via transfer learning. The obvious disadvantage is that anatomic context in the directions orthogonal to the image plane are entirely discarded. Some groups who employed 3D convolutions found that computational tractability was an issue, and classification was either impossible (Roth et al., 2014) or suffered in accuracy since compromises on patch-size had to be made (Prasoon et al., 2013). Other groups have applied 3D convolution successfully for Alzheimer’s disease detection from whole-MRI (Payan and Montana, 2015) or regression of affinity graphs from 3D convolution (Turaga et al., 2010). A different approach that was applied to full-brain segmentation from MRI in de Brébisson and Montana (2015) combined small 3D patches with larger 2.5D ones that include more context. The 2.5D patches, in particular, consisted of a stack of three 2D patches extracted respectively from the sagittal, coronal and transversal planes. All patches were assembled into eight parallel CNN pathways in order to achieve high-quality segmentation of 134 brain regions from whole brain MRI. In Milletari et al. (2016) a fully convolutional model (FCNN) making use of both short and long skip connections and residual learning was employed to perform prostate segmentation in MRI. A novel loss function based on Dice coefficient and particularly tailored to solve segmentation problems was also proposed.
In this work, we evaluate the performance of our network when 2D, 2.5D and 3D patches are employed. In particular, we supply rather long-range 3D patches which retain a large amount of anatomical context.
Another important issue in CNN-related research is the search for optimal CNN network architecture: we have found very little literature that addresses this issue systematically. Although several networks architectures were analysed in Ciresan et al. (2012) and Cirean et al. (2013), we have found only one study on “very deep CNN” (Simonyan and Zisserman, 2014), in which the number of convolutional layers was varied systematically (8–16) while keeping kernel sizes fixed. The study concluded that small kernel sizes in combination with deep architectures can outperform CNNs with few layers and large kernel sizes.
In this work we propose and benchmark six network architectures, including one very deep network having 8 convolutional layers as shown in Table 1.
We propose six different convolutional neural network architectures trained with patches extracted from annotated medical volumes. We optimise our models to correctly categorise data-points into different classes. The volumes were acquired in two different modalities, US and MRI, and depict deep structures of the human brain. Accurate segmentation of the desired regions has been achieved through a Hough voting strategy, inspired by Milletari et al. (2015), which was employed to simultaneously localise and segment the structures of interest.
A CNN consists of a succession of layers which perform operations on the input data. Convolutional layers (symbol Csk<math><msubsup is="true"><mi is="true">C</mi><mi is="true">s</mi><mi is="true">k</mi></msubsup></math>) convolve the images Isize presented to their inputs with a predefined number (k) of kernels, having a certain size s, and are usually followed by activation units which rescale the results of the convolution in a non linear manner. Pooling layers (symbol Psizestride<math><msubsup is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">s</mi><mi is="true">i</mi><mi is="true">z</mi><mi is="true">e</mi></mrow><mrow is="true"><mi is="true">s</mi><mi is="true">t</mi><mi is="true">r</mi><mi is="true">i</mi><mi is="true">d</mi><mi is="true">e</mi></mrow></msubsup></math>) reduce the dimensionality of the responses produced by the convolutional layers through downsampling, using different strategies such as average-pooling or max-pooling. Finally, fully connected layers (symbol F#neurons<math><msub is="true"><mi is="true">F</mi><mrow is="true"><mo is="true">#</mo><mi is="true">n</mi><mi is="true">e</mi><mi is="true">u</mi><mi is="true">r</mi><mi is="true">o</mi><mi is="true">n</mi><mi is="true">s</mi></mrow></msub></math>) extract compact, high level features from the data. The kernels belonging to convolutional layers as well as the weights of the neural connections of the fully connected layers are optimised during training through back-propagation. The network architecture is specified by the user, by defining the number of layers, their kind, and the type of activation unit. Other relevant parameters are: the number and size of the kernels employed during convolution, the amount of neurons in the fully connected part and the downsampling ratio applied by the pooling layers. We propose six network architectures that are described in Table 1.
CNNs perform machine learning tasks without requiring any handcrafted feature to be engineered and supplied by the user. That is, discovering optimal features describing the data at hand is part of the learning process. During training the network parameters are first initialised and then the data is processed through the layers in a feed-forward manner. The output of the network is compared with the ground-truth through a loss function and the error is back-propagated (LeCun et al., 2015) in order to update the filters and weights of all the layers, up to the inputs. This process is repeated until it converges. Once the network is trained, predictions can be made by using it in a feed-forward manner and reading out the outputs of the last layer.
In our approach we made use of parametric rectified linear units (He et al., 2015) (PReLU) as our activation functions.(1)PReLU(x)={xifx≥0αxifx<0<math><mrow is="true"><mi is="true">P</mi><mi is="true">R</mi><mi is="true">e</mi><mi is="true">L</mi><mi is="true">U</mi><mrow is="true"><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mi is="true">x</mi><mspace width="4.pt" is="true"></mspace><mtext is="true">if</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">x</mi><mo is="true">≥</mo><mn is="true">0</mn></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mi is="true">α</mi><mi is="true">x</mi><mspace width="4.pt" is="true"></mspace><mtext is="true">if</mtext><mspace width="4.pt" is="true"></mspace><mi is="true">x</mi><mo is="true">&lt;</mo><mn is="true">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></math>The parameter α in the PReLU activation function is learnt during training, along with other network weights. In this context we initialise the network parameters using MSRA (He et al., 2015) as it is an appropriate choice when employing PReLU activation units.
Many authors (Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, Krizhevsky, Sutskever, Hinton, 2012) reported that the tendency of the network to overfit can be decreased by using a technique called “drop-out” during training which inhibits the outputs of a random fraction of the neurons of the fully connected layers in each iteration. In this way it is possible to limit their excessive specialisation to specific tasks, which is believed to be at the origin of overfitting in CNNs.
Finally, we employ max-pooling layers to reduce the dimensionality of the data as it traverses the network. The input of the pooling layer is exhaustively subdivided into sub-patches having fixed size and overlapping by an amount controlled by the “stride” parameter. Only the maximal value in each sub-patch is forwarded to the next layer. This procedure is known to incorporate a spatial invariance to the network which contradicts the desired localisation accuracy required for segmentation. For this reason we limit the usage of pooling layers to the minimum amount required to meet the existing hardware constraints.
A set T={p1,…,pN}<math><mrow is="true"><mi mathvariant="bold" is="true">T</mi><mo is="true">=</mo><mo is="true">{</mo><msub is="true"><mi mathvariant="bold" is="true">p</mi><mn is="true">1</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">p</mi><mi is="true">N</mi></msub><mo is="true">}</mo></mrow></math> of square (or cubic) patches having size p pixels is extracted from J annotated volumes Vj with j∈{1…J}<math><mrow is="true"><mspace width="4.pt" is="true"></mspace><mi is="true">j</mi><mo is="true">∈</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mn is="true">1</mn><mo is="true">…</mo><mi is="true">J</mi><mo stretchy="true" is="true">}</mo></mrow></mrow></math> along with the corresponding ground truth labels Y={y1,…,yN}∈R<math><mrow is="true"><mi mathvariant="bold" is="true">Y</mi><mo is="true">=</mo><mo is="true">{</mo><msub is="true"><mi is="true">y</mi><mn is="true">1</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">y</mi><mi is="true">N</mi></msub><mo is="true">}</mo><mo is="true">∈</mo><mi mathvariant="double-struck" is="true">R</mi></mrow></math>. Based on this training set CNNs are optimised to categorise the patches correctly. The resulting trained networks are capable of performing voxel-wise classification, also called semantic segmentation, of volumes by interpreting them in a patch-wise fashion. However, due to the lack of regularisation and enforcement of statistical priors this approach delivers sub-optimal results (Fig. 7). For this reason we introduce a novel segmentation method that is based on simultaneous localisation of the anatomy of interest and robust contour extraction (Fig. 2).
We introduce a robust segmentation approach that is scalable to multiple regions and implicitly encodes shape priors. This method employs a Hough-voting strategy to perform anatomy localisation and a database containing segmentation patches to retrieve the contour of the anatomy. Instead of relying only on categorical predictions produced by the CNNs we also make use of features extracted from their intermediate layers, in particular from the second-last fully connected one. Several authors (Farfade, Saberian, Li, 2015, Girshick, Donahue, Darrell, Malik, 2014, Krizhevsky, Sutskever, Hinton, 2012) have reported that these features (sometimes also called descriptors) can be used for tasks such as image retrieval by mapping images to the feature space and identifying their neighbours. These findings are employed at the core of our voting strategy.
To keep our notation as simple and understandable as possible we describe our approach for single region segmentation in the following.
During training, we make use of the dataset of training volumes Vj with j∈{1…J},<math><mrow is="true"><mspace width="4.pt" is="true"></mspace><mi is="true">j</mi><mo is="true">∈</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mn is="true">1</mn><mo is="true">…</mo><mi is="true">J</mi><mo stretchy="true" is="true">}</mo></mrow><mo is="true">,</mo></mrow></math> and respective binary segmentation volumes Sj with j∈{1…J}<math><mrow is="true"><mspace width="4.pt" is="true"></mspace><mi is="true">j</mi><mo is="true">∈</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mn is="true">1</mn><mo is="true">…</mo><mi is="true">J</mi><mo stretchy="true" is="true">}</mo></mrow></mrow></math>. We collect patches from both foreground and background and train a CNN. As a result, we obtain the parameters θ^<math><mover accent="true" is="true"><mi is="true">θ</mi><mo is="true">^</mo></mover></math> that define the network. The CNN not only differentiates patches belonging to foreground and background through classification, but also associates each input to a feature vector obtained from its second-last fully connected layer. The macroscopic effect of the network can be summarised using two functionsf1(pi,θ^)=li∈{0,1}andf2(pi,θ^)=fi∈Rd<math><mrow is="true"><msub is="true"><mi is="true">f</mi><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">p</mi><mi is="true">i</mi></msub><mo is="true">,</mo><mover accent="true" is="true"><mi is="true">θ</mi><mo is="true">^</mo></mover><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mrow is="true"><mo is="true">{</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">}</mo></mrow><mspace width="4.pt" is="true"></mspace><mtext is="true">and</mtext><mspace width="4.pt" is="true"></mspace><msub is="true"><mi mathvariant="bold" is="true">f</mi><mn is="true">2</mn></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">p</mi><mi is="true">i</mi></msub><mo is="true">,</mo><mover accent="true" is="true"><mi is="true">θ</mi><mo is="true">^</mo></mover><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mi mathvariant="bold" is="true">f</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mi is="true">d</mi></msup></mrow></math>respectively mapping each input patch pi to its label li and to the feature fi, which has as many dimensions d as there are neurons in the fully connected layer it is collected from.
We exhaustively collect a dataset T={p1…pN}<math><mrow is="true"><mi mathvariant="bold" is="true">T</mi><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mi mathvariant="bold" is="true">p</mi><mn is="true">1</mn></msub><mo is="true">…</mo><msub is="true"><mi mathvariant="bold" is="true">p</mi><mi is="true">N</mi></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math> of either 2D, 2.5D or 3D patches from the locations X={x1…xN}<math><mrow is="true"><mi mathvariant="bold" is="true">X</mi><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mn is="true">1</mn></msub><mo is="true">…</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">N</mi></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math> of the foreground region of each of the training volumes Vj, and we use the CNN to obtain the features fi introduced before. Our goal is to create a database storing triples consisting of a feature vector fi, a vote vi and a segmentation patch si.
The vote vi is a displacement vector joining the voxel xi, where the ith patch was collected from, and the position anatomy centroid cj in the training volume Vj:vi=xi−cj;cj=1|Fg|∑xi∈Fgxi<math><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">v</mi><mi is="true">i</mi></msub><mo is="true">=</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">−</mo><msub is="true"><mi mathvariant="bold" is="true">c</mi><mi is="true">j</mi></msub><mo is="true">;</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi mathvariant="bold" is="true">c</mi><mi is="true">j</mi></msub><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mi is="true">F</mi><mi is="true">g</mi></msub><mrow is="true"><mo is="true">|</mo></mrow></mrow></mfrac><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">F</mi><mi is="true">g</mi></msub></mrow></munder><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub></mrow></math>where Fg is the set of all the voxels belonging to foreground. The binary segmentation patches assume values 1 or 0 respectively for foreground and background area since they are collected from the positions xi of the binary annotation volumes Sj.
During testing, in order to segment a previously unseen volume I, we make use of both the trained CNN and the database established before. We first obtain the classification label for each voxel xi by processing the relative patch pi through the CNN, which delivers also the features fi for all the patches being classified as foreground. Each of such features is compared to those contained in the database in order to retrieve the K closest entries using Euclidean distance as criterion. This K-nearest neighbour search (K-nn) (Muja and Lowe, 2014) is performed computing Euclidean distances d1…Ki<math><msubsup is="true"><mi is="true">d</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">K</mi></mrow><mi is="true">i</mi></msubsup></math> between features, as previously done in Krizhevsky et al. (2012) for image retrieval.
Once the neighbours are identified, their votes v1…Ki<math><msubsup is="true"><mi mathvariant="bold" is="true">v</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">K</mi></mrow><mi is="true">i</mi></msubsup></math> and associated segmentation patches s1…Ki<math><msubsup is="true"><mi mathvariant="bold" is="true">s</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">K</mi></mrow><mi is="true">i</mi></msubsup></math> from the database, are employed to respectively perform localisation and segmentation. The votes are weighted by the reciprocal of the Euclidean distance computed during K-nn search w1…K=1d1…Ki<math><mrow is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">K</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><msubsup is="true"><mi is="true">d</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">K</mi></mrow><mi is="true">i</mi></msubsup></mfrac></mrow></math> and contribute to a vote-map at positionsv^ki=xi+vki;∀k∈{1…K}<math><mrow is="true"><msubsup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">v</mi><mo is="true">^</mo></mover><mi is="true">k</mi><mi is="true">i</mi></msubsup><mo is="true">=</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">+</mo><msubsup is="true"><mi mathvariant="bold" is="true">v</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup><mo is="true">;</mo><mspace width="4.pt" is="true"></mspace><mo is="true">∀</mo><mi is="true">k</mi><mo is="true">∈</mo><mrow is="true"><mo is="true">{</mo><mn is="true">1</mn><mo is="true">…</mo><mi is="true">K</mi><mo is="true">}</mo></mrow></mrow></math>We repeat the steps described above for each of the patches that were classified as foreground (Fig. 2b). Since the region of interest occurs only once in each volume, we smooth the final vote map and retrieve the region centroid by finding the location c where the maximal value of the vote map is reached (Fig. 2c). Smoothing reduces the possibility of small localisation mistakes due to “noise” in the vote map around the position where its maximum occurs.
The region of interest can now be segmented by re-projecting the votes vki<math><msubsup is="true"><mi mathvariant="bold" is="true">v</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup></math> to the locations xi where they have been originated from. However, not all the votes should be re-projected, since a relevant portion of them is erroneous, i.e. did not contribute to the vote-map anywhere close to the estimated anatomy location. Thus, only those that contributed to the vote-map within a certain range r from the predicted centroid are taken into consideration and are actually allowed to contribute to the final segmentation contour with their own segmentation patch ski<math><msubsup is="true"><mi mathvariant="bold" is="true">s</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup></math>. The segmentation patches ski<math><msubsup is="true"><mi mathvariant="bold" is="true">s</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup></math> are centred at the location xi, weighted by wki<math><msubsup is="true"><mi is="true">w</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup></math> and accumulated in the segmentation map S (Fig. 2d). Assuming that the segmentation patches ski<math><msubsup is="true"><mi mathvariant="bold" is="true">s</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup></math> have been extended to an infinite spatial extent by zero-padding, we can write:S^(x)=∑xi∑k=1KInd(v^ki,c^)wkiski(x−xi)<math><mrow is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">S</mi><mo is="true">^</mo></mover><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">x</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><munder is="true"><mo is="true">∑</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub></munder><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">K</mi></munderover><mi is="true">I</mi><mi is="true">n</mi><mi is="true">d</mi><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mover accent="true" is="true"><mi mathvariant="bold" is="true">v</mi><mo is="true">^</mo></mover><mrow is="true"><mi is="true">k</mi></mrow><mi is="true">i</mi></msubsup><mo is="true">,</mo><mover accent="true" is="true"><mi mathvariant="bold" is="true">c</mi><mo is="true">^</mo></mover><mo is="true">)</mo></mrow><mspace width="4.pt" is="true"></mspace><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">k</mi></mrow><mi is="true">i</mi></msubsup><mspace width="4.pt" is="true"></mspace><msubsup is="true"><mi is="true">s</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">x</mi><mo is="true">−</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow></mrow></math>Ind(a,b)={1∥a−b∥<r0∥a−b∥≥r<math><mrow is="true"><mi is="true">I</mi><mi is="true">n</mi><mi is="true">d</mi><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">a</mi><mo is="true">,</mo><mi mathvariant="bold" is="true">b</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mn is="true">1</mn></mtd><mtd columnalign="left" is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">∥</mo><mi mathvariant="bold" is="true">a</mi><mo is="true">−</mo><mi mathvariant="bold" is="true">b</mi><mo stretchy="true" is="true">∥</mo></mrow><mo is="true">&lt;</mo><mi is="true">r</mi></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mn is="true">0</mn></mtd><mtd columnalign="left" is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">∥</mo><mi mathvariant="bold" is="true">a</mi><mo is="true">−</mo><mi mathvariant="bold" is="true">b</mi><mo stretchy="true" is="true">∥</mo></mrow><mo is="true">≥</mo><mi is="true">r</mi></mrow></mtd></mtr></mtable></mrow></mrow></math>In this sense, the segmentation patches ski<math><msubsup is="true"><mi mathvariant="bold" is="true">s</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup></math> can be seen as basis functions ski(x),<math><mrow is="true"><msubsup is="true"><mi is="true">s</mi><mi is="true">k</mi><mi is="true">i</mi></msubsup><mrow is="true"><mo is="true">(</mo><mi mathvariant="bold" is="true">x</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math> which take binary values, that need to be scaled and re-centered at appropriate locations in order to produce the desired effect in the segmentation map. Once the segmentation map S is normalised to take only values comprised between 0 and 1, it is thresholded and the final contour is obtained.
The approach is summarised schematically in Fig. 2. Extending this method to multiple regions requires little effort. In our implementation, we treated each region independently by creating region-specific databases as well as dedicated vote-maps and segmentations. The memory requirements of this approach can be decreased by retrieving the segmentation patches directly from the volumes S1…J<math><msub is="true"><mi mathvariant="bold" is="true">S</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">J</mi></mrow></msub></math> instead of storing them in the database. In this case, the database contains coordinates that are used to fetch contour portions from the S1…J<math><msub is="true"><mi mathvariant="bold" is="true">S</mi><mrow is="true"><mn is="true">1</mn><mo is="true">…</mo><mi is="true">J</mi></mrow></msub></math>.
When dealing with images or volumes, patches are extracted in a sliding-window fashion and processed through a CNN. This approach is inefficient due to the high amount of redundant computations that need to be performed for neighbouring patches. In case no padding is used within the convolutional layers, the whole volume can be convolved with the respective kernels in one pass, instead of treating each patch separately, while achieving the same result. The same holds true for pooling layers whose pooling windows can be arranged to process the whole volume at once. However, as soon as fully connected layers are employed, the volume can no longer be processed in one pass due to the fact that the connections of this layer are limited to the size of the input patch.
To solve this issue we modify the network structure as proposed by Sermanet et al. (2013b) in order to be able to process the whole volume at once, yet retrieving the same results that we would obtain if the data would be processed patch-wise.
In this section we show that CNNs not only can be used to robustly segment medical volumes (Fig. 3, Fig. 4), but they also posses the ability of learning extremely effective features (outputs of upper layers) from the data. Even in ultrasound, where the structures of interest are often not clearly visible or the images are affected by artefacts, CNNs are able to focus on salient information and therefore recognise patterns. We demonstrate the superior performances of our Hough-voting-based segmentation algorithm by evaluating our method on two datasets of US and MRI volumes depicting the human brain. The two modalities provide complementary information, but are inherently different both from the point of view of the challenges they offer and the range of anatomy they can image.
Our MRI dataset is composed of MRI volumes of 55 subjects, which were acquired using 3D gradient-echo imaging (magnitude and phase) with an isotropic spatial resolution of 1x1x1 mm. The sequence (Dietrich et al., 2015) is designed for quantitative susceptibility mapping (QSM) and sensitivity towards iron deposits. These are biomarkers for movement disorders like Parkinson’s Disease and create visible contrast in relevant basal ganglia like SN and STN. For our study, basal ganglia and other deep-brain structures were annotated in an atlas volume in two ways. One set of bi-lateral atlas labels (brainstem, n. accumbens, amygdala, caudate, thalamus, hippocampus, pallidum, putamen) were annotated semi-automatically via a shape- and appearance-model segmentation (FSL FIRST (Patenaude et al., 2011)) plus manual correction of generated labels (one neuroimage technician, verified by one expert neurologist). Another set of bi-lateral labels (separation of pallidus into GPi and GPe, midbrain, red nucleus, substantia nigra pars compacta and substantia nigra pars reticulata) was annotated in a fully manual manner (neuroimage technician, verified by expert neurologist) based on visible contrast. The atlas labels were transferred using a state-of-the-art atlas approach (Avants et al., 2010). As a summary, the list of structures of interest is also visible in Fig. 6.
The US dataset was acquired transcranially on 34 subjects, with several freehand 3D sweeps recorded through the left and right temporal bone window each. Altogether, 162 volumes were acquired with slight variations in bone window positioning, and reconstructed at 1–mm isotropic resolution. For all 162 TCUS volumes, midbrain outlines were annotated in 3D by a single human expert. Inter-rater agreement of the midbrain annotations, in terms of Dice coefficient, has been reported in Plate et al. (2012) to be 0.85. CNN training was performed on data from 8 subjects (40 sweeps), and testing on data from 24 previously unseen subjects (114 sweeps), while validation data was performed on 8 sweeps from 2 subjects. Performing segmentation on more than 100 test volumes is a good indicator of actual clinical applicability of (Hough-)CNN-based segmentation. The experiments show that the method generalises very well on previously unseen data, which is a highly desirable property in clinical settings.
In order to test our approach and to benchmark the capabilities of the proposed CNNs when they are trained with a variable amount of data, we establish, for each dimensionality (2D, 2.5D and 3D) two differently sized training sets in US and three in MRI respectively. For each of the 40 training volumes in US we collect either 2K or 10K patches per volume such that half of the training set depicts the background and the other half the foreground. The resulting training sets have respective sizes of 80K and 400K patches. A validation set containing 5K patches has been established for US using images of subjects that have not been used for training or testing and employed to assess the generalisation capabilities of the models. From the 45 MRI training volumes, we extract either circa 100, 1K or 10K patches per volume per region (including background). The resulting training sets have respective sizes of 135K, 1.35M and 13.5M patches.
We analyse six different network architectures, presented in Table 1, by training each of them for 15 epochs using Stochastic Gradient Descent (SGD) with mini-batches of 64 or 124 samples, learning rate varying between 10−2<math><msup is="true"><mn is="true">10</mn><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msup></math> and 5·10−3<math><mrow is="true"><mn is="true">5</mn><mo is="true">·</mo><msup is="true"><mn is="true">10</mn><mrow is="true"><mo is="true">−</mo><mn is="true">3</mn></mrow></msup></mrow></math> depending on the individual network architecture, momentum 0.9 and weight decay 5·10−4<math><mrow is="true"><mn is="true">5</mn><mo is="true">·</mo><msup is="true"><mn is="true">10</mn><mrow is="true"><mo is="true">−</mo><mn is="true">4</mn></mrow></msup></mrow></math>. All our models converged after a few epochs, and often before the seventh epoch.
Each network is analysed three times, with patches capturing the same amount of context from the neighbourhood, but having different dimensionality. That is, our networks process 2D data, 2.5D data and 3D data in order to investigate how the networks respond to the higher amount of information carried by patches in 2.5D and 3D patches compared to 2D. During training, we randomly sample patches from annotated volumes and we feed them to the networks along with their ground truth labels. The patches of the 2D dataset are all square and have a size of 31 × 31 pixels; the 2.5D dataset is composed of patches having the same size and three channels consisting of 2D patches from the sagittal, coronal and transversal plane centred at the same location; the 3D dataset contains cubic patches having size 31 × 31 × 31 voxels.
Some of the parameters supplied to our Hough-CNN algorithm are empirically chosen. Parameters names and respective values are reported in Table 2. These parameters remained constant throughout all experiments, both in ultrasound and MRI. All the trainings were performed on Intel i7 quad-core workstations with 32GB of ram and graphic cards from Nvidia, specifically “Tesla k40” or “Titan X” (12GB VRAM). All tests were made on a similar workstation equipped with a Nvidia GTX 980 (4GB VRAM).
We train our CNNs with different amount of data having different dimensionality, as explained in Section 4.1. Each of the six proposed architectures is trained six times (five for 3D) in order to cover all the possible combinations of dimensionalities (2D, 2.5D, 3D patches) and amount of data (training set sizes 80K, 400K). We test each CNN on 114 ultrasound volumes acquired from subjects whose scans have never been used during training or validation.
Table 3 shows the average performance in terms of Dice coefficients, mean distances of the estimated contours to the ground truth annotations and failure rates of the proposed Hough-CNN segmentation approach when different CNNs are employed. Since we segment one region per volume, the failure rate represents the percentage of volumes where the region of interest could not be segmented due to wrong localisation (Dice 0). In Fig. 5 we provide summary of the performances of each network, when various amounts of training data are used and patches of different dimensionality are supplied. Better networks produce Dice histograms whose higher values are occurring far away from the origin.
Visual examples of ultrasound segmentation results are visible in Fig. 3. It is notable that the Hough-CNN segmentation is able to localise and segment the midbrain accurately, regardless of whether the scan was acquired through the left or right bone window. It is also robust to bone window quality and overall visibility of structures, as well as signal-drop regions and blurring.
We train each of our networks nine times (eight for 3D) in order to explore all the possible combination of different data dimensionality and size of the training set as explained in Section 4.1. We test each of the models on 10 volumes, using their respective atlas-based annotations for evaluation. We verified, through visual inspection performed by a technician and an expert neurologist, that the annotation appropriately delineate the regions of interest.
Table 5 reports the average performance in terms of Dice coefficients, mean distances of the estimated contours to the ground truth annotations and failure rates of the proposed Hough-CNN segmentation approach when different CNNs are employed at its core. The failure rate, in particular, refers to the percentage of regions of the whole training set (total number: 26 × 10 regions), that were not segmented correctly by Hough-CNN due to the fact that they could not be correctly localised. The results are clustered by the size of the training set employed to train the model to improve readability and the possibility of making comparisons between CNNs employing data having different dimensionality (2D, 2.5D and 3D). From these results we observe that the best performing architecture is “7-5-3”.
In Fig. 6 we compare the results achieved by the architecture “7-5-3”, on each of the 26 brain region of interest separately, when different data dimensionalities are used. The bar plot shows the results in terms of Dice coefficient, while the dashed line plot conveys the results in terms of average distance of the estimated contour to ground-truth delineation. We observe that Hough-CNN yields better Dice coefficients when bigger regions and high contrast area are segmented. Small and low contrast regions could be correctly localised but they were in general harder to segment.
Visual examples of MRI segmentation results are visible in Fig. 4. It is notable that the Hough-CNN segmentation is able to correctly localise and segment multiple structures, despite large anatomical variability, such as cortical atrophy and enlarged lateral ventricles.
In order to put our results into prospective and compare our approach with other state of the art methods, we compared the results achieved by our Hough-CNN with the results achieved by V-Net (Milletari et al., 2016). V-Net is a fully convolutional (FCNN) approach trained end to end, which makes use of short and long skip connection and residual blocks in its architecture. One of the particularities of V-Net is the use of a loss function based on Dice coefficient which is specifically tailored and has proved useful in binary segmentation tasks. In our comparison we kept all the hyper-parameters of the model fixed to what the authors of Milletari et al. (2016) used and trained the model for 20 thousand iterations, until convergence, on the same training set we employed to train Hough-CNN. When we evaluated the method on our training set we noticed that although the rate of failure (cases with Dice equal 0) was slightly lower, the contours were often leaking into regions that didn’t belong to the midbrain and in some cases their shape was not resembling any of the training shapes. As a result, the performance of V-Net on this dataset was much inferior to the one of Hough-CNN. This can be observed in Fig. 8 and Table 4 where the distribution of dice coefficients across the test set and quantitative results and respectively shown. In particular, the results obtained on the ultrasound dataset by the best and the worst architectures employed in this study have been compared to V-Net and have clearly shown superior performances.
Unfortunately the same study could not be run on the MRI dataset due to the fact that V-Net does not support multiple regions and that other works from recent literature do not provide a readily working implementation or require too much memory to be tested on the hardware that is currently available to us. The limiting factor is in the latter case the memory which is required by the high resolution prediction layer that need to have as many high resolution channels as regions to be segmented.
Training of CNNs requires a large amount of data in order to achieve satisfactory voxel-wise classification results and perform semantic segmentation. However, as described in the introduction, obtaining such large annotated datasets is rarely possible in clinical settings. By using a voting-based strategy, it is possible to localise the anatomy of interest with high precision, even when the rate of mis-classified voxels is very high. Additionally, our Hough-CNN approach implicitly enforces shape priors which facilitate segmentations in images where the anatomy of interest is poorly visible. Furthermore, when using 3D patches, only 1.35M training patches were required to surpass the performance obtained with datasets of 13.5 millions 2D and 2.5D patches. This marks a 90% reduction of required training data. In all three dimensionalities, 2D, 2.5D and 3D, Hough-CNN outperforms voxel-wise segmentation (cf. Fig. 7). Similar to related works (Liang, Liu, Shen, Yang, Liu, Dong, Lin, Yan, 2015, Ngo, Carneiro, 2013, Ranftl, Pock, 2014, Turaga, Murray, Jain, Roth, Helmstaedter, Briggman, Denk, Seung, 2010), we thus demonstrate that it may be beneficial to embed CNNs as powerful classifiers into higher-level methods which encode anatomic shape- and appearance priors.
The experiments performed on MRI highlight important aspects of both our CNNs and the modality itself. Most of the brain regions considered in this study (e.g. midbrain, STN, caudate) can be recognised by a human rater by clearly visible contrasts, while the position and boundaries of difficult regions with less contrast (e.g. GPi, GPe, SNpc, SNpr) can be inferred through anatomical knowledge and neighborhood context. Ultrasound volumes are much more challenging from this point of view. Human midbrain in TCUS can be difficult to discern and human observers can be mislead by artefacts and signal-loss areas having similar shape. The CNNs employed in this study had various architectures and therefore different pattern recognition capabilities. In MRI, where the most part of regions of interest have good contrast while the position of the others can be inferred by the context, the best performing network was “7-5-3”. Although this architecture is the simplest, it delivered best results in all the MRI experiments. In US, which is a challenging modality, the networks that delivered best results were among the most complex. “SmallAlex” and “3-3-3-3-3-3-3-3” are deeper and therefore recognise more complex visual content than “7-5-3”.
While we observed a strong performance advantage when segmenting MRI volumes considering 3D data (Table 5), we observed the opposite effect when segmenting ultrasound as shown in the bottom left of Table 3. In MRI, processing data in 3D brings additional useful information which improves the performance of both automated methods and human raters, who refer simultaneously to sagittal, coronal and axial views when establishing the ground truth. In US, we observed that experts segmenting the ground truth used only the axial plane, since it is the only plane in which the characteristic shape of the midbrain can be recognised. Similarly, CNNs produce best results when they are not supplied with misleading information from sagittal and coronal planes.
Altogether, using Hough-CNNs, we segmented 10 previously unseen MRI volumes achieving very high Dice coefficients for large and high-contrasted regions, while some of the smallest and most challenging regions were almost always localised accurately and segmented with sub-voxel mean surface distance. Additionally, we achieved very robust midbrain segmentation in 3D-TCUS, in a test dataset of more than 20 subjects and 114 volumes, with a large variation of 3D sweep geometry, bone window qualities, midbrain appearance, location and orientation. Given the size and variety of the 3D-TCUS test set, we are confident to say that the method generalises well to unseen patients.
Compared to atlas-segmentation, Hough-CNN is faster (30 s in US, and 3–4 min in MRI on the machine employed for testing) and entirely registration-free. This makes our approach applicable to TCUS data, in which registration-dependent methods like atlas-based segmentation would be extremely difficult, if not impossible, due to largely missing anatomical and structural context. Our approach is flexible since both votes and segmentation patches can be substituted without any need for re-training or augmented to include information from multiple experts. As a future work, we plan to investigate the extendability of the trained CNN classifier to other modalities via transfer learning, e.g. from our QSM sequences to T1 or T2. It is also noteworthy that in this work, we have only used the CNN method for segmentation. However, as other works have demonstrated (Payan and Montana, 2015), the learned data representations in the last layers of the CNN can be directly used for classification or regression of disease parameters. This can be interleaved with segmentation, which goes far beyond the capabilities of purely atlas-based methods.
In this work, we applied CNNs to medical image segmentation, under the constraints of limited training data and computational resources. We performed a large study of several CNN parameters, including architectures, patch dimensionality and training set size, highlighting CNN performance given challenges from different modalities. We proposed Hough-CNN, a patch-wise multi-atlas method which implicitly encodes priors on anatomic shape and context. The method outperformed voxel-wise semantic segmentation of CNNs in all parameter settings, while using less training data and delivering smooth segmentation contours without the need for post-processing. The method is modality-independent and scalable to multiple regions and harnesses the impressive classification power of CNNs and Deep Learning for application in clinical settings.
This study was funded by the Lüneburg Heritage and Deutsche Forschungsgesellschaft (DFG) Grant BO 1895/4-1. We gratefully acknowledge the support of NVIDIA Corporation in donating a “Tesla K40” GPU for this study.