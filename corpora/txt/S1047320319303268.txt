Since the beginning of the 21st century, the Internet has developed rapidly and spread in all fields of human society. Nowadays, the vast majority of people communicate, transmit and acquire information through the network. With the great development of visual content and technology, the number, availability, complexity, diversity and importance of images in various fields have undergone unparalleled changes. Especially after the popularity of smartphones, people can record the events around them anytime and anywhere and upload them to the network. It is more convenient for data transmission and sharing. For example, many users of social software such as Weichat and Weibo not only record things around them with words, but also upload and share images and videos. This leads to the explosive growth of image resources in the network [1], [2]. However, facing the huge amount of image data, how to quickly and accurately acquire the information resources that people really need has become an urgent problem and a hot research topic in the academic circles.
The earliest method of image retrieval is to find similar images by calculating Euclidean distance between features. Due to the rapid growth of the number of network images, it takes a lot of time to calculate Euclidean distance, which greatly limits the retrieval efficiency. In order to improve the retrieval efficiency, such as CNNH (CNN Based Hashing) [3], [4], which combines DNNs with hash algorithm, has been proposed successively [5], [6], [7], [8], [9]. Image retrieval of the same object refers to the search of a specific object in the query image from the established retrieval database to the image containing the object. The definition of image retrieval is to find the image similar to the query image from the database, and then return the similar image to the user. However, image similarity refers to the difference in human understanding of image content, which cannot be reflected from the visual features of the image. Here the user wants to query the specific object or target contained in the image, so the target of retrieval should be to return those images containing the object. For example, assuming that an image containing the “Mona Lisa” is entered, the task of image retrieval for the same object is to find those images containing the “Mona Lisa” characters from the retrieval library. Therefore, in the retrieval results returned, it is necessary to ensure that the images containing the “Mona Lisa” characters should appear in front. This is also the reason why low-level features are gradually replaced by high-level features in image retrieval in recent years. At present, the ability of Deep Neural Networks (DNNs) to obtain high-level features of images has been widely recognized [10], [11], [12], [13]. The same object image retrieval is generally called Object Retrieval [14], or Instance Retrieval in English literature, and the same object image retrieval method can be directly applied to similar sample search or detection. Image retrieval of the same object has great value both in research and in the application of commodity image search, such as searching for the same item or similar clothes and bags in Taobao search and Taobao search.
The development of image retrieval technology has gone through three stages. The earliest method of image retrieval is Text-Based Image Retrieval (TBIR) [15], [16], [17]. Text-based image retrieval needs to understand the multi-picture information artificially, annotate the image by text description, and then retrieve the image information by text information retrieval technology. The advantages of text-based image retrieval include simple experiment, fast retrieval speed, easy to understand and easy to operate, etc. [18], [19]. But its shortcomings are also obvious. Firstly, because of the huge growth of digital images, it is an incomplete task to carry out manual standard for each image. Secondly, because pictures contain abundant content information, it is difficult to describe them clearly with several keywords, and the text cannot describe the underlying information of pictures [20], [21], [22]. Finally, because people have a strong subjective understanding of image information, different people may get different information for the same picture. The lack of uniqueness of image annotation will affect image retrieval results to a certain extent. TBIR is the earliest image retrieval technology and the most widely used one. Baidu, Google and other popular search engines mainly use this way to retrieve images.
Deep Learning is one of the most important breakthroughs in the field of artificial intelligence in recent ten years. It has achieved great success in speech recognition, natural language processing, computer vision, image and video analysis, multimedia and many other fields. The biggest difference between deep learning and traditional pattern recognition methods is that it learns features automatically from large data rather than using hand-designed features [23], [24]. The structure of deep learning network is usually separated from the original structure. The performance of an architecture is generally determined by the data set, scope and conditions of use it uses. So it is generally difficult to judge the performance of an architecture [25], [27], [28], [29], [30], [31]. With the rapid development of in-depth learning, new systems, structures and algorithms are emerging. Deep learning includes many hidden layers, which deal with data in a non-linear way. The multi-level features make the model more abstract and expressive. We can choose the appropriate deep learning network architecture according to different purposes of use, so we can divide the deep learning structure into:
Generative depth structure, whose main functions are: 1. Describe high-level correlation characteristics between input samples; 2. Statistical analysis of data correlation. Because in-depth learning does not need to pay attention to data labels, unsupervised learning related to production is often applied. Most of the deep networks can sample the generated samples in the network, so as to build a generative network model. Among all kinds of generative deep learning models, the most common ones are energy-based deep learning models, such as deep Boltzmann machine and deep belief network.
Distinguishing depth structure, which aims to classify different patterns and characterize the posterior distribution of sample categories. The classification technology applied to information processing is shallow discriminatory architecture [26]. In recent years, researchers have output the input data of conditional random domain model in shallow structure to a higher level through each iteration of the bottom layer, and then generated conditional random domain model in deep structure. This model is effectively applied in speech recognition and speech processing.
Hybrid depth structure. The purpose of this structure is to distinguish. In the process of learning, we use the generative structure which is easy to improve by combining the use of distinction and production. When using productive structure to solve classification tasks, the weights are usually optimized by combining discriminatory depth structure in the pre-training stage. An example of the better effect of using this depth structure is the deep belief network.
The main research content of this paper is image retrieval algorithm based on deep neural network learning features. Using convolution network, the error rate is reduced to 14.41% in this test set. The deep learning model is used to represent the trademark image in depth. Then the similarity between the features is measured by the traditional pattern recognition algorithm, which solves the problem of trademark image classification. In three open image libraries, namely Oxford, Holidays and ImageNet, the performance of traditional SIFT-based retrieval algorithms and other CNN-based image retrieval algorithms in tasks are compared and analyzed. The experimental results show that the proposed algorithm can not only effectively utilize the ability of depth convolution neural network to transfer and automatically learn image features, but also combine depth representation with traditional pattern recognition algorithm to further improve the accuracy of trademark image classification.
The learning method of in-depth learning is to construct a non-linear multi-level neural network, whose purpose is to extract deep-seated feature vectors. Deep learning model mainly refers to three or more layers of neural network structure. Compared with traditional feature extraction methods, its special structure can extract the deep features of the target. Therefore, compared with the neural network, deep learning mainly solves the problem of feature learning in solving tasks. Structurally, the difference between deep learning and neural networks is that deep learning networks generally have multiple hidden layers. By learning layer by layer and iterating step by step, the deep features that machine learning and neural networks cannot extract are obtained. For example, in the field of image, it can gradually interpret the original image pixels into the composition of the picture, the scene of the picture and even translate the picture into text. That is to say, deep learning of high-level features, which is gradually abstracted from low to high level like human understanding of the picture, is generally divided into two categories. The first is supervised in-depth learning, that is, training samples are labeled, and the other is unsupervised learning, whose training samples are labeled. Convolutional Neural Network (CNN), as a model for supervised in-depth learning, has achieved good results in image recognition and other tasks related to images for a long time recently, and for unsupervised in-depth learning model. This paper mainly uses convolutional neural network to solve the target problem.
Deep Convolutional Neural Networks (DCNN) is essentially a multi-layer neural network. Convolutional operations make each layer consist of multiple two-dimensional matrices. Similar to traditional neural networks (NN), the upper output of the network acts as the input of the next layer. However, unlike traditional neural networks, the most representative operation of convolution neural networks is to introduce the Convolution calculation in the process of network computing. It does not need to extract features in advance, but directly operates the original image. At present, with the continuous development of convolutional neural network, the number of layers of network from the beginning of five layers, eight layers, to the later 16 layers, 19 layers to 22 layers, to the later 34 layers, 50 layers, 152 layers and even to 1001 layers, the number of layers of convolutional neural network has become deeper and deeper, and the ability of data expression has become more and more powerful.
Input Layer of convolutional neural network can be regarded as the entrance of the whole network model. Data enter the whole convolutional neural network model through the input layer, and then enter the Convolutional Layer. The process of data convolution is equivalent to the process of feature extraction, which is an unsupervised feature learning (UFL) process. In the process of learning, the label information of training data is needed, and features are automatically learned from the data by supervised training, without human intervention. Therefore, this process is also known as the process of automatic learning features. The mathematical definition of convolution is as follows:(1)hx=fx∗gx=∫-∞+∞ftgx-tdt<math><mrow is="true"><mi is="true">h</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi></mrow></mfenced></mrow><mo linebreak="goodbreak" is="true">=</mo><mi is="true">f</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi></mrow></mfenced></mrow><mo is="true">∗</mo><mi is="true">g</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi></mrow></mfenced></mrow><mo linebreak="goodbreak" is="true">=</mo><msubsup is="true"><mo is="true">∫</mo><mrow is="true"><mo is="true">-</mo><mi is="true">∞</mi></mrow><mrow is="true"><mo is="true">+</mo><mi is="true">∞</mi></mrow></msubsup><mrow is="true"><mi is="true">f</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">t</mi></mrow></mfenced></mrow><mi is="true">g</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">x</mi><mo is="true">-</mo><mi is="true">t</mi></mrow></mrow></mfenced></mrow><mi is="true">d</mi><mi is="true">t</mi></mrow></mrow></math>
The functions f (x) and G (x) are convolution variables, t is integral variable, x is the displacement of function g (−t), and the symbol * represents convolution calculation. For example, suppose the size of the input image is 5 × 5, the size of the Convolutional Kernel is 3 × 3, and the Stride is 1. The convolution core scans the image from left to right and from top to bottom. During the scanning process, the convolution core multiplies and adds the elements corresponding to the position of the image one by one. Finally, the feature map with the output size of 3 × 3 is obtained. The convolution process is shown in Fig. 1.
The output of the convolution layer is defined as:
Among them, djl<math><mrow is="true"><msubsup is="true"><mi is="true">d</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup></mrow></math> represents the j-th feature map in layer l, wijl<math><mrow is="true"><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow><mi is="true">l</mi></msubsup></mrow></math> is the connection weight between the i-th feature map in layer L-1 and the j-th feature map in layer l, bjl<math><mrow is="true"><msubsup is="true"><mi is="true">b</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup></mrow></math> is the offset of the j-th feature map calculated, f·<math><mrow is="true"><mi is="true">f</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mo is="true">·</mo></mrow></mfenced></mrow></mrow></math> is the activation function, and the symbol* the convolution operation.
CNN model has achieved great success in many computer vision tasks, especially in image processing. At the same time, we can also use CNN model as a feature extractor, so we can extract features from different network layers for other tasks. Generally speaking, global feature extraction based on CNN mainly includes three processes: pre-training CNN model, fine-tuning CNN model and feature extraction.
Pre-training CNN model: Pre-training CNN model using training data set ImageNet (1.2 million images, 1000 categories) in ILSVRC image classification task. This paper adopts a network architecture consistent with VGG [2], that is, the VGG-net model of D configuration in Table 2–1. VGG model is now the most commonly used CNN model. It uses the same network framework as Alexnet, only through depth replacement width strategy to make each layer deeper. In the D-configuration VGG-net model, there are five groups of convolutions (each group contains a different number of 3 * 3 convolution layers, a ReLU activation layer and a 2 * 2 maxpooling layer), two fully connected layers (FC, fully connected) and one classification layer. It can be seen as a total of eight parts like Alexnet, but each convolution part gets deeper.
Fine tuning CNN model: Usually, the image set used in image retrieval task is quite different from the ImageNet data set, regardless of the number of categories or image content. If the pre-trained CNN model is used directly, the effect of the target task will be affected. In the retrieval task of target image set, the purpose of using pre-trained CNN model is only to make the actual training stage of the model converge better. Therefore, in order to make the CNN model more suitable for target tasks, it is necessary to fine-tune the parameters of the pre-trained CNN model using the target image set. The basic process of fine-tuning is: assuming the number of categories of the target image set is c, since ImageNet is an image data set containing 500 categories, the number of neurons in the soft Max classification layer of CNN model is 500. We need to change it to c, and randomly initialize the parameters of the fully connected network with Gauss distribution. For the hidden layer before the full connection layer, the same parameters are directly used as those after the pre-training. Finally, the learning rate is increased appropriately, the number of iterations is reduced, and the target data set is used to train the network. The pre-training and fine-tuning process of CNN model is included in Fig. 3.
Feature extraction: There are two kinds of network layers in CNN model, convolution layer and pooling layer. The convolution layer convolutes the input information through multiple convolution kernels to extract different image information. The input information is sampled by the pooling layer. Finally, the input features are abstracted nonlinearly by the activation function. The original image enters the CNN model and propagates forward through multi-layer convolution, pooling and non-linear transformation. The image information is also abstracted constantly. The final output features are gradually abstracted from local details to high-level semantic information. Therefore, the closer the CNN model is to the full connection layer, the more high-level semantic information it contains, and the closer the CNN model is to the input convolution layer, the more low-level details it contains. We can extract the full-connection layer information of VGG network as the global semantic information of image. As shown in Fig. 2, some research work has applied the global semantic information of full-connection layer to image retrieval task, and achieved some results.
In classification tasks, CNN model has shown great advantages because it can extract high-level semantic information from images. However, in image retrieval tasks, we often pay more attention to the specific details of the image. Therefore, we need to generate local CNN features to improve the retrieval effect.
The purpose of image enhancement is to improve the visual effect of the image and make the image more suitable for human or machine analysis and processing. Using image enhancement technology to preprocess the image, the noise signal in the image can be weakened, and the contrast between the target and the background is enhanced. At the same time, the details of the image will be clearer. From the perspective of image scope, image intensity can be divided into spatial domain and frequency domain filtering methods. The former refers to calculating and processing the gray value of an image in the pixel domain. The commonly used spatial domain methods can be divided into two categories: linear filtering and non-linear filtering. The advantages of linear filters are simple mathematical expression, good performance, easy design and application. The disadvantages of linear filters are that they cannot effectively protect edges and filter impulse noise. Typical linear filters include low-pass two-dimensional filters, high-pass two-dimensional filters, band-pass two-dimensional filters, unweighted neighborhood average filters, etc. The advantage of non-linear filter is that it can effectively remove Gauss white noise and long tail additional noise, and it can protect the edge information of the object in the image. However, the non-linear filter does not fully consider the characteristics of each pixel in the image, and cannot realize the adaptive filter. The typical non-linear filter has median filter. Morphological filter, polynomial filter and Winner filter. In this paper, the mean filter algorithm is used for image enhancement.
Mean filtering, also known as neighborhood averaging method, is a very simple spatial filtering method. The main purpose of mean filter is to remove the noise signal in the image. The main principle of mean filtering is to replace the gray value of a pixel with the gray value of multiple pixels around it. Among them, there are two ways to select the surrounding pixels, one is to form four neighborhoods in unit distance Δx<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><mi is="true">x</mi></mrow></math>, the other is to form eight neighborhoods in unit distance x<math><mrow is="true"><msqrt is="true"><mrow is="true"><mi is="true">x</mi></mrow></msqrt></mrow></math>, and r as radius. The schematic diagrams of four and eight neighborhoods are shown in Fig. 3 below.
The image smoothed by neighborhood is as follows:(2)ĝi,j=1M∑i,j∈sgi,j=1M∑i,j∈sfi,j+1M∑i,j∈sni,j<math><mrow is="true"><mover accent="true" is="true"><mi is="true">g</mi><mo stretchy="false" is="true">̂</mo></mover><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">M</mi></mfrac><munder is="true"><mo movablelimits="false" is="true">∑</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced><mo is="true">∈</mo><mi is="true">s</mi></mrow></munder><mrow is="true"><mi is="true">g</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced></mrow><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">M</mi></mfrac><munder is="true"><mo movablelimits="false" is="true">∑</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced><mo is="true">∈</mo><mi is="true">s</mi></mrow></munder><mrow is="true"><mi is="true">f</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced></mrow></mrow></mrow><mo linebreak="badbreak" is="true">+</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">M</mi></mfrac><munder is="true"><mo movablelimits="false" is="true">∑</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced><mo is="true">∈</mo><mi is="true">s</mi></mrow></munder><mrow is="true"><mi is="true">n</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced></mrow></mrow></mrow></math>
Among them, S is the neighborhood of (i, j) points and M is the total number of pixels in (i, j) neighborhoods. The first two terms of the formula are the results of mean filtering for noiseless and noisy images respectively. According to statistical analysis, the variance of the second item noise is as follows:(3)D1M∑i,j∈sni,j=1M2∑i,j∈sDni,j=1Mσ2<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mfrac is="true"><mn is="true">1</mn><mi is="true">M</mi></mfrac><munder is="true"><mo movablelimits="false" is="true">∑</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced><mo is="true">∈</mo><mi is="true">s</mi></mrow></munder><mrow is="true"><mi is="true">n</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced></mrow></mrow></mrow></mrow></mfenced></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mn is="true">1</mn><msup is="true"><mi is="true">M</mi><mn is="true">2</mn></msup></mfrac><munder is="true"><mo movablelimits="false" is="true">∑</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced><mo is="true">∈</mo><mi is="true">s</mi></mrow></munder><mrow is="true"><mi is="true">D</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">n</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></mrow></mfenced></mrow></mrow></mrow></mfenced></mrow><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">M</mi></mfrac><msup is="true"><mi is="true">σ</mi><mn is="true">2</mn></msup></mrow></mrow></math>
In the formula, D refers to the calculation of noise variance, σ2<math><mrow is="true"><msup is="true"><mi is="true">σ</mi><mn is="true">2</mn></msup></mrow></math> refers to the variance of the original image noise before the neighborhood smoothing operation. Neighborhood smoothing can greatly reduce the variance of noise in the original image, so it can smooth the image.
This model can be used for many different NLP tasks. Two sentences to be compared are replaced by matrix Q∈Rd×A<math><mrow is="true"><mi is="true">Q</mi><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">d</mi><mo is="true">×</mo><mi is="true">A</mi></mrow></msup></mrow></math> and A∈Rd×A<math><mrow is="true"><mi is="true">A</mi><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">d</mi><mo is="true">×</mo><mi is="true">A</mi></mrow></msup></mrow></math> with word embedding, where Q and A represent two matrices, and d represent the number of vectors in the dictionary. That is to say, a column vector of the matrix represents a word vector in the sentence. Given a sentence pair Q and A, our goal is to predict a label y. In the task of matching two f sentences, y is the similarity between Q and A. Consider the problem as a supervised problem. Mark two similar pictures in the corpus as 1, and train the weight of the network as training data input network structure. There are four modules in the network structure: pretreatment module, Attention module, comparison module and aggregation module.
The preprocessing module uses a RNN to process the transformation matrix of two sentences, where LSTM is chosen and input gates in LSTM are used only to remember input data.(4)Q′=σWlQ+bi⊗eQ·tanhWUQ+bu⊗eQ<math><mrow is="true"><msup is="true"><mi is="true">Q</mi><mo is="true">′</mo></msup><mo linebreak="goodbreak" is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">l</mi></msup><mi is="true">Q</mi><mo is="true">+</mo><msup is="true"><mi is="true">b</mi><mi is="true">i</mi></msup><mo is="true">⊗</mo><msub is="true"><mi is="true">e</mi><mi is="true">Q</mi></msub></mrow></mrow></mfenced></mrow><mo is="true">·</mo><mo is="true">tanh</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">U</mi></msup><mi is="true">Q</mi><mo is="true">+</mo><msup is="true"><mi is="true">b</mi><mi is="true">u</mi></msup><mo is="true">⊗</mo><msub is="true"><mi is="true">e</mi><mi is="true">Q</mi></msub></mrow></mrow></mfenced></mrow></mrow></math>(5)A′=σWlQ+bi⊗eQ·tanhWUQ+bu⊗eQ<math><mrow is="true"><msup is="true"><mi is="true">A</mi><mo is="true">′</mo></msup><mo linebreak="goodbreak" is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">l</mi></msup><mi is="true">Q</mi><mo is="true">+</mo><msup is="true"><mi is="true">b</mi><mi is="true">i</mi></msup><mo is="true">⊗</mo><msub is="true"><mi is="true">e</mi><mi is="true">Q</mi></msub></mrow></mrow></mfenced></mrow><mo is="true">·</mo><mo is="true">tanh</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">U</mi></msup><mi is="true">Q</mi><mo is="true">+</mo><msup is="true"><mi is="true">b</mi><mi is="true">u</mi></msup><mo is="true">⊗</mo><msub is="true"><mi is="true">e</mi><mi is="true">Q</mi></msub></mrow></mrow></mfenced></mrow></mrow></math>
Among them ⊙represents matrix element multiplication, Wi,Wu∈Rl×d,bl,bu<math><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">i</mi></msup><mo is="true">,</mo><msup is="true"><mi is="true">W</mi><mi is="true">u</mi></msup><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">l</mi><mo is="true">×</mo><mi is="true">d</mi></mrow></msup><mo is="true">,</mo><msup is="true"><mi is="true">b</mi><mi is="true">l</mi></msup><mo is="true">,</mo><msup is="true"><mi is="true">b</mi><mi is="true">u</mi></msup></mrow></math> which is the weight that needs to be learned, and ⊗ex<math><mrow is="true"><mo is="true">⊗</mo><msub is="true"><mi is="true">e</mi><mi is="true">x</mi></msub></mrow></math> represents the matrix on the left of the operator repeated X times to generate a matrix or row vector. Attention module is built on the basis of the above Q′<math><mrow is="true"><msup is="true"><mi is="true">Q</mi><mo is="true">′</mo></msup></mrow></math> and A′<math><mrow is="true"><msup is="true"><mi is="true">A</mi><mo is="true">′</mo></msup></mrow></math>.(6)G=softmaxWgQ′+bg⊗eQTA′<math><mrow is="true"><mi is="true">G</mi><mo linebreak="goodbreak" is="true">=</mo><mi is="true">s</mi><mi is="true">o</mi><mi is="true">f</mi><mi is="true">t</mi><mo movablelimits="true" is="true">max</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msup is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">g</mi></msup><msup is="true"><mi is="true">Q</mi><mo is="true">′</mo></msup><mo is="true">+</mo><msup is="true"><mi is="true">b</mi><mi is="true">g</mi></msup><mo is="true">⊗</mo><msub is="true"><mi is="true">e</mi><mi is="true">Q</mi></msub></mrow></mrow></mfenced></mrow><mi is="true">T</mi></msup><msup is="true"><mi is="true">A</mi><mo is="true">′</mo></msup></mrow></mrow></mfenced></mrow></mrow></math>(7)H=Q′G<math><mrow is="true"><mi is="true">H</mi><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mi is="true">Q</mi><mo is="true">′</mo></msup><mi is="true">G</mi></mrow></math>
Among them, Wg∈Rl×1<math><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">g</mi></msup><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">l</mi><mo is="true">×</mo><mn is="true">1</mn></mrow></msup></mrow></math>, bg∈Rl<math><mrow is="true"><msup is="true"><mi is="true">b</mi><mi is="true">g</mi></msup><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mi is="true">l</mi></msup></mrow></math> is the weights that need to be learned, G∈RQ×A<math><mrow is="true"><mi is="true">G</mi><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">Q</mi><mo is="true">×</mo><mi is="true">A</mi></mrow></msup></mrow></math> is the weights of Attention matrix, and H∈Rl×A<math><mrow is="true"><mi is="true">H</mi><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">l</mi><mo is="true">×</mo><mi is="true">A</mi></mrow></msup></mrow></math> is the weights of weight vectors, and the weights of all column vectors listed in Q in J of H matrix.
The function of the contrast module is to match the weight matrix hj<math><mrow is="true"><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></math> of the j word aj<math><mrow is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub></mrow></math> and Q matrix in A. We use aj<math><mrow is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub></mrow></math> and hj<math><mrow is="true"><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></math> to express the comparison method of f, and use tj<math><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">j</mi></msub></mrow></math> to express their comparison results. One of the choices of f is a standard network layer composed of linear transformations and a non-linear activation layer, namely,(8)tj=faj,hj=ReLUWajhj+b<math><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">j</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mi is="true">f</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msub is="true"><mi is="true">a</mi><mrow is="true"><mi is="true">j</mi><mo is="true">,</mo></mrow></msub><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></mrow></mfenced></mrow><mo linebreak="goodbreak" is="true">=</mo><mi is="true">R</mi><mi is="true">e</mi><mi is="true">L</mi><mi is="true">U</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><mi is="true">W</mi><mrow is="true"><mfenced close="]" open="[" is="true"><mrow is="true"><mrow is="true"><mtable is="true"><mtr is="true"><mtd is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></mtd></mtr></mtable></mrow></mrow></mfenced></mrow><mo is="true">+</mo><mi is="true">b</mi></mrow></mrow></mfenced></mrow></mrow></math>
Among them, W∈Rl×2i<math><mrow is="true"><mi is="true">W</mi><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mi is="true">l</mi><mo is="true">×</mo><mn is="true">2</mn><mi is="true">i</mi></mrow></msup></mrow></math>, b∈Rl<math><mrow is="true"><mi is="true">b</mi><mo is="true">∈</mo><msup is="true"><mi is="true">R</mi><mi is="true">l</mi></msup></mrow></math> is the need to learn the weight. However, we have noticed that in many text matching tasks, more attention should be paid to the deep-seated semantic similarity of two sentences, so at the word level, more attention should be paid to the distance between aj<math><mrow is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub></mrow></math> and hj<math><mrow is="true"><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></math>. Therefore, if we choose to add the Euro-cos distance, then f becomes(9)tj=faj,hj=aj-hj2cosaj-hj<math><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">j</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mi is="true">f</mi><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></mrow></mfenced></mrow><mo linebreak="goodbreak" is="true">=</mo><mrow is="true"><mfenced close="]" open="[" is="true"><mrow is="true"><mrow is="true"><mtable is="true"><mtr is="true"><mtd is="true"><msub is="true"><mrow is="true"><mfenced close="∥" open="∥" is="true"><mrow is="true"><mrow is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub><mo is="true">-</mo><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></mrow></mfenced></mrow><mn is="true">2</mn></msub></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mrow is="true"><mo is="true">cos</mo><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mrow is="true"><msub is="true"><mi is="true">a</mi><mi is="true">j</mi></msub><mo is="true">-</mo><msub is="true"><mi is="true">h</mi><mi is="true">j</mi></msub></mrow></mrow></mfenced></mrow></mrow></mrow></mtd></mtr></mtable></mrow></mrow></mfenced></mrow></mrow></math>
After adding the Euclidean-cosine distance, the result vector t becomes a two-dimensional vector. Although Euclidean-cosine is a comparative function with very high evaluation ability, because of the sharp reduction of dimension, some information in the original data disappears.
The deep learning framework used in this experiment is Caffe, the programming interface is python, the system is Ubuntu 16.04; the GPU uses GTX1070, the display memory is 8G; the CPU uses i76700hq, and the memory is 16G. Caffe is a C++/CUDA framework for in-depth learning proposed by Jia Yang-qing. Deep neural network is a modular model, which is composed of a series of internal network layers acting on data blocks. Caffe builds a network from top to bottom by using its own model architecture and layer-by-layer definition. The network starts from the data input layer and connects layer by layer until the final output layer. Caffe stores, transfers and processes all kinds of information in the process of forward and backward propagation in the network by defining the structure of Blobs. In Caffe architecture, there are three basic components. Blob is the main data structure of Caffe, Layer is the basic unit of Caffe model and calculation, and Net is a set of Layers and their connections. Blob defines in detail how data is stored and transmitted in Layers and Net. Solving files need to be configured, including some parameter settings for model training and optimization. Caffe users expose most of the current cutting-edge CNN model frameworks. The network starts from the data input layer and connects layer by layer until the final output layer. Caffe stores, transfers and processes all kinds of information in the process of forward and backward propagation in the network by defining the structure of Blobs. In Caffe architecture, there are three basic components.
Blob is the main data structure of Caffe, Layer is the basic unit of Caffe model and calculation, and Net is a set of Layers and their connections. Blob defines in detail how data is stored and transmitted in Layers and Net. Solving files need to be configured, including some parameter settings for model training and optimization. Caffe users expose most of the current cutting-edge CNN model frameworks, such as the VGG model used in this chapter. Tensor in TensorFlow means an N-dimensional array, which can be seen as modules or layers in the network. Label pair information as supervisory information makes the feature cosine similarity between similar images larger, close to 1. The gradient descent is used to optimize the loss function. The batchsize is set to 120 and the learning rate is set to 0.001. The preparation process of label information during training is shown in Fig. 4 of 20,000 training iterations.
There is the same label flower between im30 and im33, so these two images can be used as similar images, and their label-to-information SIJ = 1 is recorded in the similarity label matrix S = sij. For im33 and im60, because the label information is different, they are considered to be dissimilar images, and their label-to-information SIJ = 0 is recorded in the similarity label matrix S. Because the distance between hash codes between similar images should be as small as possible, in the model, we can use the information in the matrix S of similarity label to calculate the distance between hash codes after completing the hash code learning process for sij = 1 similar images. By updating the network parameters through back propagation, similar images can learn hash codes with smaller Hamming distance.
In feature learning sub-network module, we use V-GG16 model which is fine-tuned by Flickr data set after migration to learn image features. The corresponding comparison method is to use VGG16 model which is pre-trained by ImageNet data set directly as feature learning network. The training data and test data used in the two comparative experiments are the same. CIFAR-10 and Flickr datasets are used to test the deep hash learning method proposed in this paper. The experimental results on CIFAR-10 dataset are shown in Table 1.
As can be seen from the data in Table 1, the experimental results of fine-tuned network model combined with deep hash learning method using retrieved data sets are better than those using pre-trained network model directly. Compared with the comparison method, the accuracy of this method on 6-bit, 12-bit, 24-bit and 48-bit hash codes is improved by 2.58% on average. Through the analysis of the experimental results, when the hash number increases, the corresponding retrieval accuracy is also increasing. Compared with 12-bit hash code, the retrieval accuracy of this method is improved by 5.3% in 48-bit hash code. 1658 images are used as detection data set, and the remaining 232 images are used as training data set. As shown in Fig. 5.
From the figure above, we can see that when T < 0.12, the average precision rate (mAP) increases significantly with the increase of t. When 0.13 < T < 0.3, the average precision rate (mAP) does not increase significantly with the increase of threshold t, but decreases with the increase of threshold T when t > 0.3. From the figure below, we can see that when t = 0.12, the average precision ratio (mAP) is just right at the peak. So in the next experiment to validate our algorithm, we set the threshold T to 0.12.
In order to obtain better retrieval accuracy, a simple technique, extended query, will be used in the last step of image retrieval. In CroW, whitening training is performed in different databases (Paris6k database is used for whitening training when searching Oxford 5 K database, and vice versa). On this basis, Oxford 105 K and Paris 106 K will use Paris6k and Oxford 5 K databases for whitening training respectively. Whitening will improve the final retrieval accuracy in the actual results, which is complementary to accumulative pooling. Sometimes, the accuracy can even be improved by 9.78%. As is shown in Table 2.
The size of the scale grade of the sliding frame also needs to be measured by experiment. Therefore, an experiment to measure the size of scale grade is designed here, which is tested on Paris6k database. In order to reflect the performance of F-CroW and compare it with RoW, the results of 255-dimensional CroW eigenvectors are recalculated and meet the conditions of AlexNet and QE = 5. In the test, the performance of image retrieval results mAP under different slider scales is not only related to the variables produced by different network models, but also to the differences caused by the application of QE = 5, which will be considered and compared.
The application range of approximate target location is confirmed. Because the approximate target localization is based on the F-CroW search result sequence, in the previous experiment, all the images in the database are directly selected for target localization. This method is not close to the practical application, and will result in long response time and waste of memory. Therefore, in the process of target location, the first N images in F-CroW ranking are taken as objects, which need to be confirmed and then converted into the value of N. Similar to other experiments, the QE value is 5, while Oxford 5 k, Oxford 105 K and Paris 106 K are used as data sources, and various adjusted parameters are obtained from previous experiments. The results are shown in Fig. 6.
The sketch of the retrieval results of F-CroW combined with target location is shown in Fig. 7. The accuracy of this location method in Paris6k database @30 is 0.951, @100 is 0.898, @200 is 0.938.
During the experiment, the change of QE has an effect on the result of image retrieval. When retrieval experiments are carried out on different image databases, if different QE values are used in the experiments, the results will be much higher than those under the condition of uniform QE values. Therefore, based on the above experimental results, different QE values are used to measure the mAP of retrieval results in Oxford 5 k, Oxford 105 K and Paris 6 k. The trend chart of QE is shown in Fig. 7. The results are obvious: when QE = 30, Paris 6 K and Paris 106 K get the best results; when QE = 5, Oxford 5 K and Oxford 105 K get the best results.
With the advent of the electronic information age in modern society, computer vision and machine learning are paid more and more attention by the state, society and enterprises. New and new deep learning neural networks are combined with image retrieval to promote the development of traditional content-based image retrieval. Moreover, the pursuit of deeper image representation and information processing methods for image retrieval is a hot topic in the current academic circles. In content-based image retrieval, the increasing number of image data on the Internet and the unprecedented complexity of image content aggravate the challenge to traditional algorithms. People need to innovate the traditional model, add new features closer to the human brain to revitalize it, or completely overthrow the new image retrieval model algorithm to meet the requirements of large data. In this paper, aiming at the demands of these practical applications, combined with the new in-depth learning, image features, traditional retrieval model and image retrieval based on in-depth learning are studied. This paper mainly studies image retrieval algorithm based on content and depth learning. Through in-depth analysis and discussion of the existing research work of image retrieval methods, according to the shortcomings of existing methods and the current problems to be solved urgently, the research work of this paper is carried out on this basis. The main research results of this paper are as follows:
Describe the background and current situation of image retrieval technology. The common algorithms of content-based image retrieval task are briefly introduced, and the flow structure of content-based image retrieval task is explained. The related models and design principles of in-depth learning and the related research of image understanding task are also discussed. At the same time, some common manual features in the field of image retrieval are introduced and analyzed.
By analyzing the disadvantage that global CNN features cannot effectively describe local details in image retrieval tasks, a strategy of aggregating low-level CNN feature maps to generate local features is proposed. Using the more and more abstract features of CNN model from low to high, this paper extracts the low-level CNN feature map, uses Caffe and python to simulate at the end of the study, and makes experimental analysis and comparison with some excellent algorithms on the same object retrieval data set.
By referring to the generation process of traditional local feature descriptors, a strategy of generating deep local features by extracting salient regions, describing salient regions and coding is proposed. Some excellent local descriptors are still superior to global CNN features in image retrieval. Compared with the current excellent image retrieval algorithms on the open classification dataset, the simulation verification and comparison of the same category image retrieval are carried out.

In three open image libraries, namely Oxford, Holidays and ImageNet, the performance of traditional SIFT-based retrieval algorithms and other CNN-based image retrieval algorithms in tasks are compared and analyzed.
The deep learning model is used to represent the trademark image in depth.
The proposed algorithm can not only effectively utilize the ability of depth convolution neural network to transfer and automatically learn image features, but also combine depth representation with traditional pattern recognition algorithm to further improve the accuracy of trademark image classification.
There is no conflict of interest.
The authors gratefully acknowledge the National Natural Science Foundation of China (Grant no. 51474072), Zhejiang Provincial Natural Science Foundation (Grant no. LY19F020001), Ningbo Soft Science Projects (Grant no. 2017A10062).