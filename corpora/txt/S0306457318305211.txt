Due to the ever-increasing number of available online texts, many machine learning techniques have been developed to treat this kind of information (Belbachir, Boughanem, 2018, Hsu, Lee, Chang, Sung, 2018, Kim, Kang, 2018, Sicilia, Giudice, Pei, Pechenizkiy, Soda, 2018, Symeonidis, Effrosynidis, Arampatzis, 2018, Xiong, Wang, Zhang, Ma, 2018). Among many statistical methods, network-based approaches have also been proposed to address several natural language processing problems, including writing style analysis (Amancio, 2015c), authorship attribution (Posadas-Durán et al., 2017) and sentiment analysis (Giatsoglou et al., 2017). Several graph-based approaches hinge on the topological information of the obtained networks to perform some type of classification (Amancio, 2015b, Angelova, Weikum, 2006, Cancho, Solé, 2001, Erkan, Radev, 2004, Jin, Srihari, 2007, Liu, Cong, 2013, Yu, Wang, Zhang, Zhang, Liu, 2017).
A well-known representation of texts as complex networks is the co-occurrence model (Amancio, 2015b, Cancho, Solé, 2001, Liu, Cong, 2013, Wachs-Lopes, Rodrigues, 2016). This model represents words as nodes, and edges are established for every pair of adjacent words. Recently, this representation was found to capture mainly syntax features (Amancio, Altmann, Rybski, Oliveira Jr, Costa, 2013, Masucci, Rodgers, 2006), which has been confirmed by numerous works using co-occurrence networks to study language styles (Amancio, 2015a, de Arruda, Costa, Amancio, 2016b, Cong, Liu, 2014, Mehri, Darooneh, Shariati, 2012, Segarra, Eisen, Ribeiro, 2015). In order to grasp features that go beyond syntax, other models have been proposed. In de Arruda, Costa, and Amancio (2016a), the authors still consider words as nodes, but the connections are created using a larger window, rather than only consecutive words. Upon applying community detection methods, this approach was successfully employed to detect topics. In addition to representing words as networks, larger scales have also been considered to construct text networks de Arruda, Silva, Marinho, Amancio, and Costa (2018b).
In this work, we propose a novel paragraph-based network. Edges are created based on the similarity between texts, which is created with basis on similarities by employing tf-idf (term frequency-inverse document frequency) weighting (Manning & Schütze, 1999) together with cosine similarity. Differently from previous approaches (Salton, Singhal, Mitra, & Buckley, 1997), the paragraph-based networks considered here are analyzed concerning their topological and dynamical properties. The properties of the adopted network representation were probed by considering two different criteria. To test the informativeness of the networks, we investigated whether paragraph-based networks are able to discriminate real from shuffled texts. In the second test, we analyzed if the networks can capture syntax and, most importantly, to reflect the content of the texts. Our results showed that the modularity played an important role in distinguishing real and shuffled texts since the presence of communities turned out to be a characteristic inherent of real texts. We also found that particular measurements are able to capture content features of texts, a characteristic that has not been observed in most co-occurrence networks modeling texts (Amancio et al., 2013), since co-occurrence networks capture mostly syntactical (stylistic) text features.
In addition to the analysis aimed at better understanding the statistical properties of paragraph-based networks, we probed the statistical properties of an unknown text – the Voynich manuscript – using the framework based on the network representation proposed here. The Voynich manuscript is an undeciphered text with uncertain origins (Reddy & Knight, 2011). It is written with a set of unknown characters and, consequently, the approached subject is unknown. Researchers have been studying the characteristics of this mysterious text, which include textual analysis (Reddy & Knight, 2011) and also network science-related tools (Amancio, Altmann, Rybski, Oliveira Jr, Costa, 2013, Montemurro, Zanette, 2013). In order to deal with this text, it is necessary to convert the Voynich characters into an intelligible representation. In this study, we employed the European Voynich Alphabet (Znadbergen, 2018), in which the characters were manually translated into European characters.
In contrast with other approaches, we did not assume that pages are organized in any specific order. It is an important feature because a recent study revealed that the traditionally assumed pages ordering might be unreliable (Reddy & Knight, 2011). Observe that here we employed a method that is unable to capture synonyms (tf-idf) since the proposed network representation is intended to analyze any arbitrary sequence of symbols (e.g., unknown languages). So, it is difficult to use a more sophisticated method because there are no other texts that can be used as a training set (this is the case when analyzing the Voynich manuscript). However, the results obtained for real and artificial texts suggest that the proposed methodology was able to analyze the nature of unknown documents. Interestingly, our results indicate that the Voynich manuscript is compatible with natural languages and incompatible with shuffled texts. These conclusions were mostly corroborated by observing the community structure arising from the manuscript.
The remainder of this paper is organized as follow. Section 2 presents the related studies and Section 3 describes the goals and research questions of this study. In Section 4, we present the employed datasets, the proposed methodology, and the used complex network measurements. Section 5 presents an analysis of the paragraph-network properties by comparing real documents with two versions of shuffled texts. Furthermore, in the same section, we present a case study where we analyze the Voynich manuscript. Finally, in Section 6, we conclude the study with perspectives for further works.
Many efforts have been made to develop approaches devoted to understanding unstructured data using computational tools. Popular research topics in the text mining area include topic modeling (Dai, Olah, & Le, 2015), disambiguation (Henry, Cuffy, & McInnes, 2017), sentiment analysis (Giatsoglou et al., 2017), among others (Amancio, 2015c, Posadas-Durán, Gómez-Adorno, Sidorov, Batyrshin, Pinto, Chanona-Hernández, 2017). Recent advances in language processing and deep learning have enabled analyzing texts with word embeddings (Mikolov, Sutskever, Chen, Corrado, & Dean, 2013b). This methodology consists in creating a distributional representation of words, with some interesting characteristics. For example, if the vectors representing the words “king” and “woman” are summed, the expected result is very close to the vector representing the word “queen”. Another methodology that also has similar characteristics is the method based on document embeddings (Le & Mikolov, 2014), referred to as doc2vec (Le & Mikolov, 2014). In this case, the vector representation aims at representing the subject of the text. This methodology has been employed in the task of topic modeling (Dai et al., 2015). Observe that the training step in embedding-based techniques normally requires extensive datasets to work properly (Dai, Olah, & Le, Mikolov, Chen, Corrado, & Dean).
Another type of text-mining approach is that based on networked/graph representations. Complex networks have been used to model several language problems over the last years (Metzler, Croft, 2004, Zhao, Mao, Lu, 2018). The structure of the co-occurrence networks – formed by linking adjacent words – was found to display several characteristics of other real systems, including the small-world effect, characterized by typical low distances and high-clustering coefficient (Cancho & Solé, 2001). Interestingly, when more accurate links (syntactic links) are considered, the same patterns were observed (Amancio, Nunes, Oliveira Jr, Costa, 2012a, Cancho, Solé, Köhler, 2004). In addition to the small-world effect, word networks were found to exhibit the power-law distribution, which is a consequence of the well-known Zipf’s Law (Newman, 2005). The network structure has also been used to explain some linguistic patterns (Cancho & Solé, 2003).
Word networks have been used to tackle stylistic problems by using structural and dynamical properties of complex networks. The analysis of language complexity was analyzed in Wu, Zhang, and Ren (2017), where the authors found that there is a strong relationship between the complexity of the network structure and textual cognitive complexity. In addition, word networks have also been used to tackle the word sense disambiguation problem. In Agirre and Soroa (2009), the authors constructed graphs from natural language documents and then studied how ambiguous words in these documents can be connected to semantic concepts in the WordNet (Miller, 1995). They found that it is possible to identify the sense being conveyed by a word when the PageRank algorithm is used to identify which is the most central synset in a given context. The same problem was addressed using a structural approach in Silva and Amancio (2012).
It has been shown that word co-occurrence networks are able to model structural features of languages since such representation captures most of the syntactical links (Cancho et al., 2004). This finding explains, in a certain way, the widespread use of word networks in structural natural language processing tasks. More recently, language networks have been studied using a larger textual scale, by modeling texts where nodes are sentences, paragraphs or larger chunks. A network based on the similarity of large chunks was proposed by de Arruda et al. (2018b). This methodology was found to be useful to understand and visualize the unfolding of stories (de Arruda, Marinho, Lima, Amancio, Costa, 2018a, Marinho, de Arruda, Lima, Costa, Amancio, 2017). In the summarization context, another approach that also took into consideration larger chunks of texts is the network of connected paragraphs (Salton et al., 1997). Networked representations of texts have also been used to distinguish real from shuffled texts (de Arruda, Silva, Marinho, Amancio, Costa, 2018b, Margan, Martinčić-Ipšić, Meštrović, 2014a, Margan, Mestrovic, Martincic-Ipsic, 2014b, Masucci, Rodgers, 2009). In these studies, the authors found that some topological patterns are occurring only in real texts. Differently from these previous works, here we focus on the analysis of the statistical properties of paragraph-based networks with application to authenticity verification.
The objective of this study encompasses mainly two research questions. The first regards the use of a network model that is mostly related to the content (i.e., the meaning) of the text. In other words, here we investigate the characteristics of the paragraph-based networks, including its ability to discriminate between real and shuffled texts. Note that this approach contrasts with the traditional networked models that primarily reflects syntactical characteristics (Amancio et al., 2013). The second research question regards the Voynich manuscript, which has an undiscovered subject written in an unknown language. More specifically, here we aim at answering the following question: “Does the Voynich manuscript bear any meaning?”. This issue has drawn the attention of many scholars from various disciplines (Belfield, 2007). More specifically, we compare the characteristics of a network created from Voynich with real and shuffled documents, which were written in different languages. These questions are addressed by representing texts as paragraph-networks, which are then analyzed via structural and dynamical measurements combined with traditional machine learning techniques.
This section describes the employed datasets, the approach devised to create paragraph-based networks and the measurements extracted from the text networks.
We employed two datasets. The first one, henceforth referred to as the Holy Bible dataset, was used to represent the variation of syntax across different languages when the text/content is the same. It comprises three books from the New Testament of the Holy Bible: Matthew, Mark, and Luke. 16 different languages were considered: Arabic, Basque, English, Esperanto, German, Greek, Hebrew, Hungarian, Korean, Latin, Maori, Portuguese, Russian, Swahili, Vietnamese, and Xhosa. The three books were concatenated into a single document to obtain a larger text, as our method is more reliable when larger pieces of texts are used to construct the network. This same procedure has been applied in similar studies (Amancio et al., 2013). For all considered languages, the paragraphs comprise the same verses. In total, 658 paragraphs were manually identified.
The second dataset, henceforth referred to as Books dataset, comprises 53 books in different languages, namely English, French, German, Italian and Portuguese. This dataset was used to analyze how the network structure varies across different documents in the same language. The list of books is presented in Appendix A.
In this work, texts are modeled as complex networks. A network (or graph) can be defined as a set V={v1,v2,…,vn}<math><mrow is="true"><mi is="true">V</mi><mo is="true">=</mo><mo is="true">{</mo><msub is="true"><mi is="true">v</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">n</mi></msub><mo is="true">}</mo></mrow></math> of nodes and a set E={e1,e2,…,em}<math><mrow is="true"><mi is="true">E</mi><mo is="true">=</mo><mo is="true">{</mo><msub is="true"><mi is="true">e</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">e</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">e</mi><mi is="true">m</mi></msub><mo is="true">}</mo></mrow></math> of edges. In an unweighted network, the element aij of the adjacency matrix A is equal to 1 if node i is connected to node j; otherwise, aij=0<math><mrow is="true"><msub is="true"><mi is="true">a</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>. In weighted networks, the element aij corresponds to the weight of the link between nodes i and j.
The main objective of the adopted network model is to represent how short contexts (i.e., paragraphs) relate to each other in a textual document. To create a paragraph-based network the raw text is divided into paragraphs. Each paragraph is considered as a network node, as illustrated in Fig. 1(a)). In order to establish links between paragraphs, each node is considered as a document d in the set D of documents. The tf-idf (term frequency-inverse document frequency) weighting map (Manning & Schütze, 1999) is then computed to quantify the relevance of each word w ∈ d:(1)tf-idf(w,d,D)=fw,dn×log(|D|dw),<math><mrow is="true"><mtext is="true">tf-idf</mtext><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">d</mi><mo is="true">,</mo><mi is="true">D</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">f</mi><mrow is="true"><mi is="true">w</mi><mo is="true">,</mo><mi is="true">d</mi></mrow></msub><mi is="true">n</mi></mfrac><mo is="true">×</mo><mi is="true">log</mi><mo stretchy="true" is="true">(</mo><mfrac is="true"><mrow is="true"><mo is="true">|</mo><mi is="true">D</mi><mo is="true">|</mo></mrow><msub is="true"><mi is="true">d</mi><mi is="true">w</mi></msub></mfrac><mo stretchy="true" is="true">)</mo><mo is="true">,</mo></mrow></math>where fw, d is the frequency of w ∈ d, n is the total number of words in d and dw is the number of documents (paragraphs) in which w appears. Observe that, to compute tf-idf, we consider each paragraph as a single document. Due to the differences of pre-processing of texts written in different languages, we opted for keeping all words of the text, even the stop words (i.e., the words conveying low semantic content). Because stop words tend to be frequent and topic independent, the tf-idf weights computed for such words also tend to be low. For each paragraph, a vector containing the tf-idf weights for the words is created, and the edge weights are computed by using the cosine similarity for all pairs of paragraphs (nodes). Note that this methodology creates a fully connected, weighted graph, as illustrated in Fig. 1(a). Because many of the complex networks measurements are defined only for unweighted networks, we converted the network into their unweighted versions. For that, we removed the weakest edges which have values lower than a threshold T, and the remaining edges were considered as unweighted. For the considered networks, we chose a threshold for each network in order to keep all networks with the same size and density E=5%<math><mrow is="true"><mi is="true">E</mi><mo is="true">=</mo><mn is="true">5</mn><mo is="true">%</mo></mrow></math>. It is an important step in the pre-processing phase because several network measurements are known to be very sensitive to both size and density (Amancio, 2015c, Costa, Rodrigues, Travieso, Villas Boas, 2007). In preliminary experiments, we found that a perturbation in T does not alter the conclusions reported here, as preliminary tests confirmed it. The effect of thresholding the weighted network is illustrated in Fig. 1(b).
The proposed methodology is similar to the mesoscopic networks approach (de Arruda et al., 2018b) regarding the network edge weights. Actually, paragraph networks can be understood as a specific case of mesoscopic networks in which each chunk of text is a single paragraph with no forced overlap between adjacent chunks. An advantage of the present study is that here we can analyze documents in which the order of the pages and paragraphs are unknown. Note that other techniques of word representation, such as word embeddings, were not considered to compute the content similarity because the proposed method was developed to be applied even in texts whose language is unknown.
In order to compare real and shuffled texts, three types of networks were considered. The paragraph-based network – denoted as real texts (RT) – is obtained from the pre-processed texts of the considered datasets, as described in Section 4. The other networks are obtained from shuffled versions of the original text. The versions are created by shuffling words (SW) or sentences (SS). It is important to highlight that both shuffled versions (SW and SS) recover the paragraph sizes of the original text. More specifically, the process of shuffling is applied to the entire text, and the new paragraphs are defined according to the original number of words of the respective text. The versions obtained from an extract of the book The Adventures of Sherlock Holmes, by Arthur Conan Doyle are:
Real Text (RT) version: ”Quite so,” he answered, lighting a cigarette, and throwing himself down into an armchair. ”You see, but you do not observe. The distinction is clear.
Shuffled words (SW) version: ”Quite a into do distinction armchair. but lighting and answered, The observe. himself down you so,” not throwing he see, cigarette, is clear. ”You an
Shuffled Sentences (SS) version: ”You see, but you do not observe. ”Quite so,” he answered, lighting a cigarette, and throwing himself down into an armchair. The distinction is clear.
The following network measurements were used to characterize the paragraph-based networks:
Degree (k): This measurement quantifies the number of immediate neighbors of a node i (Costa et al., 2007) and it is obtained as ki=∑jAij<math><mrow is="true"><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><mo is="true">=</mo><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><msub is="true"><mi is="true">A</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub></mrow></math>.
Betweenness (B):This measurement quantifies the relevance of a node (or edge) in terms of the number of shortest paths including that node (or edge) (Freeman, 1977). The betweenness centrality of a given node i is calculated as(2)Bi=∑s,tgs,tigs,t,<math><mrow is="true"><msub is="true"><mi is="true">B</mi><mi is="true">i</mi></msub><mo is="true">=</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">s</mi><mo is="true">,</mo><mi is="true">t</mi></mrow></munder><mfrac is="true"><msubsup is="true"><mi is="true">g</mi><mrow is="true"><mi is="true">s</mi><mo is="true">,</mo><mi is="true">t</mi></mrow><mi is="true">i</mi></msubsup><msub is="true"><mi is="true">g</mi><mrow is="true"><mi is="true">s</mi><mo is="true">,</mo><mi is="true">t</mi></mrow></msub></mfrac><mo is="true">,</mo></mrow></math>where gs,ti<math><msubsup is="true"><mi is="true">g</mi><mrow is="true"><mi is="true">s</mi><mo is="true">,</mo><mi is="true">t</mi></mrow><mi is="true">i</mi></msubsup></math> is the number of shortest paths connecting nodes s and t that include node i, and gs, t is the number of shortest paths connecting s and t, for all pairs s and t. In text networks, this measurement has been applied to identify if a concept/node is semantically related to one or more topological communities (Amancio, Altmann, Oliveira Jr., & Costa, 2011).
Clustering coefficient (cc): The clustering coefficient represents the probability of two neighbors of a given node being connected with each other (Strogatz, 2001). Locally, the clustering coefficient is calculated as cci=2ei/(ki2−ki)<math><mrow is="true"><mi is="true">c</mi><msub is="true"><mi is="true">c</mi><mi is="true">i</mi></msub><mo is="true">=</mo><mn is="true">2</mn><msub is="true"><mi is="true">e</mi><mi is="true">i</mi></msub><mo is="true">/</mo><mrow is="true"><mo is="true">(</mo><msubsup is="true"><mi is="true">k</mi><mi is="true">i</mi><mn is="true">2</mn></msubsup><mo is="true">−</mo><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow></mrow></math>. In text analysis, the clustering coefficient has also been used to identify if a concept appears in generic or specific contexts. Differently, from the betweenness, only local information is considered.
Neighborhood (N): this measurement quantifies the amount of nodes in the h-th concentric level around node i (Newman, 2010). In this study, we used h=3<math><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>.
Eccentricity (Ecc): the eccentricity of a node i is a centrality index equal to the maximum length of all the shortest paths from i to the other nodes in the network (Harary, 1969).
Eigenvector centrality (EC): the eigenvector centrality assigns a value to a given node i proportional to the sum of the eigenvector centrality values of the nodes connected to i. By doing so, the centrality value of a node increases when it is connected to nodes with high eigenvector centrality (Bonacich, 1987).
Closeness centrality (C): this measurement is given by the inverse of the average distance from a node to the other nodes in the network (Newman, 2010). It is obtained as Ci=li−1=n/∑jdij,<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub><mo is="true">=</mo><msubsup is="true"><mi is="true">l</mi><mi is="true">i</mi><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msubsup><mo is="true">=</mo><mi is="true">n</mi><mo is="true">/</mo><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><msub is="true"><mi is="true">d</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">,</mo></mrow></math> where li is the average distance from node i to all the other nodes, and dij is the length of a geodesic path connecting nodes i and j.
Accessibility (α(h)): This measurement quantifies the number of accessible nodes at the h-th concentric level centered at node i (Travenolo & Costa, 2008) (we used h={2,3}<math><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">}</mo></mrow></math>). This analysis accounts for the accessibility of a node taking into account the probability pi,j(h)<math><msubsup is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup></math> of a random walker to reach a given node j departing from i, in h steps. The equation that describes this measurement is based on the Shannon entropy, as follows(3)αi(h)=exp(−∑pi,j(h)logpi,j(h)).<math><mrow is="true"><msubsup is="true"><mi is="true">α</mi><mi is="true">i</mi><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">=</mo><mi is="true">exp</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mo is="true">−</mo><mo is="true">∑</mo><mrow is="true"><msubsup is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup><mi is="true">log</mi><msubsup is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">.</mo></mrow></math>In language networks, the accessibility (and its variations) has been used as an important feature to identify the relevance of words in the context of structural/stylistic analysis (Amancio, 2015b, Amancio, 2015c).
Generalized Accessibility (α(∞)): The generalized accessibility does not depend on the parameter h. In contrast with the previous measurement, generalized accessibility uses a modified random walk, called accessibility random walk, which assigns higher weights to the shortest paths and penalizes the longest ones (de Arruda et al., 2014). Mathematically, the measurement is defined as(4)αi(∞)=exp(−∑Pi,jlogPi,j),<math><mrow is="true"><msubsup is="true"><mi is="true">α</mi><mi is="true">i</mi><mrow is="true"><mo is="true">(</mo><mi is="true">∞</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">=</mo><mi is="true">exp</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mo is="true">−</mo><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub><mi is="true">log</mi><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>where P is computed as the probability transition of all the pairs of nodes i and j. More details can be found in de Arruda et al. (2014).
Symmetry (S(h)): As another variation of accessibility, this measurement quantifies the symmetry of the topology around a given node i, by considering its neighborhood (h) (Silva et al., 2016b). S(h) is defined in a two-fold manner: (i) the backbone (Sb(h)), in which the connections between nodes in the same hierarchical level (h) are removed and (ii) merged (Sm(h)), where the nodes that are connected and belong to the same hierarchical level (h) are merged into a single node. The measurement is computed as(5)Si(h)=exp(−∑pi,j(h)logpi,j(h))|Hh(i)|+∑r=0h−1ηr,<math><mrow is="true"><msubsup is="true"><mi is="true">S</mi><mi is="true">i</mi><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">exp</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mo is="true">−</mo><mo is="true">∑</mo><mrow is="true"><msubsup is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup><mi is="true">log</mi><msubsup is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">h</mi><mo is="true">)</mo></mrow></msubsup></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mi is="true">H</mi><mi is="true">h</mi></msub><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">)</mo></mrow><mo is="true">|</mo><mo is="true">+</mo></mrow><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">r</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msubsup><msub is="true"><mi is="true">η</mi><mi is="true">r</mi></msub></mrow></mfrac><mo is="true">,</mo></mrow></math>where Hh(i) is the set of all nodes in the h−th<math><mrow is="true"><mi is="true">h</mi><mo is="true">−</mo><mi is="true">t</mi><mi is="true">h</mi></mrow></math> hierarchic level of node i, |Hh(i)| is the number of nodes in Hh(i), and by considering a given hierarchic level r, ηr is the number of nodes without edges connecting to the next hierarchical level. In this study, we employed h={2,3,4}<math><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">,</mo><mn is="true">4</mn><mo is="true">}</mo></mrow></math>. In text networks, the symmetry has been useful to identify the authorship of texts (Amancio, Silva, & Costa, 2015).
Modularity (Q): proposed by Newman and Girvan (2004), the modularity measures the quality of a given network partitioning in terms of its communities. It can be obtained as:(6)Q=12m∑i=1n∑j=1n[aij−kikj2m]δ(ci,cj),<math><mrow is="true"><mi is="true">Q</mi><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mn is="true">2</mn><mi is="true">m</mi></mrow></mfrac><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></munderover><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></munderover><mo stretchy="true" is="true">[</mo><msub is="true"><mi is="true">a</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">−</mo><mfrac is="true"><mrow is="true"><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><msub is="true"><mi is="true">k</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">m</mi></mrow></mfrac><mo stretchy="true" is="true">]</mo><mi is="true">δ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">c</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>where m is the number of edges, n is the number of nodes, δ(ci,cj)=1<math><mrow is="true"><mi is="true">δ</mi><mo is="true">(</mo><msub is="true"><mi is="true">c</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub><mo is="true">)</mo><mo is="true">=</mo><mn is="true">1</mn></mrow></math> if the nodes i and j are from the same class (community) and δ(ci,cj)=0,<math><mrow is="true"><mi is="true">δ</mi><mo is="true">(</mo><msub is="true"><mi is="true">c</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub><mo is="true">)</mo><mo is="true">=</mo><mn is="true">0</mn><mo is="true">,</mo></mrow></math> otherwise. This measurement ranges from −∞≤Q<1<math><mrow is="true"><mo is="true">−</mo><mi is="true">∞</mi><mo is="true">≤</mo><mi is="true">Q</mi><mo is="true">&lt;</mo><mn is="true">1</mn></mrow></math>. For Q > 0, the number of edges inside the communities is greater than the expected in a equivalent random network. In other words, a positive value of modularity is an indicative that the network is organized in communities.
Apart from the modularity, all of the measurements mentioned above are locally defined, i.e., each node has a specific value. To summarize the values obtained for a measurement X across all nodes of the network, we took the average (⟨X⟩) and the standard deviation (σ(X)). Note that this approach has already been adopted in similar works (de Arruda, Costa, Amancio, 2016b, Marinho, de Arruda, Lima, Costa, Amancio, 2017).
An important issue arising from the characterization and classification of networks concerns the comparison of networks with different sizes. Since several network measurements may depend on the total number of nodes, we decide to construct the networks so as the total number of nodes (|V|) is constant. This number corresponds, by construction, to the total number of paragraphs, which was set as |V|=723<math><mrow is="true"><mo is="true">|</mo><mi is="true">V</mi><mo is="true">|</mo><mo is="true">=</mo><mn is="true">723</mn></mrow></math> in order to match the length of the Voynich manuscript. So, for all of the books, we considered only the first 723 paragraphs.
In the adopted network representation, we define as informative the measurements whose values obtained from real books and the respective shuffled versions are significantly different. Measures complying with this condition are therefore able of discriminating between real and random manuscripts. Note that an informative measurement is useful to verify if an unknown manuscript is compatible with a known textual structure (e.g., the structure observed in documents written in natural languages).
Two criteria were used to test the informativeness of the networks:
Criterion A: this criterion is aimed at verifying if the values obtained from the set of all shuffled texts of the dataset can be discriminated from the values obtained for all real texts. Let NRT and NS be the total number of books in the RT dataset and the number of shuffled versions generated for each book in RT, respectively. Here, we perform a comparison of NRT values in RT with NRT · NS values obtained from shuffled texts.
Criterion B: it consists in comparing the value obtained for the real (RT) text with the values obtained in the corresponding shuffled versions of the same text. For a given measurement X, the distance between a real text and the respective shuffled versions is obtained by computing the z-score (i.e. the standard score):(7)z(X)=x−〈X(R)〉σ(X(R)),<math><mrow is="true"><mi is="true">z</mi><mrow is="true"><mo is="true">(</mo><mi is="true">X</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">x</mi><mo is="true">−</mo><mo is="true">〈</mo><msup is="true"><mi is="true">X</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow></msup><mo is="true">〉</mo></mrow><mrow is="true"><mi is="true">σ</mi><mo is="true">(</mo><msup is="true"><mi is="true">X</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow></msup><mo is="true">)</mo></mrow></mfrac><mo is="true">,</mo></mrow></math>where x is the value obtained in the real text, X(R) is the set of values obtained from the NS shuffled versions (SW or SS); and 〈…〉<math><mrow is="true"><mo is="true">〈</mo><mo is="true">…</mo><mo is="true">〉</mo></mrow></math> and σ represent the mean and standard deviation of the distribution, respectively.
In our tests, for each real text, we created NS=30<math><mrow is="true"><msub is="true"><mi is="true">N</mi><mtext is="true">S</mtext></msub><mo is="true">=</mo><mn is="true">30</mn></mrow></math> samples for each SW and SS versions.
An important property to be verified in a text network is the ability of the extracted measurements to capture syntactical and/or content-based features of the represented texts (Amancio et al., 2013). In order to study the dependency of the measurements on syntax and content, the measurements are extracted in two classes of datasets. For a given measurement, Xt=π,l<math><msub is="true"><mi is="true">X</mi><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mi is="true">π</mi><mo is="true">,</mo><mi is="true">l</mi></mrow></msub></math> represents the set of values obtained for X in a dataset comprising the same book t=π<math><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mi is="true">π</mi></mrow></math> in different languages (l). In a similar fashion, Xt,l=λ<math><msub is="true"><mi is="true">X</mi><mrow is="true"><mi is="true">t</mi><mo is="true">,</mo><mi is="true">l</mi><mo is="true">=</mo><mi is="true">λ</mi></mrow></msub></math> represents the set of values obtained for X in a dataset comprising different texts (t) written in the same language l=λ<math><mrow is="true"><mi is="true">l</mi><mo is="true">=</mo><mi is="true">λ</mi></mrow></math>. If a given network measurement depends more on the language (i.e. the syntax) than on the approached subject (i.e. the content), one expects that variability of the distribution of Xt=π,l<math><msub is="true"><mi is="true">X</mi><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mi is="true">π</mi><mo is="true">,</mo><mi is="true">l</mi></mrow></msub></math> will be larger than the variability of Xt,l=λ<math><msub is="true"><mi is="true">X</mi><mrow is="true"><mi is="true">t</mi><mo is="true">,</mo><mi is="true">l</mi><mo is="true">=</mo><mi is="true">λ</mi></mrow></msub></math>. Conversely, if X is more dependent on content, one expects that the variability of Xt,l=λ<math><msub is="true"><mi is="true">X</mi><mrow is="true"><mi is="true">t</mi><mo is="true">,</mo><mi is="true">l</mi><mo is="true">=</mo><mi is="true">λ</mi></mrow></msub></math> will be larger than the variability of Xt=π,l<math><msub is="true"><mi is="true">X</mi><mrow is="true"><mi is="true">t</mi><mo is="true">=</mo><mi is="true">π</mi><mo is="true">,</mo><mi is="true">l</mi></mrow></msub></math> (Amancio et al., 2013). Here, the variability of the distributions is computed by using the coefficient of variation (CV) of the distribution, i.e.(8)CV(X)=σ(X)〈X〉.<math><mrow is="true"><mtext is="true">CV</mtext><mrow is="true"><mo is="true">(</mo><mi is="true">X</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">σ</mi><mo is="true">(</mo><mi is="true">X</mi><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">〈</mo><mi is="true">X</mi><mo is="true">〉</mo></mrow></mfrac><mo is="true">.</mo></mrow></math>
In this section, we analyze the properties of the metrics extracted from the proposed network representation. Here we focus on two main properties: informativeness and the ability of the metrics to capture syntactical and/or content features. The applicability of the adopted representation is then illustrated in the analysis of an unknown text: the Voynich manuscript.
In this study, we used distinct ways to quantify informativeness (Amancio et al., 2013). In the first approach, we consider a measurement X as informative if the value obtained for X in a real (RT) text differs from the values of X obtained in any other shuffled (SW and SS) text of the considered dataset (see Condition A described in the methodology). The results obtained for this type of analysis are shown in Table 1. To facilitate the comparison of measurements taking values in distinct intervals a normalization was applied. For each measurement, and for each of the datasets (Holy Bible and Books), the results are standardized considering all three types of texts (RT, SS, and SW). As such, the average value of each normalized measurement in Table 1 is zero (i.e. RT+SS+SW=0<math><mrow is="true"><mtext is="true">RT</mtext><mo is="true">+</mo><mtext is="true">SS</mtext><mo is="true">+</mo><mtext is="true">SW</mtext><mo is="true">=</mo><mn is="true">0</mn></mrow></math>) and the standard deviation is one.
Considering the Holy Bible dataset, the modularity (Q) was the measurement that best discriminated real from shuffled texts. The modularity in real networks differs 2.24 and 1.73 (standardized values) from the SW and SS versions, respectively. This result suggests that the community structure is much more apparent in real networks, which might be a consequence of the bursty topical textual structure present in real texts (de Arruda et al., 2016a). In addition to the modularity, other measurements were also found to be informative. When comparing RT and SW, the largest differences of values were found for the accessibility (σ(α(3))), symmetry (σ(Sb(3)) and ⟨Sm(4)⟩) and the clustering coefficient (⟨cc⟩). The best discrimination between RT and SS was found for the symmetry (σ(Sb(3)) and ⟨Sm(4)⟩), accessibility (σ(α(3))) and closeness (⟨C⟩). Interestingly, several of the measurements were able to distinguish between real and shuffled texts, regardless of the considered shuffling process.
By considering the Books dataset, the modularity also turned out to be the measurement that best discriminated real from shuffled texts. Once again, real texts frequently displayed a clearer community structure. It means that the informativeness achieved by the modularity is a characteristic that seems to depend neither on syntax nor content features. Apart from Q, the following measurements were also found to discriminate real from both shuffled networks: clustering coefficient (σ(C)), degree (σ(k)) and accessibility (α(∞), and σ(α(2)) and σ(α(3))).
As a complementary test, for each measurement, we used the z-score (see Eq. (7)) to compare a real text and its corresponding shuffled versions (informativeness test based on Condition B). Note that this is a less strict informativeness test because, differently from the previous case, we do not compare a real text with shuffled versions from all texts of the dataset. Here, we rather compare a real text and the shuffled versions generated only from the same book. In Table 2, we show the percentage of documents in which we observed a significant difference between real and shuffled texts – according to the z-score defined in Eq. (7).
As found in the first test, Q is the most critical measurement for both of the considered datasets, reaching 100% of informativeness. Other measurements had similar results for both datasets, eg., σ(k), ⟨Sm(3)⟩, and ⟨cc⟩, which we found to be informative for approximately 50% of the samples. However, for many other measurements, the level of informativeness varied according to the dataset. For example, σ(Sb(3)) and σ(α(3)) were found to be more informative in the Holy Bible dataset. Conversely, σ(C) and ⟨Sm(2)⟩ seemed to be more informative in the Books dataset.
All in all, the results obtained here suggest that, apart from the modularity, it is important to analyze the characteristics of the dataset to decide if network measurements extracted from paragraph networks can be classified as informative – even if a less strict definition of informativeness is taken into account. Interestingly, the results obtained here indicate that paragraph networks are less informative than other types of text networks, as supported by Amancio (2015c). In the case of word adjacency networks, most of the measurements were found to be informative, independently of the characteristics of the considered datasets.
In this section, we evaluate the dependency of the measurements by considering their variability in two distinct scenarios: in datasets where (i) the content (text) is constant and the language (syntax) is varied; and (ii) the language is constant and the content varies. To represent (i), we used the Holy Bible dataset. The dataset employed in the second scenario was created by selecting only the Books in English from the Books dataset. We decided to use the English language because, in the considered dataset, a larger number of books written in this language is available.
In the first analysis, we identified the measurements that were able to capture syntax/language subtleties. The measurements that were found to display significant variability in this scenario (i.e. in the Holy Bible dataset) were: accessibility (⟨α3⟩ and σ(α2)), degree (⟨k⟩), eccentricity (σ(Ecc)), symmetry (⟨Sb2⟩, ⟨Sb3⟩, σ(Sm4) and σ(Sb2)), neighborhood (⟨N⟩) and betweenness (σ(B)).
We also identified the measurements that are sensitive to changes in content. The measurements taking the highest coefficients of variation in the English Books datasets were: eccentricity (⟨Ecc⟩), closeness (⟨C⟩), symmetry (⟨Sm4⟩ and ⟨Sb4⟩), betweenness (⟨B⟩), degree (⟨k⟩), eigenvector centrality (⟨EC⟩), accessibility (⟨α2⟩ and ⟨α∞⟩) and neighborhood (⟨N⟩). Note that some measurements might depend on both syntax and content. This is the case of ⟨N⟩. Interestingly, for both symmetry and accessibility, the ability to capture syntax or content subtleties depends on the hierarchical level being analyzed.
In addition to the tests as mentioned above, we probed, for each measurement, which of the two phenomena is more prevalent: (i) the ability to detect changes in syntax; and (ii) the ability to detect changes in content. This prevalence analysis was conducted by comparing the coefficient of variation in the considered datasets, as described in the methodology. The obtained results are shown in Fig. 2. The top sub-panels illustrate the results obtained for σ(k), Q, σ(C) and σ(α∞). In most of these cases, while the variability across languages (Bible dataset) or topics (English dataset) is high, there is no significant difference between these values. This means that, for these measurements, both syntax and content are captured.
A different behavior can be observed for the measurements depicted in the bottom sub-panels of Fig. 2. For both ⟨cc⟩ and σ(Sb(3)), a significant difference of coefficients of variation was found. This result is an interesting finding in text networks since measurements extracted from other texts networks (such as co-occurrence networks) are mostly dependent on syntax (Amancio et al., 2013). By considering that co-occurrence networks represent distinct information, this result suggests that paragraph-based networks can be used to complement the analysis based on traditional co-occurrence networks when both syntax and content are relevant for the problem being addressed.
To illustrate the applicability of the paragraph-based network in classification tasks, some classification problems were tackled using paragraph-based networks. In the first example, we considered the problem of deciding whether a manuscript has a structure compatible with a shuffled, meaningless document. In the second classification problem, we probed whether an unknown text – the Voynich manuscript – can be considered compatible with real texts.
We applied our method to distinguish real from shuffled texts in order to illustrate the capabilities of paragraph-based networks to characterize texts regarding a real application. For each book presented in Appendix A, the three paragraph-based networks were created, RT, SW, and SS. After that, the network measurements described previously were extracted, the values were standardized, and those values were used as classification features. To select the features for this task, we considered the most informative measurements obtained from Table 1. More specifically, for each pair of real vs. shuffled texts (i.e., RT vs. SS and RT vs. SW) we identified the top 10 measurements that provide the best discrimination. Then, we selected those measurements appearing in both top 10 lists. The measurements selected as feature vectors are: Q, σ(C), σ(α(∞)), σ(k), σ(B), σ(α(3)), σ(α(2)), ⟨Sm(2)⟩, and σ(EC).
The classification was evaluated by the live-one-out cross-validation and the SMO classifier algorithm, which is an SVM implementation available in Weka (Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009, Witten, Frank, Hall, Pal, 2016). The parameters were chosen according to the procedure defined in (Amancio et al., 2014). When considering three classes (RT, SW, and SS), the accuracy was 76.08%. However, the true positive rate was 0.98 for the RT samples and 0.71 and 0.50 for SW and SS, respectively. The false negative rates were 0.02, 0.24, and 0.14 for RT, SW, and SS, respectively. These results mean that the proposed framework can easily differentiate between real and shuffled texts. Conversely, the discrimination between the two classes of shuffled documents represents a more challenging task.
A variation of the same classification problem considered both shuffled versions as having the same class. In this case, SVM reached 98.72% of accuracy. The false positive rate of the RT networks was 0, and the only two classification mistakes were made by real texts classified as shuffled texts. Fig. 3 illustrates the separation between the two classes by considering the projection into a single dimension obtained via linear discriminant analysis (Friedman, Hastie, & Tibshirani, 2001). Given the importance stressed by the modularity in the informativeness analysis, we also evaluated the performance when only this measurement is used for the classification. In this case, the accuracy rate reached 96.79%, which confirms the importance of the modularity in discriminating real and shuffled texts.
We also compared our methodology to a word2vec based approach. Here we considered the version implemented in Gensim library1, which is based on an embedding model (Le & Mikolov, 2014), and negative sampling method (Mikolov et al., 2013b). Also, we adopted the distributed memory version, and we employed the mean of the context word vectors (Dai et al., 2015). The size of the vectors was set as 200 elements, the window size (maximum distance between the words that represent the current and predicted positions in a sentence) was 8. We disregarded words with an occurring less than 19 times (this value was set to provide a similar vocabulary size to the methodology presented by (Dai et al., 2015)). The model number of iterations was set to 10. Due to the characteristics of this type of method, we considered only the English part of our dataset. Because our dataset consists of a small set of books, we pre-trained our word2vec by using the entire English version of Wikipedia2. Besides, in order to increase the number of English documents to this test, we included the following books: The Book of Snobs (by William Makepeace Thackeray), The Unbearable Bassington (by Saki), The Works of Edgar Allan Poe – Volumes 1, 3 and 5 (by Edgar Allan Poe), Through the Magic Door (by Arthur Conan Doyle), and When William Came (by Saki). Observe that these books were not considered in the other tests presented here because they contain a fewer number of paragraphs than Voynich manuscript.
Fig. 4shows a PCA (Principal Component Analysis) (Jolliffe, 2002) projection of the word2vec vectors computed from the three versions of the books (RT, SW, and SS). By considering the three classes and the SVM classifier, with the same characteristics as employed before, we found the accuracy of 70.8%. Interestingly, all samples of SW were correctly classified. Note that this methodology was able to classify texts that do not follow the English grammar (SW) with 100% of the samples being right classified. However, in the case of RT, many texts were classified as SS, which are either without meaning. More specifically, in 57.5% of the RT samples were classified as SS. In order to compare the word2vec based results to the previous cases, we classified only between two classes, Real (RT) and the shuffled versions (SW and SS). For this test, we found the accuracy of 63, 3%, which is a low value when we consider only two classes. Observe that 67.5% of the RT samples were incorrectly classified as a shuffled text (SW + SS group). From these results, we conclude that the employed methodology is not able to distinguish between RT and SS.
The Voynich manuscript is known to be a mysterious text, and many of its aspects have been studied for several years (Reddy & Knight, 2011). Some studies have relied on textual analysis (Reddy & Knight, 2011), while others have used complex networks tools to study its properties (Amancio, Altmann, Rybski, Oliveira Jr, Costa, 2013, Montemurro, Zanette, 2013). In order to handle the manuscript – originally written in an unknown alphabet –, it is necessary to translate its characters into a known set of symbols. Here we used the European Voynich Alphabet (EVA) (Znadbergen, 2018), which provides the original characters manually translated into European characters. To provide a better quality translation, for each line of the text, different translations are available. Here we considered the voting of the most recurring character for all different translations of the same line. Additionally, because our approach relies on text paragraphs, we detected paragraphs by visually inspecting the original manuscript. The paragraphs were identified by a single person, by following the criteria of the distance between chunks of texts. We believe that the possible mistakes are not significative to the final results since the average size of the Voynich paragraphs is compatible with the books dataset statistics, as shown in Fig. 5. When comparing the Voynich manuscript with shuffled texts, we disregarded the SS versions because there is no trivial way to detect sentences in the Voynich manuscript.
First, we analyzed if the Voynich manuscript, when characterized by the metrics extracted from paragraph-based networks, is compatible with real texts and not compatible with gibberish, shuffled texts. It is a long-standing question about the manuscript since several scholars have questioned the existence of a meaningful textual structure in this mysterious text (Belfield, 2007). An illustration comparing the structure of the Voynich manuscript and a shuffled network is shown in Fig. 6. It is clear from the visualizations that the Voynich manuscript presents a well-defined community structure, with two dominant groups. The communities seem to capture the topical organization of the manuscript in some degree: the extract about plants seems to be separated in a specific community. The equivalent shuffled network, shown in Fig. 6(b) reveals no apparent community structure. Since the modularity was found to be informative in the previous analysis, the organization in communities in Fig. 6(a) suggests, at the paragraph level, that the Voynich manuscript is not compatible with shuffled texts. Observe that the same conclusion has been reported when different types of networks are used to represent the manuscript (Amancio, Altmann, Rybski, Oliveira Jr, Costa, 2013, Montemurro, Zanette, 2013, Reddy, Knight, 2011).
As a complementary analysis, taking into account the community structure of the networks, we analyzed the modularity Q, which is much higher for the set of real texts (RT) when compared to the set of shuffled texts (SS and SW), as shown in Fig. 7. The modularity obtained for the Voynich (represented with a blue arrow in the figure) is not compatible with any of the two distributions obtained for shuffled texts. On the other hand, the modularity of the manuscript is compatible with the modularity extracted from real texts.
In order to analyze the Voynich manuscript, we employed the same classifier as in the previous section. As a result, the document was classified as real text. A set of 30 SW networks of Voynich were also classified, and the accuracy of 100% was found. This perfect classification can also be seen in Fig. 7, which shows that the generated SW networks of Voynich (orange arrows) are mostly compatible with the distributions obtained for shuffled texts.
In the current study, we probed the properties of a paragraph-based networked representation of texts. Interestingly, we found that the most informative measurement is the modularity since artificial, shuffled texts, are not organized in well-defined communities. Our results also revealed that several measurements are able to capture content. It is an important feature since the well-known word adjacency (co-occurrence) networks are only able to capture syntax features. Our findings suggest that both co-occurrence and paragraph-based networks can be used in a complementary way when both syntax and content features are important for a natural language processing task.
The adopted network representation was used to analyze the statistical nature of the Voynich manuscript. Previous studies hinging on word networks showed that the Voynich syntax is coherent with natural languages (Amancio, Altmann, Rybski, Oliveira Jr, Costa, 2013, Montemurro, Zanette, 2013). Recently, an extensive analysis using several natural languages argued that Hebrew is the most probable language of the manuscript (Hauer & Kondrak, 2016). Here, we proposed a different analysis, by focusing on the organization in paragraphs. Our analysis revealed that the Voynich manuscript is compatible with natural languages at the paragraph level. This finding was confirmed by analyzing the organization of the text into well-defined communities: similarly to several natural languages, the Voynich manuscript also displays a clear community structure organization. Furthermore, we applied our classification approach, and the Voynich manuscript was classified as real text. As a complement, the accuracy of 100% was found when we classified 30 samples of the shuffled version of the Voynich manuscript.
Because we could identify the features that are more dependent either on language or content, we paved the way to new applications regarding the proposed features. Here, we have found promising results for the task of discriminating between real and shuffled texts. Furthermore, the proposed features were employed to analyze the Voynich manuscript, which was found to be compatible with natural languages at the paragraph level. This result suggests that the attempts to decipher the manuscript are not worthless. Apart from the presented results, many other tasks could employ the same methodology, or use paragraph-based features as a complement to the standard features. Some examples of tasks are authorship and plagiarism identification (Amancio, 2015a), automatic identification of literary movements (Amancio, Oliveira Jr, & da Fontoura Costa, 2012b), fraud detection (Wang & Xu, 2018), among others (Salloum, Al-Emran, Monem, & Shaalan, 2017).
Henrique F. de Arruda Henrique F. de Arruda acknowledges the Coordena£o de Aperfeioamento de Pessoal de Nvel Superior - Brasil (CAPES) - Finance Code 001. Vanessa Q. Marinho thanks FAPESP (grant no. 2015/05676-8) for financial support. Luciano da F. Costa thanks CNPq (grant no. 307333/2013-2) and NAP-PRP-USP for sponsorship. Diego R. Amancio acknowledges FAPESP (grant nos. 16/19069-9 and 17/13464-6) for financial support. This work has been supported also by FAPESP grants 11/50761-2 and 2015/22308-2. The authors acknowledge Filipi Nascimento Silva for fruitful conversations.
The list of books used to analyze how the network structure varies across different documents in the same language is shown below. Five different languages were considered: English, French, German, Italian and Portuguese. The list of books is organized by language. The author of each book is listed between parentheses after each title. The books were obtained from the Project Gutenberg3.
English: The Adventures of Sherlock Holmes (Arthur Conan Doyle) – 2523 paragraphs, The Tragedy of the Korosko (Arthur Conan Doyle) – 871 paragraphs, The Valley of Fear (Arthur Conan Doyle) – 1523 paragraphs, Uncle Bernac - A Memory of the Empire (Arthur Conan Doyle) – 1211 paragraphs, Dracula’s Guest (Bram Stoker) – 739 paragraphs, The Lair of the White Worm (Bram Stoker) – 871 paragraphs, The Jewel Of Seven Stars (Bram Stoker) – 1194 paragraphs, The Man (Bram Stoker) – 1834 paragraphs, The Mystery of the sea (Bram Stoker) – 1625 paragraphs, A Tale of Two Cities (Charles Dickens) – 3268 paragraphs, Barnaby Rudge: A Tale of the Riots of Eighty (Charles Dickens) – 4598 paragraphs, American Notes (Charles Dickens) – 1022 paragraphs, Great Expectations (Charles Dickens) – 3835 paragraphs, Hard Times (Charles Dickens) – 2217 paragraphs, The Works of Edgar Allan Poe – Volume 2 – 906 paragraphs, The Works of Edgar Allan Poe – Volume 4 (Edgar Allan Poe) – 961 paragraphs, Beasts and Super-Beasts (Hector H. Munro) – 1319 paragraphs, The Chronicles of Clovis (Hector H. Munro) – 1006 paragraphs, The Toys of Peace (Hector H. Munro) – 1077 paragraphs, The Girl on the Boat (P. G. Wodehouse) – 2425 paragraphs, My Man Jeeves (P. G. Wodehouse) – 1943 paragraphs, Something New (P. G. Wodehouse) – 2259 paragraphs, The Adventures of Sally – 2357 paragraphs, The Clicking of Cuthbert (P. G. Wodehouse) – 1876 paragraphs, A Pair of Blue Eyes (Thomas Hardy) – 3666 paragraphs, Far from the Madding Crowd (Thomas Hardy) – 3407 paragraphs, Jude the Obscure (Thomas Hardy) – 2990 paragraphs, The Mayor of Casterbridge (Thomas Hardy) – 2117 paragraphs, The Hand of Ethelberta (Thomas Hardy) – 3112 paragraphs, Barry Lyndon (William M. Thackeray) – 1169 paragraphs, The History of Pendennis – 924 paragraphs, The Virginians (William M. Thackeray) – 1410 paragraphs, and Vanity Fair (William M. Thackeray) – 2589 paragraphs;
French: Le fils du Soleil (Gustave Aimard) – 2337 paragraphs, Face au Drapeau (Jules Verne) – 1487 paragraphs, Pierre de Villerglé (Louis Amédée Achard) – 1106 paragraphs, Les Idoles d’argile (Louis Reybaud) – 1083 paragraphs, and Han d’Islande (Victor Hugo) – 3983 paragraphs;
German: Die Wahlverwandtschaften (Goethe) – 3503 paragraphs, Der Moloch (Jakob Wassermann) – 1483 paragraphs, Königliche Hoheit – 1208 paragraphs, and Lichtenstein (Wilhelm Hauff) – 1770 paragraphs;
Italian: Il Peccato di Loreta (Alberto Boccardi) – 1592 paragraphs, La Montanara (Anton Giulio Barrili) – 2481 paragraphs, Alla Finestra (Enrico Castelnuovo) – 2069 paragraphs, Sciogli la treccia, Maria Maddalena (Guido da Verona) – 1834 paragraphs and La Pergamena Distrutta (Virginia Mulazzi) – 5861 paragraphs;
Portuguese: Amor de Perdi£o (Camilo Castelo Branco) – 1612 paragraphs, A Cidade e as Serras (Eça de Queirós) – 1453 paragraphs, Os Bravos do Mindello (Faustino da Fonseca) – 2115 paragraphs, Transviado (Jaime de Magalhães Lima) – 1947 paragraphs, and Uma Família Inglesa (Júlio Dinis) – 5241 paragraphs.