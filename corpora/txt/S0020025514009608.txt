In supervised data classification, for a given training set, a map from the input data to the corresponding desired output is estimated. The constructed map, called a classifier, is used to predict new input instances. Many supervised data classification techniques have been developed [6], [11], [19], [23], [30], [31], [35], [42], [43], such as k-nearest neighbors, Bayesian classifiers, neural networks, decision trees, committee machines, spectral biclustering, genetic algorithms, gravitational-based methods, and so on. In essence, all these techniques train and, consequently, classify unlabeled data items according to the physical features (e.g., distance, similarity or distribution) of the input data. These techniques that predict class labels using only physical features are called low-level classification techniques [36].
Usually, data items are not isolated points in the attribute space, but instead tend to form certain patterns. For example, in Fig. 1, the two test instances represented by the triangles are most probably to be classified as members of the square-shaped class if only physical features, such as distances among data instances, are considered. On the other hand, if we take into account the relationships among the data, we would intuitively classify the triangle-shaped items as members of the circular-shaped class, since a clear pattern (lozenge) is formed. The human (animal) brain performs both low and high orders of learning and it has facility of identifying patterns according to the semantic meanings of the input data. In general, however, this kind of task is still hard to be performed by computers. Supervised data classification by considering not only physical attributes but also pattern formation is referred to as high-level classification [36].
Broadly speaking, low-level classification techniques often share the same heuristic: division of the data space into sub-spaces, each of which representing a class. They are short in reproducing complex-formed or twisted classes, because they often rely on assumptions such as fixed shapes or predefined distributions. In contrast, the salient feature of the proposed technique is that it really provides two distinct classification heuristics: low- and high-level classifications. The former performs the prediction by the data’s physical features, while the latter captures the data’s pattern formations, which, in turn, permits the classifier to reproduce complex-formed and (or) twisted classes. As a result, a test instance is declared as member of the class to which it complies in a structural sense, no matter how far it is from the center or any members of that class.
It is well known that the network representation can capture arbitrary levels of relationships or interactions of the input data [37], [38], [39]. For this reason, we here show how the networks’ topological properties can help in identifying the pattern formation and, consequently, be used for general high-level classification. In this work, these topological properties are revealed by the tourist walks. A tourist walk can be defined as follows [21]. Given a set of cities, at each time step, the tourist (walker) goes to the nearest city that has not been visited in the past μ time steps. It has been shown that tourist walk is useful for data clustering [8] and image processing [3]. Each tourist walk can be decomposed in two terms: (i) the initial transient part of length t and (ii) a cycle (attractor) with period c. However, all these kinds of works are realized in regular lattices. Here, we study tourist walks in networks and we show that it has the ability of capturing the topological properties of the underlying network in a local to global fashion. It is worth observing that the application of tourist walks to graph-based environments is a new approach taken here. In addition, the employment of the tourist walks’ dynamics for discovering patterns in networks is a totally novel scheme in the literature.
Following the literature stream on such matter, several kinds of works related to high-level classification may be highlighted, such as:
the Semantic Web [4], [12], [34], which uses ontologies to describe the semantics of the data;
statistical relational learning, which may be decomposed in methods that realize collective inference [15], [25], [45], [46], [47] or graph-based semisupervised learning [9], [48];
contextual classification techniques [5], [10], [24], [27], [40], [41], [44], which consider the spatial relationships between the individual pixels and the local and global configurations of neighboring pixels in an image for assigning classes.
All the above-mentioned techniques, on one hand, try to make inferences for a new data item in accordance with the neighborhood relationships between data samples (nodes) within the graph. On the other hand, our approach aims at finding out global patterns formed by all the training samples. At the implementation level, while the former determines the class label of a test instance by analyzing the transition probabilities or other kinds of relational information, such as neighbors’ edge weights, our approach is realized by calculating the network’s topological measures, permitting the extraction of some kinds of semantic structures presented in the training data.
Another interesting related area is the graph-based structural pattern recognition [14], [16]. This topic is usually characterized as a graph matching problem. Both graph matching and the proposed approach intend to find out structural information, instead of pure geometrical information in the input patterns or data. However, in graph matching, pairs of patterns are compared. From the viewpoint of data classification, such structural information can be considered as a local information, because, at each time, only a limited amount of patterns is analyzed. In opposition, the proposed approach extracts pattern formations by considering the training data as a whole. As a consequence, our approach may reveal global organizations of data under analysis.
In this paper, we propose a technique that combines the low- and high-level supervised data classifications. The idea of this paper is built upon the general framework recently proposed by [36], where the high-level classification problem is treated using three existing network measures in a combined way: assortativity, clustering coefficient, and average degree. As highlighted by Silva and Zhao [36], a serious open problem is how one may choose other network measures in an intuitive way and also how one may define the learning weights that are associated for each of them. For instance, in their original paper, those three network measures were chosen under a series of trial and error attempts against several well-known network measures. In this paper, we address these two open issues as follows:
We propose a unified measure to capture the pattern formation of the data. In this way, one does not need to discover suitable and convenient sets of network measures to build up the high-level classifier, as occurs in [36]. In this paper, we show that the dynamical information generated by the tourist walks process can itself capture local-to-global organizational and complex features of the network by adjusting the walker’s memory length parameter. For example, when the memory window of the tourist is low, local structural features of the network are extracted. As the memory window grows larger, the walk dynamics compels the walker to venture far away from its starting point, permitting it to learn global features of the network.
The model selection procedure is simplified. In the original work in [36], the several learning weights of the high-level classifier must be carefully adjusted by the user. Because they are in a large number, the model selection procedure takes time and may be unfeasible for large data sets. As opposed to that, in this work, they are automatically adjusted by utilizing a statistical approach to fit the training data, which runs in linear time. As a result, the model selection effort is reduced at a large extent.
In addition, the adoption of tourist walks in this paper presents some interesting characteristics and advantages over the previous approach taken by [36]. For example, it occurs that the tourist walk method presents a class-dependent critical memory length, where any values larger than this critical point provide no changes in the behaviors of the transient and cycle lengths. This is an interesting phenomenon, which is observed when the memory length reaches a sufficient large value. We say that, when this happen, the walks have reached the “complexity saturation” of the class component. In this occasion, the global topological and organizational features of the network are said to be completely characterized in the sense of the tourist walks process. Moreover, we relate this phenomenon to phase transition in the context of complex networks. Finally, we show how the proposed technique can be used to solve general invariant pattern recognition problems [17], [29], [33], particularly when the pattern variances are nonlinear and there is not a closed form to describe the invariance.
The remainder of the paper is organized as follows. The proposed model is defined in Section 2. Computer simulations are performed on synthetic and real-world data sets in Section 3. In Section 4, the proposed technique is adapted to perform manual digits recognition. Finally, Section 5 concludes the paper.
In this section, the proposed model is described in detail.
The hybrid classifier is designed to work in a supervised learning environment. In the following, some mathematical notations and premises are discussed.
Consider that Xtraining={(x1,y1),…,(xl,yl)}⊂X×L<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">training</mi></mrow></msub><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">}</mo><mo is="true">⊂</mo><mi mathvariant="script" is="true">X</mi><mo is="true">×</mo><mi mathvariant="script" is="true">L</mi></mrow></math> denotes the training set, which is composed of l labeled training instances. Each training instance i,xi∈Xtraining<math><mrow is="true"><mi is="true">i</mi><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">training</mi></mrow></msub></mrow></math>, is given a discrete label or target yi∈L<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>. The goal here is to construct a hypothesis, in a way that the classifier maps x↦y<math><mrow is="true"><mi is="true">x</mi><mspace width="0.25em" is="true"></mspace><mo is="true">↦</mo><mspace width="0.25em" is="true"></mspace><mi is="true">y</mi></mrow></math>. Commonly, the constructed classifier is checked with regard to its prediction power by submitting it to a test set Xtest={xl+1,…,xn},n=l+u<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">test</mi></mrow></msub><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">n</mi></mrow></msub><mo stretchy="false" is="true">}</mo><mtext is="true">,</mtext><mi is="true">n</mi><mo is="true">=</mo><mi is="true">l</mi><mo is="true">+</mo><mi is="true">u</mi></mrow></math>, in which labels are not provided. In this case, each data item is called test instance. For an unbiased learning, the training and test sets must be disjoint, i.e., Xtraining∩Xtest=∅<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">training</mi></mrow></msub><mo is="true">∩</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">test</mi></mrow></msub><mo is="true">=</mo><mo is="true">∅</mo></mrow></math>.
In this section, we review the hybrid classification framework [36]. Specifically, the particularities of the training and classification phases, and the general definition of the classification scheme are discussed.
In this phase, the data in the training set are mapped into a graph G<math><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow></math> using a network formation technique g:Xtraining↦G=〈V,E〉<math><mrow is="true"><mi is="true">g</mi><mo is="true">:</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">training</mi></mrow></msub><mspace width="0.25em" is="true"></mspace><mo is="true">↦</mo><mspace width="0.25em" is="true"></mspace><mi mathvariant="script" is="true">G</mi><mo is="true">=</mo><mo stretchy="false" is="true">〈</mo><mi mathvariant="script" is="true">V</mi><mtext is="true">,</mtext><mi mathvariant="script" is="true">E</mi><mo stretchy="false" is="true">〉</mo></mrow></math>, where V={1,…,l}<math><mrow is="true"><mi mathvariant="script" is="true">V</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><mn is="true">1</mn><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mi is="true">l</mi><mo stretchy="false" is="true">}</mo></mrow></math> is the set of vertices and E<math><mrow is="true"><mi mathvariant="script" is="true">E</mi></mrow></math> is the set of edges. Each vertex in V<math><mrow is="true"><mi mathvariant="script" is="true">V</mi></mrow></math> represents a training instance in Xtraining<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">training</mi></mrow></msub></mrow></math>.
The idea is to build a graph component for each class encompassing the data set. The strategy for creating edges depends on the type of region at which each training instance is located. When the region is sparse, the k-nearest neighbor (k-NN) approach is employed, whereas, when it is dense, the ∊-radius technique is used. While the k-NN sets up an edge between the k most similar vertices and the reference vertex, the ∊-radius method creates a link to whichever vertex that is within a hyper-sphere of radius ∊ centered at the reference vertex. During the network formation step, only edges between vertices of the same class are permitted to be created. With these simple rules, it is expected that each class will have a representative graph component, in which the pattern and organizational features will be extracted.
In the classification phase, the unlabeled data items in the Xtest<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">test</mi></mrow></msub></mrow></math> are presented to the classifier one by one. In contrast to the training phase, the class labels of the test instances are unknown. The same strategy is employed with a slight change: the labels of the test instance’s neighborhood are not considered. Fig. 8 and [36, Fig. 2] illustrate this point.
Once the data item is inserted, each class analyzes, in an independent manner, the structural impacts that its insertion caused on the component. If slight or no changes occur, then it is said that the test instance is in compliance with that class pattern. Conversely, if these changes dramatically modify the class’ structural patterns, it is said that the test instance does not comply with that class pattern. As we will see, this information is estimated by using the deterministic process of tourist walks with varying memory lengths.
The hybrid classification framework F consists of a convex combination of two orthogonal visions, as follows:
i. A low-level classifier, for instance, a decision tree, SVM, or a k-NN classifier. The vision that it stresses is the physical similarities among the data;
ii. A high-level classifier, which is responsible for classifying new test instances according to their organizational or semantic meaning with the data. The vision that it values most is the pattern compliance of new test instances with the existing structure (model) built up in the training process.
Mathematically, the membership of the test instance xi∈Xtest<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">test</mi></mrow></msub></mrow></math> with respect to the class j∈L<math><mrow is="true"><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math> yielded by the hybrid framework, here written as Fi(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">F</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math>, is given by:(1)Fi(j)=(1-λ)Li(j)+λHi(j),<math><msubsup is="true"><mrow is="true"><mi is="true">F</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">=</mo><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo is="true">-</mo><mi is="true">λ</mi><mo stretchy="false" is="true">)</mo><msubsup is="true"><mrow is="true"><mi is="true">L</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">+</mo><mi is="true">λ</mi><msubsup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mtext is="true">,</mtext></math>where Li(j),Hi(j)∈[0,1]<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">L</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mtext is="true">,</mtext><msubsup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">1</mn><mo stretchy="false" is="true">]</mo></mrow></math> represent the membership of the test instance xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> towards class j produced by the low- and high-level classifiers, respectively, and λ∈[0,1]<math><mrow is="true"><mi is="true">λ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">1</mn><mo stretchy="false" is="true">]</mo></mrow></math> is the compliance term, which plays the role of counterbalancing the classification decisions supplied by both classifiers.
Motivated by the intrinsic ability of describing topological structures among the data items, a network-based (graph-based) technique for the high-level classifier H is proposed based on tourist walks. From the tourist walks process, we are interested in two particular derived variables: the transient and cycle lengths. The transient length is the number of vertices that the tourist visits before it gets trapped in an eternal loop. The cycle length denotes the number of different vertices that it visits once it enters the eternal loop. Since the tourist walker must respect the graph topology, it may get in a dead end with no available neighboring vertex to go. In this case, we say that the cycle length is null. In spite of being a simple rule, it has been shown that this movement dynamic possesses complex behavior when μ>1<math><mrow is="true"><mi is="true">μ</mi><mo is="true">&gt;</mo><mn is="true">1</mn></mrow></math> [21]. Moreover, the transient and cycle lengths are dependent on the choice of the memory length μ.
Having in mind these concepts, the decision output of the high-level classifier is given by:(2)Hi(j)=KH∑μ=0μc(j)winter(j)(μ)wintra(j)(μ)Ti(j)(μ)+1-wintra(j)(μ)Ci(j)(μ),<math><msubsup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">K</mi></mrow><mrow is="true"><mi is="true">H</mi></mrow></msub><mstyle displaystyle="true" is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">μ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></munderover></mstyle><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mrow is="true"><mfenced open="[" close="]" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><msubsup is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><msubsup is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mtext is="true">,</mtext></math>where:
μc(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> is a critical value that indicates the maximum memory length of the tourist walks performed in the training phase for class j;
Ti(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Ci(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> are functions that depend on the transient and cycle lengths, respectively, of the tourist walk applied to the ith data item with regard to class j. These functions are responsible for providing an estimate of whether or not the data item i under analysis possesses the same patterns of component j;
winter(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> is the weight or influence that is given for the tourist walk with memory length μ on class j. Observe that we have used the subscript inter to make clear that this coefficient deals with the regulation of tourist walks with different μ;
wintra(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> is the weight or influence of the transient length of a particular tourist walk with memory length μ on class j. The complementary value, i.e., 1-wintra(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math>, records the same information but for the cycle length. Note that we have used the subscript intra to denote that such coefficient is modulating the dynamics generated within the same tourist walk;
KH<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">K</mi></mrow><mrow is="true"><mi is="true">H</mi></mrow></msub></mrow></math> is a normalization constant which ensures the fuzziness of the high-level classifier H.
Mathematically, these descriptors are expressed by:(3)Ti(j)(μ)=1-Δti(j)(μ)p(j),Ci(j)(μ)=1-Δci(j)(μ)p(j),<math><mtable columnspacing="0em" is="true"><mtr is="true"><mtd columnalign="right" is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mn is="true">1</mn><mo is="true">-</mo><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><msup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mtext is="true">,</mtext></mrow></mtd></mtr><mtr is="true"><mtd columnalign="right" is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mn is="true">1</mn><mo is="true">-</mo><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><msup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mtext is="true">,</mtext></mrow></mtd></mtr></mtable></math>where Δti(j)(u),Δci(j)(u)∈[0,1]<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">u</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">u</mi><mo stretchy="false" is="true">)</mo><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">1</mn><mo stretchy="false" is="true">]</mo></mrow></math> are the variations of the transient and cycle lengths on the j-class component if test instance i joins it and p(j)∈[0,1]<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">1</mn><mo stretchy="false" is="true">]</mo></mrow></math> is the proportion of data items pertaining to class j. Remembering that each class has a representative graph component, the strategy to check the pattern compliance of the test instance is to examine whether its insertion causes a great variation of the j-class component’s network measures. In other words, if there is a small change in these network measures, we say the test instance is in compliance with that class, as it follows the same pattern of its original members. In contrast, if its insertion is responsible for a significant variation of such measures, then probably the test instance may not belong to that class. These behaviors are mathematically modeled by (2), (3). For instance, small (large) values for Ti(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Ci(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> force H to output small (large) values.
In the following, we explain how to compute Δti(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Δci(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> that appear in (3). Firstly, we need to numerically quantify the transient and cycle lengths of a component. Since the tourist walks are strongly dependent on the starting vertices, for a fixed μ, we perform tourist walks initiating from each one of the vertices that are members of a class component. The transient and cycle lengths of the jth component, ti(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math> and ci(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math>, are simply given by the average transient and cycle lengths of all its vertices, respectively. In order to estimate the variation of the component’s network measures, consider that xi∈Xtest<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">test</mi></mrow></msub></mrow></math> is a test instance. In relation to an arbitrary class j, we virtually insert xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> into component j using the network formation technique that we have seen, and recalculate the new average transient and cycle lengths of this component. We denote these new values as ti′(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math> and ci′(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math>, respectively. This procedure is performed for all classes j∈L<math><mrow is="true"><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>. It may occur that some classes u∈L<math><mrow is="true"><mi is="true">u</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math> will not share any connections with the test instance xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>. Using this approach, ti(k)(μ)=ti′(k)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">k</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mo is="true">=</mo><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">k</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math> and ci(k)(μ)=ci′(k)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">k</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mo is="true">=</mo><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">k</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math>, which is undesirable, since this configuration would state that xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> complies perfectly with class u. In order to overcome this problem, a simple post-processing is necessary: for all components u∈L<math><mrow is="true"><mi is="true">u</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math> that do not share at least 1 link with xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>, we deliberately set ti′(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math> and ci′(j)(μ)<math><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></math> to a high value.
With all this information at hand, we are able to calculate Δti(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Δci(j)(μ),∀j∈L<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">∀</mo><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>, as follows:(4)Δti(j)(μ)=ti′(j)(μ)-ti(j)(μ)∑u∈Lti′(u)(μ)-ti(u)(μ),Δci(j)(μ)=ci′(j)(μ)-ci(j)(μ)∑u∈Lci′(u)(μ)-ci(u)(μ),<math><mtable columnspacing="0em" is="true"><mtr is="true"><mtd columnalign="right" is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mfenced open="|" close="|" is="true"><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mo is="true">-</mo><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></mfenced></mrow></mrow><mrow is="true"><msub is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">u</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></msub><mrow is="true"><mfenced open="|" close="|" is="true"><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">u</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mo is="true">-</mo><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">u</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></mfenced></mrow></mrow></mfrac><mtext is="true">,</mtext></mrow></mtd></mtr><mtr is="true"><mtd columnalign="right" is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mfenced open="|" close="|" is="true"><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mo is="true">-</mo><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></mfenced></mrow></mrow><mrow is="true"><msub is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">u</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></msub><mrow is="true"><mfenced open="|" close="|" is="true"><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">u</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow><mo is="true">-</mo><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">u</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow></mfenced></mrow></mrow></mfrac><mtext is="true">,</mtext></mrow></mtd></mtr></mtable></math>where the denominator is introduced only for normalization matters. According to (4), for insertions that result in a considerable variation of the component’s transient and cycle lengths, Δti(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Δci(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> will be large. In view of (3), Ti(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Ci(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> are expected to be small, yielding a low membership value predicted by the high-level classifier Hi(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math>, as (2) reveals. On the other hand, for insertions that do not significantly interfere in the pattern formation of the data, Δti(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Δci(j)(μ)<math><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> will be small, and, as a result, Ti(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and Ci(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> are expected to be large, producing a high membership value for the high-level classifier Hi(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math>, as (2) exposes.
The network-based high-level classifier quantifies the variations of the transient and cycle lengths of tourist walks with limited memory μ that occur in the class components when a test instance artificially joins each of them in isolation. According to (2), this procedure is performed for several values of the memory length μ, ranging from 0 (memoryless) to a critical value μc<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math>. This is done in order to capture complex patterns of each of the representative class components in a local to global fashion. When μ is small, the walks tend to possess a small transient and cycle parts, so that the walker does not wander far away from the starting vertex. In this way, the walking mechanism is responsible for capturing the local structures of the class component. On the other hand, when μ increases, the walker is compelled to venture deep into the component, possibly very far away from its starting vertex. In this case, the walking process is responsible for capturing the global features of the component. In sum, the fundamental idea of the high-level classifier is to make use of a mixture of local and global features of the class components by means of a combination of tourist walks with different values of μ.
In (3), we also see that the changes in the transient and cycle lengths are modulated by the size proportions of the classes in the problem, i.e., p(j)∈[0,1]<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo is="true">∈</mo><mo stretchy="false" is="true">[</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">1</mn><mo stretchy="false" is="true">]</mo></mrow></math>. Oftentimes, in real-world databases, several unbalanced classes are encountered, that is the representative classes possess different sizes. The network measures used here (transient and cycle lengths of tourist walks) are very sensitive to the size of the components. In an attempt to soften this problem, it is intuitive to regulate and to re-scale the changes in the components by their corresponding relative sizes. Mathematically, the size proportion of class j is given by:(5)p(j)=1l∑u=1l1{yu=j},<math><msup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">l</mi></mrow></mfrac><mstyle displaystyle="true" is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">u</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">l</mi></mrow></munderover></mstyle><msub is="true"><mrow is="true"><mn mathvariant="double-struck" is="true">1</mn></mrow><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">u</mi></mrow></msub><mo is="true">=</mo><mi is="true">j</mi><mo stretchy="false" is="true">}</mo></mrow></msub><mtext is="true">,</mtext></math>where l is the number of vertices in the training set and 1{.}<math><mrow is="true"><msub is="true"><mrow is="true"><mn mathvariant="double-struck" is="true">1</mn></mrow><mrow is="true"><mo stretchy="false" is="true">{</mo><mo is="true">.</mo><mo stretchy="false" is="true">}</mo></mrow></msub></mrow></math> is the indicator function that yields 1 if the argument is logically true, or 0, otherwise. In view of the introduction of this mechanism, we expect to obviate the effects of unbalanced classes in the classification process.
As we have seen, the averaged dynamics of the tourist walk’s transient and cycle lengths are stored for a finite range of μ values. A natural question that arises is how more relevant is the cycle length over the transient length (or vice versa)? And in which situations does this hold true? In the model, according to (2), this calibration is performed by wintra(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math>. In this section, we show a strategy to adjust these parameters according to the training data.
For the sake of clarity, consider the following example. Suppose that we have calculated the transient length for the component representing class A (the same reasoning applies for the cycle length). Consider that, for a fixed μ, after calculating the transient and cycle lengths for each of the members of class A, we obtained the variances σt(μ)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and σc(μ)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> for the transient and cycle length descriptors, respectively. Moreover, say that σt(μ)≫σc(t)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">≫</mo><msub is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo stretchy="false" is="true">)</mo></mrow></math>. In this scenario, we can see the members of such component share almost the same vision for the cycle length descriptor (common or homogeneous vision), because its variance is relatively smaller than the other term. However, when it comes to the descriptor based on transient lengths, we can verify that there is a great diversity of visions in the same component. In other terms, each member can be thought to “see” peculiar or unique visions throughout the component, causing the large variance (heterogeneous vision). In the event of a insertion of a new test instance, it is reliable to state that its impact on the organizational formation of the component is much stronger if its insertion causes a break in the homogenous vision rather than in the heterogeneous vision. That is, a greater influence should be intuitively given for changes in homogeneous visions in detriment to changes in heterogenous visions. With this idea in mind, we propose a calibration of wintra(μ)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> using the variances of the transient and cycle lengths generated by the training set.
In the following, we formalize this idea. For a fixed μ and j∈L<math><mrow is="true"><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>, let the variances of the tourist walk’s transient and cycle lengths be σc(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and σt(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math>, respectively. Then, wintra(j)(μ),∀μ∈{0,…,μc},j∈L<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mo is="true">∀</mo><mi is="true">μ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0</mn><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo stretchy="false" is="true">}</mo><mtext is="true">,</mtext><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>, is given as follows:(6)wintra(j)(μ)=σc(j)(μ)+1σt(j)(μ)+σc(j)(μ)+2,<math><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><mn is="true">1</mn></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><mn is="true">2</mn></mrow></mfrac><mtext is="true">,</mtext></math>(7)1-wintra(j)(μ)=σt(j)(μ)+1σt(j)(μ)+σc(j)(μ)+2,<math><mn is="true">1</mn><mo is="true">-</mo><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><mn is="true">1</mn></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><mn is="true">2</mn></mrow></mfrac><mtext is="true">,</mtext></math>where (6) refers to the influence of the transient length and (7) provides the influence of the cycle length when tourist walks with memory length μ are performed. Note that we have employed the Laplace Smoothing technique into the determination of such coefficient, which adds 1 for the variances of each variable. This allows (6), (7) to be always defined for every σc(j)(μ)×σt(j)(μ)∈R2<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">×</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></math>.
Eqs. (6), (7) offer strategies for determining the influence of each of the dynamics generated by tourist walks with fixed μ (intra-relation of a tourist walk). If we return to our example, we see that, when σt(j)(μ)≫σc(j)(μ),wintra(j)(μ)→0<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">≫</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">intra</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">→</mo><mn is="true">0</mn></mrow></math>. This means that the weight given for the cycle length dynamics will be much larger than the transient length dynamics. This behavior is consistent with our idea that disturbances in homogenous visions are more penalized than in heterogeneous visions.
Another important intuition that arises is how more important is a walk with specific μ in detriment to another one with different μ. Observe that the walks are considered as a whole here, i.e., we group the transient and cycle lengths. In order to give out influence weights for each of these tourist walks, we utilize the same strategy based on variances given in the previous section. For instance, say that we capture the transient and cycle lengths for two different values of μ:μ0<math><mrow is="true"><mi is="true">μ</mi><mo is="true">:</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></mrow></math> and μ1<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></math>. For the first, say that the sum of variances of the transient and cycle lengths is σt(j)(μ0)+σc(j)(μ0)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math>. For the latter, it is σt(j)(μ1)+σc(j)(μ1)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math>. Suppose that σt(j)(μ0)+σc(j)(μ0)≫σt(j)(μ1)+σc(j)(μ1)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">≫</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math>. It should be noted that this time we are looking at different tourist walks with different μ. Therefore, if we give more weight to walks that have less total variance (transient + cycle), it is the same as to declare that we are favoring homogenous visions against heterogeneous visions, like in our previous approach.
Likewise before, this idea is formalized as follows. For a fixed μ and j∈L<math><mrow is="true"><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>, say that the variances of the transient and cycle lengths of tourist walks with μ,μ∈{0,…,μc}<math><mrow is="true"><mi is="true">μ</mi><mtext is="true">,</mtext><mi is="true">μ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0</mn><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math>, are given by σc(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and σt(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math>, respectively. Then, winter(j)(μ),μ∈{0,…,μc}<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mi is="true">μ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0</mn><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math>, is given as follows:(8)winter(j)(μ)=∑Δ=0,Δ≠μμc(j)σt(j)(Δ)+σc(j)(Δ)∑ρ=0μc(j)∑Δ=0,Δ≠μμc(j)σt(j)(Δ)+σc(j)(Δ)=kinter(j)-σt(j)(μ)+σc(j)(μ)μc(j)kinter(j),<math><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><mo is="true">=</mo><mn is="true">0</mn><mtext is="true">,</mtext><mi mathvariant="normal" is="true">Δ</mi><mo is="true">≠</mo><mi is="true">μ</mi></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></msubsup><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">ρ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></msubsup><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><mo is="true">=</mo><mn is="true">0</mn><mtext is="true">,</mtext><mi mathvariant="normal" is="true">Δ</mi><mo is="true">≠</mo><mi is="true">μ</mi></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></msubsup><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo></mrow></mfrac><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">-</mo><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><msubsup is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></mfrac><mtext is="true">,</mtext></math>where:(9)kinter(j)=∑Δ=0μc(j)σt(j)(Δ)+σc(j)(Δ).<math><msubsup is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">=</mo><mstyle displaystyle="true" is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="normal" is="true">Δ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></munderover></mstyle><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">.</mtext></math>
Note that, for a fixed μ, if σt(j)(μ)+σc(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">σ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> is large, then the corresponding influence of that tourist walk, winter(j)(μ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math>, is small in relation to the others winter(j)(Δ),Δ≠μ<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mi mathvariant="normal" is="true">Δ</mi><mspace width="0.25em" is="true"></mspace><mo is="true">≠</mo><mspace width="0.25em" is="true"></mspace><mi is="true">μ</mi></mrow></math>. Conversely, if the sum of the descriptors of the tourist walk with fixed μ is small, it will yield a large winter(j)(Δ)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">inter</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">Δ</mi><mo stretchy="false" is="true">)</mo></mrow></math>, showing the relative importance of those tourist walks in the learning process.
The high-level classifier accepts two parameters as input: k and ∊, both responsible for creating the network from the input data. The hybrid classifier, besides the parameters from the low- and high-level classifiers, accepts one additional parameter: the compliance term λ. The critical memory length for class j,μc(j)<math><mrow is="true"><mi is="true">j</mi><mtext is="true">,</mtext><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math>, can be estimated via an effective heuristic, which will be explored in the computer experiments section.
Finally, for didactic purposes, we provide exemplificative algorithms showing the main steps for the training and classification phases inAlgorithm 1,Algorithm 2, respectively.

Algorithm 1

Training phase of the high-level classification. 1.Construct a set of network components G={G1,…,GL}<math><mrow is="true"><mi mathvariant="script" is="true">G</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math>, for each class from the vector-based training set by using the combined k-NN and ∊ rules from the training phase;2.Calculate the transient and cycle lengths for each g∈G<math><mrow is="true"><mi is="true">g</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">G</mi></mrow></math>;3.For all μ, calculate the means and variances of the descriptors in Step 2 for each class component. The set of all these values compose the hypothesis of the model;4.Define the inter- and intra-weights of the learned hypothesis in agreement with (6), (8).

Algorithm 2

Classification phase of the high-level classification. 1.Virtually insert a test instance xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> into the formed graph by using the k-NN and ∊ rules from the classification phase;2.Recalculate the new transient and cycle lengths for components that received at least one connection from xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>;3.Calculate the transient and cycle length variations by using (3);4.Produce the high-level decision value by using (2).
Training phase of the high-level classification. 1.Construct a set of network components G={G1,…,GL}<math><mrow is="true"><mi mathvariant="script" is="true">G</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math>, for each class from the vector-based training set by using the combined k-NN and ∊ rules from the training phase;2.Calculate the transient and cycle lengths for each g∈G<math><mrow is="true"><mi is="true">g</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">G</mi></mrow></math>;3.For all μ, calculate the means and variances of the descriptors in Step 2 for each class component. The set of all these values compose the hypothesis of the model;4.Define the inter- and intra-weights of the learned hypothesis in agreement with (6), (8).
Construct a set of network components G={G1,…,GL}<math><mrow is="true"><mi mathvariant="script" is="true">G</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi mathvariant="script" is="true">G</mi></mrow><mrow is="true"><mi is="true">L</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math>, for each class from the vector-based training set by using the combined k-NN and ∊ rules from the training phase;
Calculate the transient and cycle lengths for each g∈G<math><mrow is="true"><mi is="true">g</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">G</mi></mrow></math>;
For all μ, calculate the means and variances of the descriptors in Step 2 for each class component. The set of all these values compose the hypothesis of the model;
Define the inter- and intra-weights of the learned hypothesis in agreement with (6), (8).
Classification phase of the high-level classification. 1.Virtually insert a test instance xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> into the formed graph by using the k-NN and ∊ rules from the classification phase;2.Recalculate the new transient and cycle lengths for components that received at least one connection from xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>;3.Calculate the transient and cycle length variations by using (3);4.Produce the high-level decision value by using (2).
Virtually insert a test instance xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> into the formed graph by using the k-NN and ∊ rules from the classification phase;
Recalculate the new transient and cycle lengths for components that received at least one connection from xi<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>;
Calculate the transient and cycle length variations by using (3);
Produce the high-level decision value by using (2).
In this section, the computational complexity of the high-level classifier is examined. Suppose here n is the number of vertices and μc(∗)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">∗</mo><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> is the largest μc<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math> of all classes. First, we analyze the training phase’s time complexity by looking at each step of Algorithm 1:
The distance matrix D constructed by the network formation technique consumes O(n2)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math>. In order to find the nearest neighbor when applying the tourist walks, we sort D so as to put the most similar (the nearest) vertices at the front;
Considering that the average transient and cycle lengths are t and c, respectively, then one tourist walk can be performed in O(t+c)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">+</mo><mi is="true">c</mi><mo stretchy="false" is="true">)</mo></mrow></math>, because the determination of the nearest neighbors is always processed in O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math> by simply consulting D. Since tourist walks are accomplished starting from each of the n vertices, this step is finished in O(n(t+c))<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">+</mo><mi is="true">c</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow></math>. We should note here that, from our simulations, we have verified that t and c are not proportional to n, in such a way that they may be considered as constants;
The mean is calculated in O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>. Once we have the mean, the determination of the variance is also processed in O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>. This is performed μc(∗)+1<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">∗</mo><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">+</mo><mn is="true">1</mn></mrow></math> times for the L classes in the data set. Since μc(∗)≪n<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">∗</mo><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">≪</mo><mi is="true">n</mi></mrow></math> and, usually, L≪n<math><mrow is="true"><mi is="true">L</mi><mo is="true">≪</mo><mi is="true">n</mi></mrow></math>, this step is approximately completed in O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>;
The evaluation of (6), (8) requires a scan over the L lists, each of which with a worst-case size of μc(∗)+1<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">∗</mo><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">+</mo><mn is="true">1</mn></mrow></math>. Since these numbers are considered as constants, the determination of both intra- and inter-weights is accomplished in O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math>.
Therefore, the computational complexity of the training phase is given by the critical step, which is Step 1 (network formation): O(n2)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math>. For comparison purposes, the SVM has between O(n2)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math> and O(n3)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math> (depends on the type of SVM and kernel used), the C4.5 decision tree algorithm is O(n2)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math>, and the k-NN is O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math> (because there is no training) [2]. In view of this, the time complexity of our algorithm’s training phase is reasonable.
In the next, we examine the classification phase’s time complexity by stepping through Algorithm 2:
We need to find the nearest neighbors of the test instance. This can be done in O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>. For each altered class, we need to increment the distance matrix to reflect the new temporary instance. Considering that the graph has an average degree of 〈k〉<math><mrow is="true"><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo></mrow></math>, this is performed in O(〈k〉)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo stretchy="false" is="true">)</mo></mrow></math>. After that, we need to reorder only the 〈k〉<math><mrow is="true"><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo></mrow></math> altered rows. This is done in O(〈k〉×〈k〉log〈k〉)=O(〈k〉2log〈k〉)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo is="true">×</mo><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mi mathvariant="normal" is="true">log</mi><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><msup is="true"><mrow is="true"><mo stretchy="false" is="true">〉</mo></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi mathvariant="normal" is="true">log</mi><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo stretchy="false" is="true">)</mo></mrow></math>;
Similarly to Step 2 of the training phase, this is performed in O(n(t+c))<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">+</mo><mi is="true">c</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow></math>;
Equivalently to Step 3 of the training phase, this is concluded in O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>;
The evaluation of a linear combination of 2μc(∗)+1<math><mrow is="true"><mn is="true">2</mn><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">∗</mo><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">+</mo><mn is="true">1</mn></mrow></math> terms is necessary. Each term consumes O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math>, because only a simple multiplication is calculated. Adding all these multiplications and taking into account that μc(∗)≪n<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">∗</mo><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">≪</mo><mi is="true">n</mi></mrow></math>, this step is finished in O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math>.
For the classification phase, Step 1 or Step 2 may be the most time-consuming ones, depending on the network’s characteristics. If the network is sparse, i.e., 〈k〉≪n<math><mrow is="true"><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo is="true">≪</mo><mi is="true">n</mi></mrow></math>, the time complexity of Step 1 reduces to O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math> and the processing time of Step 2 dominates. Therefore, the algorithm runs in O(n(t+c))<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">+</mo><mi is="true">c</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow></math>. Taking into account that t and c are often very small compared to n, the algorithm runs roughly in O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>. However, if the network is dense, then 〈k〉≈n<math><mrow is="true"><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo is="true">≈</mo><mi is="true">n</mi></mrow></math> and Step 1 is dominant. In this special occasion, the algorithm runs in O(n2logn)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mi mathvariant="normal" is="true">log</mi><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>. Since the network formation is controllable, we can always choose k and ∊ such as to satisfy the network’s sparseness in a way that the algorithm always performs approximately in linear time for each test instance. For comparison matters, the SVM classifies a test instance in O(NSV)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">SV</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math>, where SV is the number of support vectors, the C4.5 decision tree algorithm, in O(1)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo></mrow></math>, and the k-NN, in O(nlog(n))<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mi mathvariant="normal" is="true">log</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow></math> [2].
With respect to the hybrid classifier, which is composed by the low- and high-level classifiers, the resulting computational complexity is max(O(low-level),O(high-level))<math><mrow is="true"><mi mathvariant="italic" is="true">max</mi><mo stretchy="false" is="true">(</mo><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">low</mi><mo is="true">-</mo><mi mathvariant="normal" is="true">level</mi><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi mathvariant="normal" is="true">high</mi><mo is="true">-</mo><mi mathvariant="normal" is="true">level</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow></math>. Note that this is true because the training phase (often, the most time-consuming) of each of them is done in an independent, serial manner (see the model selection procedure in the computer simulations section).
In terms of memory cost, the critical step is when the algorithm builds the distance matrix from the raw data. This distance matrix can be represented by multiple adjacency lists for enhanced use of the memory. If 〈k〉<math><mrow is="true"><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo></mrow></math> is the average degree of the vertices, then the memory consumption is O(n〈k〉)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">〈</mo><mi is="true">k</mi><mo stretchy="false" is="true">〉</mo><mo stretchy="false" is="true">)</mo></mrow></math>. If the network is sparse, then the memory consumption is approximately O(n)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></math>; however, if the network is dense, O(n2)<math><mrow is="true"><mi mathvariant="script" is="true">O</mi><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math>.
In this section, computer experiments are performed in order to assess the effectiveness of the proposed hybrid classification model based on tourist walks.
Consider the classification problem arranged in Fig. 2. For simplicity, the model’s selection step is skipped. Here, the minimum required compliance term, λmin<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">min</mi></mrow></msub></mrow></math>, is empirically calculated for which the test instances are classified as members of the red or circular-shaped class. In the figure, one can see that there is a segment of line representing the red or circular-shaped class (9 vertices) and also a condensed rectangular class outlined by the blue or square-shaped class (1000 vertices). The network formation parameters are fixed as k=1<math><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math> and ∊∊=0.07<math><mrow is="true"><mi is="true">∊</mi><mo is="true">=</mo><mn is="true">0.07</mn></mrow></math> (this radius covers, for any vertex in the straight line, 2 adjacent vertices, except for the vertices in each end). The fuzzy Support Vector Machine (fuzzy SVM) technique [22] with RBF kernel (C=22<math><mrow is="true"><mi is="true">C</mi><mo is="true">=</mo><msup is="true"><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></math> and γ=2-1<math><mrow is="true"><mi is="true">γ</mi><mo is="true">=</mo><msup is="true"><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mo is="true">-</mo><mn is="true">1</mn></mrow></msup></mrow></math>) is employed as the traditional low-level classifier. The task is to classify the 14 test instances depicted by the big triangle-shape items from left to right. After a test instance is classified, it is incorporated to the training set with the corresponding predicted label (self-training). The graphic embedded in Fig. 2 shows the minimum required value of λmin<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">min</mi></mrow></msub></mrow></math> for which the triangle-shaped items are classified as members of the red or circular-shaped class. This graphic is constructed according to the triangle-shaped element that is exactly at the same position with respect to the x-axis in the scatter plot drawn above. For example, the second triangle-shaped data item is classified as member of the blue or square-shaped class if λ⩾λmin≈0.37<math><mrow is="true"><mi is="true">λ</mi><mo is="true">⩾</mo><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">min</mi></mrow></msub><mo is="true">≈</mo><mn is="true">0.37</mn></mrow></math>, and of the red or circular-shaped if otherwise. The third and forth data items would require at least λmin≈0.81<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">min</mi></mrow></msub><mo is="true">≈</mo><mn is="true">0.81</mn></mrow></math> and λmin≈0.96<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">min</mi></mrow></msub><mo is="true">≈</mo><mn is="true">0.96</mn></mrow></math>, respectively, to be declared as members of the blue or square-shaped class. Specifically, as the straight line crosses the condensed region of the blue or square-shaped class, the compliance term approaches λmin→1<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mi mathvariant="normal" is="true">min</mi></mrow></msub><mo is="true">→</mo><mn is="true">1</mn></mrow></math>, since one cannot establish its decision based on the low-level classifier, because it would erroneously decide favorably to the blue or square-shaped class. In sum, this structured example shows a situation where the high-order of learning is very useful.
In this section, several simulations are performed so that the influences of the model’s parameters can be better understood.
A first note that is interesting to highlight is a study on the compliance term, λ, in the learning process performed by Silva and Zhao [36, Section III]. They show that a large value of λ is required if the classes are highly mixed, indicating that the high-level term is specially useful for the complex cases of classification. Such a phenomenon also appears in the new model presented in this paper. Thus, we do not repeat it here.
The network formation step plays a crucial role in the learning process of the high-level classifier, since the latter depends on the constructed network structure in order to extract organizational and pattern-formation of the class components. It is worth highlighting that we may be conducting a learning process over a raw-data (vectorized) or a complex network. In the latter, the network formation step is skipped, because the network is already formed. However, in the former, the network formation must be employed so as to transform the raw-data in a networked representation. In this paper, this construction is regulated by the parameters k and ∊. Even being a simple network formation technique (combination of ∊-radius and k-NN), it is very effective in some cases. Furthermore, one should note that the resulting graph is not always guaranteed to encode high-level, semantic information. More advanced network formation techniques that uncover pattern-formation in a systematic way will be studied in future works.
In the following, an analytical study of the network formation procedure is explored. For every data item (vertex), the algorithm determines if it is located in a sparse or dense region by counting the number of data items within a hyper-sphere with radius ∊. If this number is less than k, the vertex is declared to be in a sparse region and the k-NN is used. Otherwise, the ∊-radius is employed. If k is bounded by the number of data items (n), some mathematical consequences of this procedure are given below:
If k→n⇒<math><mrow is="true"><mi is="true">k</mi><mo is="true">→</mo><mi is="true">n</mi><mo is="true">⇒</mo></mrow></math> all vertices are declared to be in sparse regions ⇒<math><mrow is="true"><mo is="true">⇒</mo></mrow></math> always use the k-NN technique, regardless of ∊;
If k→0⇒<math><mrow is="true"><mi is="true">k</mi><mo is="true">→</mo><mn is="true">0</mn><mo is="true">⇒</mo></mrow></math> all vertices are declared to be in dense regions ⇒<math><mrow is="true"><mo is="true">⇒</mo></mrow></math> always use the ∊-radius technique;
If ∊∊→∞⇒<math><mrow is="true"><mi is="true">∊</mi><mo is="true">→</mo><mi is="true">∞</mi><mo is="true">⇒</mo></mrow></math> all vertices are declared to be in dense regions ⇒<math><mrow is="true"><mo is="true">⇒</mo></mrow></math> always use the ∊-radius technique, regardless of k;
If ∊∊→0⇒<math><mrow is="true"><mi is="true">∊</mi><mo is="true">→</mo><mn is="true">0</mn><mo is="true">⇒</mo></mrow></math> all vertices are declared to be in sparse regions ⇒<math><mrow is="true"><mo is="true">⇒</mo></mrow></math> always use the k-NN technique.
The aforementioned behaviors imply that only for intermediate values of k and ∊, both the k-NN and ∊-radius techniques are enabled. This is because, if k is too large, the ∊-radius technique is disabled. Conversely, if ∊ is too large, the k-NN technique is turned off.
In order to see that, a computer experiment on the Letter Recognition data set (see Table 1 for details), where it is inspected how the accuracy rate changes as each of these parameters varies. In special, Fig. 3a displays the impact of k in the learning process (with ∊ fixed) and Fig. 3b, the influence of ∊ (with k fixed). Within each figure, the best low-level classification rate is also reported. The region above this border denotes enhancements in the learning process, while the region below it shows poorer performances of the hybrid classifier due to the insertion of the high-level classifier.
With regard to Fig. 3a, we see that the accuracy rate consistently raises until it reaches an optimal λ-dependent point. Followed by it, a smooth drop is verified until we a critical drop-off point is reached, in which the high-level classifier starts to prejudice the overall learning process (the hybrid classifier enters the region below the low-level clipping). In Fig. 3b, we see a different behavior: for small values of ∊, the hybrid classifier cannot surpass the performance of the single low-level classifier. Only for intermediate values of ∊ the hybrid classifier is able to enhance the overall accuracy rate. Likewise the previous case, for large values of ∊, the overall accuracy rate starts to drop below the low-level classifier performance. For both cases, when these parameters assume large values, the resulting network starts to become very dense and the network’s organizational features cannot be correctly captured. In the extreme, the neighborhoods of all vertices become the same and the network reduces to a complete graph, which shows no interesting patterns among the data relationships. However, when only ∊ is small, by the analysis which we have done, we have seen that only the k-NN technique is enabled, which tends to make all the vertices’ degrees homogeneous. As a result, no clear class patterns arise from the resulting network.
As we have seen, the high-level classifier makes its prediction by using combinations of several tourist walks with memory lengths up to μc(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> for every j∈L<math><mrow is="true"><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>. A natural question that arises is: is it really necessary to perform the computation of tourist walks with μ ranging from 0 to the number of vertices in a component (maximum feasible value)? In this section, it is empirically shown that, usually, it is not necessary to perform all these computations. In order to strengthen our conclusions, this interesting behavior is verified both in synthetic and real-world data sets.
For the experiments, consider the following well-known data sets from the UCI Machine Learning Repository [13]: Iris (balanced classes) and Wine (unbalanced classes). We safely skip the model’s selection phase, which will be thoroughly discussed in the next section, and provide fixed values for ∊k,∊<math><mrow is="true"><mi is="true">k</mi><mtext is="true">,</mtext><mi is="true">∊</mi></mrow></math>, and λ. Consider Fig. 4a and b, where it is depicted the transient and cycle lengths for the classes of the Iris data set, and Fig. 5a and b, in which it is displayed the same information for the Wine data set. With respect to the transient length behavior, we can see that, for both data sets, as μ increases, the transient length also increases. However, when μ is sufficiently large, the components’ transient lengths settle down in a flat region. On the other hand, for both data sets, the cycle length behavior is rather interesting, which can be roughly divided in three different regions: (i) for a small μ, it is directly proportional to μ; (ii) for intermediate values of μ, it is inversely proportional to μ; and (iii) for sufficiently large values of μ, it also settles down in a steady region. One can interpret these results as follows:
When μ is small, it is very likely that the transient and cycle parts will also be small, because the memory of the tourist is very limited. We can conceive this as a walk with almost no restrictions;
When μ assumes an intermediate value, the transient length keeps increasing but the cycle length reaches a peak and starts to decrease afterwards. This peak characterizes the topological complexity of the component and varies from one to another. Hence, this is the most important region for capturing pattern formation of the class component by using the network’s topological structure;
When μ is large, the tourist has a greater chance of getting trapped in a vertex of the graph, once all the neighborhood of the visited vertex is contained in the memory window μ. In this scenario, the transient length is expected to be very high and the cycle length, null. This phenomenon explains the steady regions in Fig. 4a and b. In this region, we can say that the tourist walks have already covered all the global aspects of the class component, and increasing the memory length μ will not capture any new topological features or pattern formation of the class. In this scenario, it is said that the tourist walks have completely described the topological complexity of the class component (saturation). In view of this, the calculation of tourist walks by further increasing μ is redundant.
This analysis suggests that the accuracy of the high-level classifier may not change given that we choose suitable μc(j),j∈L<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mtext is="true">,</mtext><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>, residing near these steady regions for each class in the problem. This means that higher values for μc(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> will only cause redundant computations and the accuracy will not be enhanced nor changed. In order to check this, Fig. 6a and b reveal the behavior of the hybrid classification framework for different values of μc=μc(j),∀j∈L<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo is="true">=</mo><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mtext is="true">,</mtext><mo is="true">∀</mo><mi is="true">j</mi><mo is="true">∈</mo><mi mathvariant="script" is="true">L</mi></mrow></math>. We have used three distinct compliance terms, namely λ∈{0,0.05,0.6}<math><mrow is="true"><mi is="true">λ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">0.05</mn><mtext is="true">,</mtext><mn is="true">0.6</mn><mo stretchy="false" is="true">}</mo></mrow></math> for the Iris data set and λ∈{0,0.06,0.7}<math><mrow is="true"><mi is="true">λ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">0.06</mn><mtext is="true">,</mtext><mn is="true">0.7</mn><mo stretchy="false" is="true">}</mo></mrow></math> for the Wine data set. When λ=0<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, only the low-level classifier is used, in such a way that the value of μc<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math> is irrelevant, since the high-level term is disabled. With respect to the other cases, when μc⩾20<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo is="true">⩾</mo><mn is="true">20</mn></mrow></math>, the model provides the same accuracy rates for both data sets, confirming our prediction that, once it reaches the steady region where the transient and cycle lengths settle down, subsequent increases of μc<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math> do not change the accuracy of the model.
This phenomenon of complexity saturation observed in these data sets can also be related to phase transition in networks. For example, when μ<μc<math><mrow is="true"><mi is="true">μ</mi><mo is="true">&lt;</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math>, we can conceive the tourist walks in the network to be in an exploratory phase, where the dynamics of the transient and cycle lengths change as the parameter μ is modified. Therefore, in this initial exploratory phase, parameter μ is sensitive to the outcome of the tourist walks’ dynamics. However, when μ=μc<math><mrow is="true"><mi is="true">μ</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math>, we transition from the exploratory to the stationary phase, where all the transient and cycle lengths of the tourist walks in the networks are not sensitive (independent) anymore. This holds true for all μ⩾μc<math><mrow is="true"><mi is="true">μ</mi><mo is="true">⩾</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math>. In this phase, the graph topology restrains the tourist walk such as to not change its dynamical information anymore. As the network becomes more dense, more different walks are probabilistic possible, and μc<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math> is expected to take on larger values. With this respect, in a complete graph, the μc<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math> of a networked topology would be exactly equal to a networkless (lattice) approach.
Based on these experiments, an heuristic for estimating the critical memory length μc(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> is provided as follows. For a particular class, the dynamics of the tourist walks are calculated starting from μ=0<math><mrow is="true"><mi is="true">μ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>. Once finished, μ is incremented and the same calculations are performed for the new μ. Say that t(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and c(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> are the curves drawn from these calculations for the transient and cycle lengths, respectively. Once the derivatives of t(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and c(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math>, i.e., t′(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and c′(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> are zero, we store the μc(j)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> in which this happened and start out a counter, which monitors how many iterations of μ the derivatives of these measures have not changed. This counter is incremented as μ increases. If t′(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">t</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> and c′(j)(μ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo is="true">′</mo><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">(</mo><mi is="true">μ</mi><mo stretchy="false" is="true">)</mo></mrow></math> remain zero-valued in few iterations, the learning process is stopped and all calculations for which μ>μc(j)<math><mrow is="true"><mi is="true">μ</mi><mo is="true">&gt;</mo><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math> are discarded, because nothing has changed since μ>μc(j)<math><mrow is="true"><mi is="true">μ</mi><mo is="true">&gt;</mo><msubsup is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup></mrow></math>.
In this section, the proposed hybrid classifier is tested against several well-known UCI data sets.
Table 1 summarizes the most important information about the data sets which will be investigated. For a detailed description, refer to [13]. A series of pre-processing steps are performed on these data sets. In the network formation step, the following similarity functions are used: for numerical attributes, the reciprocal of the Euclidean distance is employed; for categorical examples, the overlap similarity measure [7] is utilized. If missing attributes are present, we input the mean (numerical) or most frequent (categorical) value of that attribute in the missing spots. All data sets are submitted to a standardization pre-processing step.
In the current model, the hybrid classifier comprises the low- and high-level classifiers. In order to make the model selection easier, the low-level classifier is trained in an independent manner using the 10-fold cross-validation error estimation technique. After its model selection is completed, we start calibrating the high-level classifier and the compliance term as follows.
As we have seen, the high-level term requires the network formation parameters – k and ∊ – and also the compliance term λ to run. In order to select a reasonable model for these parameters, we also make use of the 10-fold cross-validation error estimation technique. In special, we divide the data set into two complementary sets: the training (9 folds) and test (1-fold) sets. The reported accuracies and standard deviations of the hybrid classification technique are given by the averaged values of the model tested against the 10 existing folds, one at a time serving as test set.
The estimation of the three parameters of the model is performed exclusively on the data from the training set. To give reliable estimates, we also apply the 10-fold cross-validation technique on the training set. In this case, we separate out from the training set a validation set (1 sub-fold) in which we estimate the accuracy of the model for the current selected parameters and the other 9 sub-folds are used for training the model’s hypothesis. We perform this 10 times by shifting the validation set among the ten sub-folds. The estimated accuracy of the model is given by the mean value of these ten rounds. Only the set of parameters (∊k,∊,λ<math><mrow is="true"><mi is="true">k</mi><mtext is="true">,</mtext><mi is="true">∊</mi><mtext is="true">,</mtext><mi is="true">λ</mi></mrow></math>) which provides the best accuracy rate is chosen to be tested against the test set (in the way we describe above).
In the parameter optimization process, the three model’s parameters may take the following values (search space): ∊k∈{1,2,…,10},∊∈{0.01,0.02,…0.10}<math><mrow is="true"><mi is="true">k</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">1</mn><mtext is="true">,</mtext><mn is="true">2</mn><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mn is="true">10</mn><mo stretchy="false" is="true">}</mo><mtext is="true">,</mtext><mi is="true">∊</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0.01</mn><mtext is="true">,</mtext><mn is="true">0.02</mn><mtext is="true">,</mtext><mo is="true">…</mo><mn is="true">0.10</mn><mo stretchy="false" is="true">}</mo></mrow></math> and λ∈{0,0.01,…,1}<math><mrow is="true"><mi is="true">λ</mi><mo is="true">∈</mo><mo stretchy="false" is="true">{</mo><mn is="true">0</mn><mtext is="true">,</mtext><mn is="true">0.01</mn><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><mn is="true">1</mn><mo stretchy="false" is="true">}</mo></mrow></math>. Again, only the best combination of these parameters that achieves the best accuracy rate in the validation set is selected to represent the algorithm in the classification phase against the test instances.
Since the model selection of the low-level classifier is done independently from the high-level classifier and the compliance term, we will verify that different models for the “high-level classifier + compliance term” will appear in the simulations as the low-level classifier changes. This is intuitive because each low-level classifier is able to correctly classify different sub-sets of data, which are not necessarily the same. As a result, the high-level classifier and the compliance term will change in such a way to accommodate accordingly to the resulting low-level model.
In order to show the importance of a networked environment over raw-data representation when performing tourist walks, in this section, we investigate the performance of the proposed model on real-world data sets with and without networks in the learning process. Specifically, we have devised two different kinds of high-level classifiers: (i) one in which the tourist walks are performed in a network constructed from the vector-based data set and (ii) one in which the tourist walks are realized in a lattice, i.e., the tourist is free to visit any other data site apart from the ones in the memory window μ. In the latter case, it is expected that the walker will perform long jumps in the data set once the memory length μ assumes large values. It will be verified that such mechanism is not very welcomed in classification tasks. Furthermore, this serves as a strong argument for the employment and introduction of network-based high-level classifiers.
Table 2 reports the results obtained by the proposed technique on the data sets listed in Table 1. For comparison purposes, we evaluate the performance of the framework against different low-level classifiers: Bayesian networks [28], Weighted k-nearest neighbors [18], Multi-layer perceptrons (MLP) [32], Multi-class SVM (M-SVM) [1], [22]. Also, for each result, three different types of results are indicated as follows:
“Pure” row: only the low-level classifier is utilized (λ=0<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>). In this case, inside the parentheses are indicated the best parameters obtained from the optimization process for each technique;
“Networkless” row: a mixture of low- and high-level classifiers is employed. The value inside the parentheses indicates the best compliance term λ. Since the tourist walk is performed in a networkless environment, the tourist may visit any other site (data item) apart from those contained in the memory window μ;
“Network” row: the same setup as before, but the tourist walks are conducted on a networked environment. In this case, the tourist has to respect the graph topology, i.e., it can only visit vertices that are in the neighborhood of the visited vertex, and also not in the memory window μ. The values inside the parameters indicate the triple (∊k,∊,λ<math><mrow is="true"><mi is="true">k</mi><mtext is="true">,</mtext><mi is="true">∊</mi><mtext is="true">,</mtext><mi is="true">λ</mi></mrow></math>).
For the sake of clarity, take the first entry of Table 2. The pure low-level classifier Bayesian Networks achieved an accuracy rate of 57.8±2.6<math><mrow is="true"><mn is="true">57.8</mn><mo is="true">±</mo><mn is="true">2.6</mn></mrow></math> (λ=0<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>). However, if we use the proposed technique in a networkless environment, the accuracy rate is refined, achieving 58.6±2.3<math><mrow is="true"><mn is="true">58.6</mn><mo is="true">±</mo><mn is="true">2.3</mn></mrow></math> when λ=0.04<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0.04</mn></mrow></math>. Now, when the proposed technique is used in a network environment, the accuracy rate reaches 66.3±2.6<math><mrow is="true"><mn is="true">66.3</mn><mo is="true">±</mo><mn is="true">2.6</mn></mrow></math> when λ=0.28<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0.28</mn></mrow></math>. In general, the proposed technique is able to boost the accuracy rates of the data sets under analysis. Furthermore, we can see that the networked high-level classifier can outperform the networkless version.
Now, we verify if the results shown in Table 2 employing tourist walks in networks (“Network” row) are statistically significant from only using the low-level classifiers (“Pure” row). One should note that the results obtained by the hybrid classifier are always better or equal to the one obtained by the low-level classifier. This is because, on the occasion that the high-level classifier prejudices the classification, one can simply take λ=0<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, and keep only the low-level results. Therefore, the statistical significance test that we will employ here serves as a verifier of whether or not the enhancement brought by the high-level classifier introduction is really worthy. For this end, we apply the McNemar’s Chi-Square Test [26] with a significance level of α=0.05<math><mrow is="true"><mi is="true">α</mi><mo is="true">=</mo><mn is="true">0.05</mn></mrow></math> for each combination of “data set + technique” in Table 2. In general terms, this test is a non-parametric method used on nominal data to determine whether the row and column marginal frequencies are equal. It is applied to 2×2<math><mrow is="true"><mn is="true">2</mn><mo is="true">×</mo><mn is="true">2</mn></mrow></math> contingency tables with a dichotomous trait with matched pairs of subjects. We have highlighted in bold the results in the “Network” row which are statistically superior than the one only using the low-level classifier (“Pure” row). Out of the 32 simulations performed, 18 had enhancements that were statistically significant, confirming that the high-level classifier can be successfully applied to real-world data.
We close this section with a final note. One of the motivations of introducing a novel high-level based on tourist walks was that the original high-level classifier in [36] had difficulty in setting the weights for each of the networks measures. Moreover, once set, they were fixed along all the classes. In this work, we have devised a strategy for dynamically designing weights that were variable according to the classes of the problem. In order to measure the enhancement in relation to the original version, we compare the new results shown in Table 2 and [36, Table V] and verify that the majority of the results were improved in this new version. Another positive point of this version is the fewer number of parameters to be tuned in the training phase, because the weighting of the network measures – which encompasses many parameters to be tuned – is not necessary anymore.
In this section, the proposed high-level scheme is applied to a real-world task: handwritten digits recognition. The goal here is to show that the hybrid classification technique is able to perform well in real situations (as opposed to the latter section in which we focused on conveying the interesting model’s properties).
While recognizing individual digits is only one of a myriad of problems that involves specific designing of practical recognition systems, it still is, undoubtedly, an excellent benchmark for comparing shape recognition methods. For this task, the Modified NIST data set [20] is investigated, which is a well-known benchmark for testing handwritten digit recognition algorithms. In its standard form, this data set is composed of a training set with 60000<math><mrow is="true"><mn is="true">60</mn><mspace width="0.12em" is="true"></mspace><mn is="true">000</mn></mrow></math> samples and a test set with 10000<math><mrow is="true"><mn is="true">10</mn><mspace width="0.12em" is="true"></mspace><mn is="true">000</mn></mrow></math> samples. Each image is displayed in frames with 28×28<math><mrow is="true"><mn is="true">28</mn><mo is="true">×</mo><mn is="true">28</mn></mrow></math> pixels. In what concerns the proposed technique, the model’s selection phase here is identical to the one explained in the previous section.
In order to understand the impact of introducing the high-level term in the learning process, three low-level classifiers are investigated. The reason of choosing several low-level classifiers is to emphasize that the proposed technique is dependent on the choice of the low-level classifier. Moreover, we also aim at showing that, for suitable values of the compliance term, the overall accuracy of the model is enhanced when compared to only using the low-level term. The techniques that will be adopted as low-level ones are listed below:
A linear classifier implemented as an 1-layer neural network (perceptron). The same experimental setup given in [20] is used;
A k-nearest neighbor classifier with an Euclidean distance measure between input images (k=3<math><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>); and
A k-nearest neighbor classifier with a similarity function given by the weighted eigenvalue measure [36] is employed. The following meta-parameters for the aforementioned function are used: ϕ=4<math><mrow is="true"><mi is="true">ϕ</mi><mo is="true">=</mo><mn is="true">4</mn></mrow></math> and β function: β(x)=16expx3<math><mrow is="true"><mi is="true">β</mi><mo stretchy="false" is="true">(</mo><mi is="true">x</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mn is="true">16</mn><mi mathvariant="normal" is="true">exp</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mn is="true">3</mn></mrow></mfrac></mrow></mfenced></mrow></mrow></math>.
The strategy here to show the effectiveness of the proposed scheme is to display how the compliance term influences the overall accuracy rate of the selected model. Fig. 7 shows the performance of the three low-level techniques acting together with the high-level classifier in a networked environment. Our main goal here is to reveal that a mixture of low- and high-level classifier is able to increase the accuracy rate. For example, the perceptron alone reached 88% of accuracy rate, while a small increase in the compliance term was responsible for increasing the overall model’s accuracy rate to 91% (λ=0.2<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0.2</mn></mrow></math>). Regarding the k-nearest neighbor algorithm, for a pure traditional classifier, we obtained 95% of accuracy rate, against 97.6% when λ=0.25<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0.25</mn></mrow></math>. For the weighted eigenvalue measure, we have obtained 98% of accuracy rate when λ=0<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, against 99.1% when λ=0.2<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0.2</mn></mrow></math>. It is worth noting that the enhancement is significant. Even in the third case, the improvement is quite welcomed, because it is a hard task to increase an already very high accuracy rate. Moreover, these curves suggest that the compliance term may be intrinsic to the data set, since, for three completely distinct low-level classifiers, the maximum accuracy rate is reached in the surroundings of λ=0.225<math><mrow is="true"><mi is="true">λ</mi><mo is="true">=</mo><mn is="true">0.225</mn></mrow></math>. This property will be studied in a future work.
Now we compare how the accuracy rate behaves as we take different ensembles of classifiers. For example, in this combination, we can use a “low + low” or “low + high” classifiers. Table 3 shows all the possible two-by-two combinations using the three low-level classifiers that we discussed (linear, k-NN, eigenvalue) and the high-level classifier based on tourist walks. One can see that, in general, there is an enhancement in the accuracy rate when two low-level classifiers are used. However, the accuracy rate is clipped by the results obtained by the eigenvalue technique, since it is superior to the others. However, even when we use good techniques such as the based on eigenvalues with the high-level classifier, we can still boost the accuracy rate. For example, the best accuracy rate when only ensemble of low-level classifiers are used is 98.0±4.4<math><mrow is="true"><mn is="true">98.0</mn><mo is="true">±</mo><mn is="true">4.4</mn></mrow></math>. On the other hand, when we use low- and high-level classifiers, the best accuracy goes up to 99.1±3.1<math><mrow is="true"><mn is="true">99.1</mn><mo is="true">±</mo><mn is="true">3.1</mn></mrow></math>. This suggests that the high-level order of learning can help even in situations that the low-level classifier can perform really well.
In this section, we visually show how the classification occurs with samples of training instances drawn from the MNIST data set. The objective here is to show how the constructed networks look like for real training and test instances and, therefore, demonstrate how the high-level term can enhance the model’s prediction in real-world data sets.
For simplification matters, we opt to show the learning process only with two classes: digits ‘5’ and ‘6’. With this respect, Fig. 8a and b illustrate how the digit classification is carried out by using simple networks containing these small samples of digits.
Firstly, let us consider Fig. 8a, where the digits ‘5’ and ‘6’ surrounded by brown and blue boxes, respectively, represent the training set. The task is to classify the test instance represented by the digit in the red box. If only the low-level classification is applied, the test digit will probably be classified as a digit ‘6’, because there are more neighbors of digit ‘6’ than that of ‘5’ in the vicinity. On the other hand, if we also consider the class’ geometrical disposition (high-level classification), it is more suggestive that the referred test instance is a member of the digit ‘5’ class, because it complies more to the pattern formed by training digits of the class ‘5’ than to the one formed by the digits of the class ‘6’. In organizational terms, if the test digit is inserted into the class ‘5’ as displayed, it will just extend the somewhat formed horizontal “line” pattern. As a consequence, the inclusion of the test digit in this class will disturb (change) the class organization in a small extent, i.e., its representative descriptors (transient and cycle length) will not vary significantly. However, if the test digit is inserted into the class ‘6’, larger variations of the component measures will occur, since cycles are formed in the component. Taking into consideration that, before the insertion of the test instance, there were no cycles in the components, it is clear that the representative descriptors of the component representing the class ‘6’ will vary considerably by virtue of this abrupt change.
Fig. 9a and b exhibit the transient and cycle lengths, as well as their corresponding variations, as a function of μ, when the test instance is inserted into the component of digit ‘5’. As we expected, we see that the variations are very small in the class representing the digit ’5’, indicating and suggesting the strong compliance of the test digit with the pattern already formed by the representative graph component of the digit ’5’. On the other hand, Fig. 9c and d show the same information, when the test digit is put into the component of digit ‘6’. Here, we see that larger variations occur, which means that the test digit does not conform to the pattern formed by the representative component of the digit ‘6’.
Putting together these two observations, we can conclude that the high-level classifier will correctly classify the test instance as a digit ‘5’. The same reasoning can be applied to the digit network shown in Fig. 8b. In this case, the transient and cycle lengths as well as the corresponding variations are shown in Fig. 9e–h, when the test digit is inserted into the component of digit ‘5’ or ‘6’, respectively. Using the same arguments and aforementioned plots, in this situation, we can verify that the test instance is correctly classified as a digit ‘6’.
In this work, we have proposed an alternative and novel technique for data classification, which combines both low- and high-level characteristics of the data. The former classifies data instances by their physical features and the latter measures the compliance of the test instance with the pattern formation of the input data. To this end, tourist walks have been employed to capture the complex topological properties of the network constructed from the input data. A quite interesting feature of the proposed technique is that the high-level term’s influence has to be increased in order to get correct classification as the complexity of the class configuration increases. This means that the high-level term is specially useful in complex situations of classification. Also, it is worth observing that the application of the tourist walks dynamics in the context of high-level classifier is also a novel approach in the literature. We have shown that, even though such walk is constructed under very simple rules, it is still able to capture topological features of the underlying network in a local-to-global basis. In addition, we have uncovered a critical memory length, above which no change occurs in the transient and cycle lengths of the network component and also have related it to phase transition in the context of complex networks. Moreover, we have seen that the transient and cycle lengths are usually not correlated. Instead, they capture different topological aspects of the network in a complementary manner.
Overall, this paper has treated two open issues highlighted by Silva and Zhao [36] when constructing high-level classifiers: (1) the proper selection of network measures to compose the high-level predictor; and (2) the simplification of the model selection procedure. The first issue was dealt with by showing that the tourist walk process is able by itself capture local to global aspects of the networks by simply adjusting the walker’s memory length. The last issue was accomplished by using a statistical approach to calibrate the learning weights of the model.
We hope our work can provide an alternative way to the understanding of the high-level data semantics in the machine learning domain. As a future work, mechanisms for taking representative samples, rather than recalculating all the tourist walks for all the vertices in the network, are going to be considered. Moreover, more sophisticated network formation techniques, such as adaptive (class-dependent) k and ∊ will be investigated.