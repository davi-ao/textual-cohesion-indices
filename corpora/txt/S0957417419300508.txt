The research of social networks has received increasing attention over the last decade. Most traditional approaches of mining networked data are based on homogeneous network with the same node type. However, many practical applications are faced with a large-scale Heterogeneous Information Network (HIN) with multiple types of nodes and complex connection relationship (Sun et al., 2012a, Sun et al., 2011).
More recently, some research works are focused on mining a heterogeneous network, including classification and clustering tasks. Classification is one of the most fundamental and vital tasks and measuring the relatedness between objects represented as nodes in the network can improve the classification performance. Gupta, Kumar, and Bhasker (2017a)) proposes a meta-path based semi-metric measure for relevance measurement on objects. However, this approach fails to use weighted combination of various meta-paths to get better results and it is expensive to compute. An ensemble learning approach for multi-type classification is presented by Serafino, Pio, and Ceci (2018), but it requires a large size of sampling.
As the semantics expressed by those paths connecting different nodes are distinguished, we make use of the multiple semantic meta-paths to implement node classification in both academic and movie heterogeneous information networks. A meta-path is a sequence of object types, which is a powerful way to abstract the binary relationship between two objects in an HIN. The result denotes that our method performances better when more multiple semantic meta-paths are utilized. Especially, the improvement is significant when the labeled training dataset is small.
Another contribution of this work is proposing AWLHIN (active weight learning in HINs) method to assign weights automatically for different meta-paths. A good active learning strategy is essential for the large-scale HINs. Our approach can select objects wisely for which labels are sought according to the combination of feature matrixes and exploit it to build accurate classifiers for predictive purposes.
Recent research work has presented some new classification methods that work on heterogeneous information networks. There are several approaches which are proposed for object classification in traditional networks. Collective Classification (Neville and Jensen, 2007, Sen et al., 2008) is a general technique that classifies an object based on the labels of the object's neighbors in the network. LLGC (Zhou, Bousquet, LAL, & Weston, 2003) designs an affinity matrix and according to this matrix, objects that are close should be given the same class label. Although LLGC is designed to classify homogeneous network objects, we can apply it to do objects classification in HINs by disregarding object types.
Most of the real-world information networks are heterogeneous in their natural setting (Gupta et al., 2015, Shi et al., 2014). The network constituted by researchers, their published academic papers and other related information is a typical heterogeneous information network. Gupta et al. (2015) studies the problem of measuring relatedness between objects in a heterogeneous network using only link information. Shi et al. (2014) puts forward a novel method HeteSim to measure the relatedness of heterogeneous objects with the same type or different types. For scientific researchers, it is vital to find the interested research information and their similar researchers. Therefore, it is important to classify them quickly by different academic domain in the complex scientific research network.
There are two kinds of typical methods for classifying the nodes in HINs. One is to treat all of the nodes and links as the same type, the other is to separate different type of nodes and study the links between them independently (Huang et al., 2014). However, these methods ignore the correlation between different types of nodes which makes the important information lost.
The heterogeneity of nodes and complexity of links in an HIN make classification more difficult (Peters et al., 2012, Sun et al., 2008, Wang and Sukthankar, 2013, Wang and Tsotsos, 2016). Usually, the prediction targets are multi-label in HINs. Sun et al. (2008) proposes a hypergraph spectral learning formulation for multi-label classification, where a hypergraph is constructed to exploit the correlation information among different labels. Peters et al. (2012) facilitates the multi-label learning process by mining label correlations and instance correlations in heterogeneous networks. A dynamic label propagation model by considering both network topology and social context features is proposes by Sun et al. (2008) to improve the performance on multi-label classification. Further interesting examples can be found in studies proposed by Pio, Serafino, Malerba, and Ceci (2018), which is able to solve multi-type classification tasks in a heterogeneous network. Barracchia, Pio, Malerba, and Ceci (2017) presents a computational approach, based on heterogeneous clustering, which has the ability to predict possibly unknown lncRNA-disease relationships by analyzing complex heterogeneous networks consisting of several different biological entities types.
Ji, Sun, Danilevsky, Han, and Gao (2010) proposes a graph-based framework GNetMine to model the link structure in information networks with arbitrary network schema and arbitrary number of object or link type. The label prediction function is constructed for each object type. Furthermore, Ji, Han, and Danilevsky (2011) introduces a novel ranking-based iterative classification framework RankClass, which integrates classification and ranking in a simultaneous, mutually enhancing process. According to the idea that the higher ranking objects in the network play a more important role during the classification process, an iterative ranking model based on the network graph is established to calculate the object sorting distribution in each category. It strengthens the specific kind of subnets and weakens the rest of the network. During the iterative convergence process, the posterior probability is calculated to obtain the label prediction. It is observed that RankClass achieves better performance than GNetMine (Ji et al., 2010) and generates more accurate and robust results.
In the heterogeneous information network, meta-path is an effective way to capture the relationship between nodes (Sun et al., 2012). It connects two objects through a sequence of relations (Shi et al., 2015, Yu et al., 2014). Previously, Kong, Yu, Ding, and Wild (2012) proposes the Heterogeneous Collective Classification (HCC) method. It makes use of the correlations between the nodes with different type to predict the class label, which can be applied to the large-scale heterogeneous information network. Inspired by this idea, Yu et al. (2014) studies the entity recommendation problem in heterogeneous information networks, and the meta-path-based latent features are used to represent the connectivity between users and items. The relatedness measurement of object pairs based on the search path that connects two objects is proposes by Shi et al. (2015)).
Our classification approach differs from the above methods in that we utilize multi-semantic meta-path to further extract the relationships between objects. In order to solve the data sparseness problem in HINs when the labeled node set is small and the training data is insufficient, we build jump-paths to enrich the path information and the active learning strategy is applied to select the node to label. Also, the weights of different meta-paths are updated by use of the feature matrix in each iteration.
Due to the diversification of node types in a Heterogeneous Information Network (HIN), there are different link relationships between nodes. The node classification technique in HINs should take full advantage of these correlations for correct label prediction. The meta-path is extracted to denote the relationship between nodes and the path weight is computed by active learning approach. The network structure examples on DBLP, IMDB and YELP data are shown in Fig. 1. DBLP data contains author node A, academic paper node P, and source of paper publication node V. IMDB data contains actor node A, director node D, and movie node M. YELP data contains user node U, review node R, and business node B.
A meta-path contains nodes with different types and different semantic relation between them. As shown in Fig. 2(a), the meta-path A1P1A2 indicates that author A1 and A2 work together to publish the paper P1. Meta-paths with different length and different type of nodes represent multiple semantics. For example, in Fig. 2(a), nodes A3 and A4 can be connected by A3P3A5P4A4 and A3P2A4 respectively. The length of A3P2A4 is significantly shorter, and it correlates A3 with A4 more closely. The same situation reflected on IMDB and YELP datasets are shown in Fig. 2(b) and (c).
The important symbol definitions in an HIN are shown in Table. 1.
Definition 1

HIN. An HIN is defined as an undirected network G = (V, E), where V is the set of nodes and E is the set of edges. There are different types of nodes and relationships that express different semantics between nodes in HINs.
HIN. An HIN is defined as an undirected network G = (V, E), where V is the set of nodes and E is the set of edges. There are different types of nodes and relationships that express different semantics between nodes in HINs.
Definition 2

Meta-path. Different nodes are connected by the meta-path in HINs. Suppose k is a type of meta-path, P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> represents a meta-path which belongs to the type k. Two nodes in an HIN can be connected by different paths which express different semantics. The longer of the path indicates the weak semantic association usually.
Meta-path. Different nodes are connected by the meta-path in HINs. Suppose k is a type of meta-path, P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> represents a meta-path which belongs to the type k. Two nodes in an HIN can be connected by different paths which express different semantics. The longer of the path indicates the weak semantic association usually.
For example, meta-path A3P2A4 in Fig. 2(a) indicates that author A3 and A4 work together to publish paper P2.
Definition 3

Feature matrix. XP¯k=(x1,x2,…,xn)T<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mspace width="0.33em" is="true"></mspace><msup is="true"><mrow is="true"><mo is="true">(</mo><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">x</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi mathvariant="normal" is="true">x</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi mathvariant="normal" is="true">x</mi><mi is="true">n</mi></msub></mrow><mo is="true">)</mo></mrow><mi is="true">T</mi></msup><mspace width="0.33em" is="true"></mspace></mrow></math>represents the feature matrix based on meta-path P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> in an HIN. We extract the meta-path between nodes in an HIN and construct the feature matrix according to the weight of the path or other statistical information. For example, the feature matrix XP¯k=[x1,1⋯x1,n⋮xi,j⋮xn,1⋯xn,n]<math><mrow is="true"><msub is="true"><mi is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mrow is="true"><mo is="true">[</mo><mtable is="true"><mtr is="true"><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></mtd><mtd is="true"><mo is="true">⋯</mo></mtd><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mi is="true">n</mi></mrow></msub></mtd></mtr><mtr is="true"><mtd is="true"><mo is="true">⋮</mo></mtd><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub></mtd><mtd is="true"><mo is="true">⋮</mo></mtd></mtr><mtr is="true"><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></mtd><mtd is="true"><mo is="true">⋯</mo></mtd><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mi is="true">n</mi></mrow></msub></mtd></mtr></mtable><mo is="true">]</mo></mrow></mrow></math>, xi,j indicates the number of meta-paths P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> between the node i and node j.
Feature matrix. XP¯k=(x1,x2,…,xn)T<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mspace width="0.33em" is="true"></mspace><msup is="true"><mrow is="true"><mo is="true">(</mo><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">x</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi mathvariant="normal" is="true">x</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi mathvariant="normal" is="true">x</mi><mi is="true">n</mi></msub></mrow><mo is="true">)</mo></mrow><mi is="true">T</mi></msup><mspace width="0.33em" is="true"></mspace></mrow></math>represents the feature matrix based on meta-path P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> in an HIN. We extract the meta-path between nodes in an HIN and construct the feature matrix according to the weight of the path or other statistical information. For example, the feature matrix XP¯k=[x1,1⋯x1,n⋮xi,j⋮xn,1⋯xn,n]<math><mrow is="true"><msub is="true"><mi is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mrow is="true"><mo is="true">[</mo><mtable is="true"><mtr is="true"><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></mtd><mtd is="true"><mo is="true">⋯</mo></mtd><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mi is="true">n</mi></mrow></msub></mtd></mtr><mtr is="true"><mtd is="true"><mo is="true">⋮</mo></mtd><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub></mtd><mtd is="true"><mo is="true">⋮</mo></mtd></mtr><mtr is="true"><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></mtd><mtd is="true"><mo is="true">⋯</mo></mtd><mtd is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mi is="true">n</mi></mrow></msub></mtd></mtr></mtable><mo is="true">]</mo></mrow></mrow></math>, xi,j indicates the number of meta-paths P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> between the node i and node j.
Definition 4

Path weight. Different types of meta-paths have different length and represent different semantic relevance. We assign and learn the weight wP¯k<math><msub is="true"><mi mathvariant="normal" is="true">w</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub></math> for each type of the meta-paths P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math>.
Path weight. Different types of meta-paths have different length and represent different semantic relevance. We assign and learn the weight wP¯k<math><msub is="true"><mi mathvariant="normal" is="true">w</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub></math> for each type of the meta-paths P¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math>.
The data sparseness in HINs will arise when the labeled node set is small and the training data is insufficient. In order to enrich the path information, we build the jump-pathP˜<math><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">˜</mo></mover></math>which connects the same type of nodes with the same class label. As shown inFig. 3, the node P1and P2have the same label
and the jump-pathP˜1,2<math><msub is="true"><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">˜</mo></mover><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn></mrow></msub></math>is generated. It makes the node A1and A5to be connected by the jump-pathA1P˜1,2A5<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">A</mi><mn is="true">1</mn></msub><msub is="true"><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">˜</mo></mover><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn></mrow></msub><msub is="true"><mi mathvariant="normal" is="true">A</mi><mn is="true">5</mn></msub></mrow></math>. The extended HIN (EX-G) is built after adding the jump-path.
The enrichment by a jump-path strengthens the association between nodes in HINs, so the problem of data sparseness can be alleviated effectively.
There are different types of meta-paths in HINs and they denote multiple semantics which will have different impact on node classification. Therefore, weights of different types of meta-path should be evaluated. We propose the AWLHIN (active weight learning in HINs) method to learn the weight of meta-path.
The weight of meta-path is learned based on the similarity matrix of the corresponding path, which records the similarity of nodes linked by the meta-path.
For meta-path P¯L=VB…D<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub><mo is="true">=</mo><mtext is="true">VB</mtext><mo is="true">…</mo><mi mathvariant="normal" is="true">D</mi></mrow></math>, where V, B and D represent different types of nodes respectively, the source node Vg and target node Dh are connected by the path P¯L<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></math>. The similarity between node Vg and Dh under the path P¯L<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></math> is computed by Eqs. (1) and (2).(1)SimP¯L(Vg,Dh)=Rel(Vg,Dh|P¯L)<math><mrow is="true"><mi is="true">S</mi><mi is="true">i</mi><msub is="true"><mi is="true">m</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mtext is="true">Rel</mtext><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub><mo is="true">|</mo><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math>(2)Rel(Vg,Dh|P¯L)=x(Vg,Dh)(1deg(Vg)+1deg(Dh))1deg(Vg)∑jx(Vg,Dj)+1deg(Dh)∑ix(Vi,Dh)<math><mrow is="true"><mtext is="true">Rel</mtext><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub><mrow is="true"><mo stretchy="true" is="true">|</mo><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">x</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mi is="true">deg</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac><mo is="true">+</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mi is="true">deg</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow><mrow is="true"><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mi is="true">deg</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><mi is="true">x</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">g</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">+</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mi is="true">deg</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac><msub is="true"><mo is="true">∑</mo><mi is="true">i</mi></msub><mi is="true">x</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">h</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac></mrow></math>
Here, deg(Vg) represents the number of edges connected with node Vg in HINs, and x(Vg,Dh) represents the corresponding value in the feature matrix XP¯L<math><msub is="true"><mi mathvariant="normal" is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></msub></math>.
The similarity between the node Vi and Vj, which have the same node type V, is computed by Eq. (3). It is based on the symmetric meta-path P¯K=VB…D…BV<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">K</mi></msub><mo is="true">=</mo><mtext is="true">VB</mtext><mo is="true">…</mo><mi mathvariant="normal" is="true">D</mi><mo is="true">…</mo><mtext is="true">BV</mtext></mrow></math>. Here, the path is divided to sub-paths P¯L<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></math> and P¯R<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">R</mi></msub></math> by symmetry point node D. P¯L=VB…D<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub><mo is="true">=</mo><mtext is="true">VB</mtext><mo is="true">…</mo><mi mathvariant="normal" is="true">D</mi></mrow></math> and P¯R=P¯L−1=D…BV<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">R</mi></msub><mo is="true">=</mo><msup is="true"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><mo is="true">=</mo><mi mathvariant="normal" is="true">D</mi><mo is="true">…</mo><mtext is="true">BV</mtext></mrow></math>.(3)SimP¯K(Vi,Vj)=S→Vi|P¯L·S→Vj|P¯LS→Vi|P¯L2+S→Vj|P¯L2−S→Vi|P¯L·S→Vj|P¯L<math><mrow is="true"><mi is="true">S</mi><mi is="true">i</mi><msub is="true"><mi is="true">m</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">K</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">V</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub><mo is="true">·</mo><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">j</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub></mrow><mrow is="true"><msup is="true"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub></mrow><mn is="true">2</mn></msup><mo is="true">+</mo><msup is="true"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">j</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub></mrow><mn is="true">2</mn></msup><mo is="true">−</mo><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub><mo is="true">·</mo><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">j</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub></mrow></mfrac></mrow></math>where S→Vi|P¯L<math><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub></math> represents the similarity vector between source node Vi and the target node set D = {D1,D2,…, Dn} under meta-path P¯L=VB…D.<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub><mo is="true">=</mo><mtext is="true">VB</mtext><mo is="true">…</mo><mi mathvariant="normal" is="true">D</mi><mo is="true">.</mo></mrow></math> That is S→Vi|P¯L=Rel(Vi,{D1,D2,…,Dn}|P¯L=VB…D)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub><mo is="true">=</mo><mtext is="true">Rel</mtext><mrow is="true"><mo is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">i</mi></msub><mo is="true">,</mo><mrow is="true"><mo is="true">{</mo><mrow is="true"><msub is="true"><mi is="true">D</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">n</mi></msub></mrow><mo is="true">}</mo></mrow><mrow is="true"><mo is="true">|</mo></mrow><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub><mo is="true">=</mo><mtext is="true">VB</mtext><mo is="true">…</mo><mi mathvariant="normal" is="true">D</mi></mrow></mrow><mo is="true">)</mo></mrow></mrow></math> and S→Vj|P¯L=Rel(Vj,{D1,D2,…,Dn}|P¯L=VB…D)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">S</mi><mo is="true">→</mo></mover><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">j</mi></msub><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub></mrow></msub><mo is="true">=</mo><mtext is="true">Rel</mtext><mrow is="true"><mo is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">V</mi><mi is="true">j</mi></msub><mo is="true">,</mo><mrow is="true"><mo is="true">{</mo><mrow is="true"><msub is="true"><mi is="true">D</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">D</mi><mi is="true">n</mi></msub></mrow><mo is="true">}</mo></mrow><mrow is="true"><mo is="true">|</mo></mrow><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">L</mi></msub><mo is="true">=</mo><mtext is="true">VB</mtext><mo is="true">…</mo><mi mathvariant="normal" is="true">D</mi></mrow></mrow><mo is="true">)</mo></mrow></mrow></math>. The similarity matrix between different nodes is achieved, which is shown in Fig. 4.
The optimization of weight parameters for different meta-paths is defined as Eq. (4).(4)θ*=argminθ={θ1,θ2,…,θn}L(θ)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">θ</mi></mrow><mo is="true">*</mo></msup><mo is="true">=</mo><mi is="true">a</mi><mi is="true">r</mi><mi is="true">g</mi><mi is="true">m</mi><mi is="true">i</mi><msub is="true"><mi is="true">n</mi><mrow is="true"><mi is="true">θ</mi><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mrow is="true"><msub is="true"><mi is="true">θ</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">θ</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">θ</mi><mi is="true">n</mi></msub></mrow><mo stretchy="true" is="true">}</mo></mrow></mrow></msub><mi is="true">L</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">θ</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>where θ = {θ1,θ2,…, θn} corresponds to the weight parameter of the meta-path set P¯={P¯1,P¯2,…,P¯n}<math><mrow is="true"><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">¯</mo></mover><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">¯</mo></mover><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">¯</mo></mover><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">n</mi></msub></mrow><mo is="true">}</mo></mrow></mrow></math>.
The loss function of weight parameter is shown in Eq. (5). This method aims to reduce the weight of meta-path with weak node correlation and increase the weight of meta-path with strong node correlation.(5)L(θ)=12∑vi,vj∈vT∥1+sgn(vi,vj)−∑k=1nθkSimP¯k(vi,vj)∥22+λ2∥θ∥22<math><mtable displaystyle="true" is="true"><mtr is="true"><mtd columnalign="right" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="normal" is="true">L</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">θ</mi><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mn is="true">2</mn></mfrac><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="normal" is="true">v</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="normal" is="true">v</mi></mrow><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mrow is="true"><mi mathvariant="normal" is="true">v</mi></mrow><mi is="true">T</mi></msub></mrow></munder><msubsup is="true"><mrow is="true"><mo stretchy="true" is="true">∥</mo><mn is="true">1</mn><mo is="true">+</mo><mi mathvariant="italic" is="true">sgn</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo is="true">−</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></munderover><msub is="true"><mi is="true">θ</mi><mi is="true">k</mi></msub><mi mathvariant="italic" is="true">Si</mi><msub is="true"><mi is="true">m</mi><msub is="true"><mover accent="true" is="true"><mrow is="true"><mi mathvariant="normal" is="true">P</mi></mrow><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">∥</mo></mrow><mrow is="true"><mn is="true">2</mn></mrow><mn is="true">2</mn></msubsup><mo is="true">+</mo><mfrac is="true"><mi is="true">λ</mi><mn is="true">2</mn></mfrac><msubsup is="true"><mrow is="true"><mo stretchy="true" is="true">∥</mo><mi is="true">θ</mi><mo stretchy="true" is="true">∥</mo></mrow><mrow is="true"><mn is="true">2</mn></mrow><mn is="true">2</mn></msubsup></mrow></mtd></mtr></mtable></math>(6)sgn(vi,vj)={1,viandvjsharethesamelabel,vi≠vj−1,others<math><mtable displaystyle="true" is="true"><mtr is="true"><mtd columnalign="right" is="true"><mrow is="true"><mi mathvariant="italic" is="true">sgn</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mspace width="0.33em" is="true"></mspace><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mspace width="0.33em" is="true"></mspace><mi mathvariant="italic" is="true">and</mi><mspace width="0.33em" is="true"></mspace><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mspace width="0.33em" is="true"></mspace><mi mathvariant="italic" is="true">share</mi><mspace width="0.33em" is="true"></mspace><mi mathvariant="italic" is="true">the</mi><mspace width="0.33em" is="true"></mspace><mi mathvariant="italic" is="true">same</mi><mspace width="0.33em" is="true"></mspace><mi mathvariant="italic" is="true">label</mi><mo is="true">,</mo><mspace width="0.33em" is="true"></mspace><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">≠</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow></mstyle></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn><mo is="true">,</mo><mi mathvariant="italic" is="true">others</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow></mtd></mtr></mtable></math>where θk ≥ 0,   k = 1, 2, …, n. The node vi and vj have the classification label in the training set VT, λ is the regularization parameter and ‖ • ‖2 is the L2 norm.
The weight of each meta-path is derived from Eqs. (7) to (9) and normalized by Eq. (10).(7)∂L(θ)∂θk=0<math><mrow is="true"><mfrac is="true"><mrow is="true"><mi is="true">∂</mi><mi is="true">L</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">θ</mi><mo stretchy="true" is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">∂</mi><msub is="true"><mi is="true">θ</mi><mi is="true">k</mi></msub></mrow></mfrac><mo is="true">=</mo><mn is="true">0</mn><mspace width="0.33em" is="true"></mspace><mspace width="0.33em" is="true"></mspace></mrow></math>(8)θk=∑vi,vj∈VTsgn(vi,vj)Simp¯k(vi,vj)f(vi,vj)λ+∑vi,vj∈VTsgn(vi,vj)2Simp¯k(vi,vj)2<math><mrow is="true"><msub is="true"><mi is="true">θ</mi><mi is="true">k</mi></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></msub><mi is="true">s</mi><mi is="true">g</mi><mi is="true">n</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mi is="true">S</mi><mi is="true">i</mi><msub is="true"><mi is="true">m</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mi is="true">f</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">λ</mi><mo is="true">+</mo><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></msub><mi is="true">s</mi><mi is="true">g</mi><mi is="true">n</mi><msup is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow><mn is="true">2</mn></msup><mi is="true">S</mi><mi is="true">i</mi><msub is="true"><mi is="true">m</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><msup is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow><mn is="true">2</mn></msup></mrow></mfrac><mspace width="0.33em" is="true"></mspace><mspace width="0.33em" is="true"></mspace><mspace width="0.33em" is="true"></mspace><mspace width="0.33em" is="true"></mspace><mspace width="0.33em" is="true"></mspace></mrow></math>(9)f(vi,vj)=1+sgn(vi,vj)−∑r≠kθrSimp¯k(vi,vj)<math><mrow is="true"><mi is="true">f</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">1</mn><mo is="true">+</mo><mi is="true">s</mi><mi is="true">g</mi><mi is="true">n</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">−</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">r</mi><mo is="true">≠</mo><mi is="true">k</mi></mrow></munder><msub is="true"><mi is="true">θ</mi><mi is="true">r</mi></msub><mi is="true">S</mi><mi is="true">i</mi><msub is="true"><mi is="true">m</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math>(10)wp¯k=θk∑i=1nθi<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">w</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">θ</mi><mi is="true">k</mi></msub><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></msubsup><msub is="true"><mi is="true">θ</mi><mi is="true">i</mi></msub></mrow></mfrac></mrow></math>
Most of HINs are huge and obtaining the class labels of enough objects to form an effective training set is costly. We adopt the active learning strategy to select the objects for expanding the training set which can give more help to classify more objects accurately. If an object is highly correlated to many unlabeled objects and meanwhile, it is barely correlated to labeled objects, we will define this object as a good candidate. The objects in the candidate set should not be highly correlated, otherwise it is redundant in the training set. The candidate value of the object is calculated according to Eq. (11).(11)candidate(vi)=∑vj∈V−VTN[i,j]∑vj∈V−VTN[i,j]+∑vk∈VTN[i,k]<math><mrow is="true"><mi is="true">c</mi><mi is="true">a</mi><mi is="true">n</mi><mi is="true">d</mi><mi is="true">i</mi><mi is="true">d</mi><mi is="true">a</mi><mi is="true">t</mi><mi is="true">e</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi is="true">V</mi><mo is="true">−</mo><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></msub><mi is="true">N</mi><mrow is="true"><mo stretchy="true" is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo stretchy="true" is="true">]</mo></mrow></mrow><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi is="true">V</mi><mo is="true">−</mo><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></msub><mi is="true">N</mi><mrow is="true"><mo stretchy="true" is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo stretchy="true" is="true">]</mo></mrow><mo is="true">+</mo><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></msub><mi is="true">N</mi><mrow is="true"><mo stretchy="true" is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">k</mi></mrow><mo stretchy="true" is="true">]</mo></mrow></mrow></mfrac></mrow></math>
Here, N[i,j] indicates the relation between the node vi and vj. The value is set to 1 when they are connected by a meta-path. Otherwise, it is set to 0. We add the object with the highest candidate value to the training set and give it a class label. Thus, the training set in the HIN is expanded. The algorithm of active weight learning for each meta-path is shown in Table. 2.
The jump-path is a special kind of meta-path and it connects nodes with the same type and the same class label. This kind of path can enrich the correlation information in an HIN and alleviate the data sparseness problem with small labeled dataset. The weight learning of jump-path is different from that of common meta-path.
The optimization of the weight parameter for jump-path is shown in Eq. (12). It aims to maximize the parameter of jump-path with strong association and lower the parameter of jump-path with weak correlation.(12)O=maxθ∑vi,vj∈VT(sgn(vi,vj)*∑p˜k∈P˜θp˜kxp˜k[i,j])−λ∥θ∥22<math><mrow is="true"><mi mathvariant="normal" is="true">O</mi><mo is="true">=</mo><mi is="true">m</mi><mi is="true">a</mi><msub is="true"><mi is="true">x</mi><mi is="true">θ</mi></msub><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">s</mi><mi is="true">g</mi><mi is="true">n</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">*</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub><mo is="true">∈</mo><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">˜</mo></mover></mrow></munder><msub is="true"><mi is="true">θ</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></msub><msub is="true"><mi is="true">x</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo stretchy="true" is="true">]</mo></mrow></mrow><mo stretchy="true" is="true">)</mo></mrow><mo is="true">−</mo><mi is="true">λ</mi><msubsup is="true"><mrow is="true"><mo is="true">∥</mo><mi is="true">θ</mi><mo is="true">∥</mo></mrow><mn is="true">2</mn><mn is="true">2</mn></msubsup></mrow></math>where VT represents the training dataset of node, λ is the regularization parameter, and function sgn(vi,vj) is computed by Eq. (6). xp˜k[i,j]<math><mrow is="true"><msub is="true"><mi is="true">x</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></msub><mrow is="true"><mo is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo is="true">]</mo></mrow></mrow></math> denotes the value between node vi and vj in feature matrix for jump-path p˜k<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></math>. The parameter θp˜k<math><msub is="true"><mi is="true">θ</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></msub></math> of the jump-path p˜k<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></math> is obtained by Eq. (13).(13)θp˜k=12λ∑vi,vjϵVT(sgn(vi,vj)xp˜k[i,j])<math><mrow is="true"><msub is="true"><mi is="true">θ</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mn is="true">2</mn><mi is="true">λ</mi></mrow></mfrac><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mi is="true">ϵ</mi><msub is="true"><mi is="true">V</mi><mi is="true">T</mi></msub></mrow></munder><mrow is="true"><mo is="true">(</mo><mi is="true">s</mi><mi is="true">g</mi><mi is="true">n</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><msub is="true"><mi is="true">x</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">k</mi></msub></msub><mrow is="true"><mo stretchy="true" is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo stretchy="true" is="true">]</mo></mrow><mo is="true">)</mo></mrow></mrow></math>
The weights of meta-path and jump-path in the HIN are normalized by Eq. (14) uniformly.(14)wp¯k=θk∑i=1nθP¯i+∑j=1mθp˜j<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">w</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">θ</mi><mi is="true">k</mi></msub><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></msubsup><msub is="true"><mi is="true">θ</mi><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></msub><mo is="true">+</mo><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">m</mi></msubsup><msub is="true"><mi is="true">θ</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">j</mi></msub></msub></mrow></mfrac></mrow></math>
Here, p¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> denotes a meta-path P¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> or a jump-path p˜j<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">˜</mo></mover><mi is="true">j</mi></msub></math>, and θk represents the corresponding parameter. The active strategy is also adopted similarly shown in Table. 2.
The nodes in HINs are connected by meta-paths. The feature matrix is constructed for each type of meta-paths and the classifier is trained for node classification. The node classification structure in HINs is shown in Fig. 5.
The feature matrix between nodes is constructed based on a single meta-path. For the meta-path p¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math>, the n*n feature matrix Xp¯k<math><msup is="true"><mi is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msup></math> is built by the connection information in an HIN. As shown in Fig. 6, the value is 0 if there is no meta-path p¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> between two nodes; otherwise, the value is the number of meta-path p¯k<math><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></math> between them. The matrix is symmetric such as Xp¯k[i,n]=Xp¯k[n,i]=p<math><mrow is="true"><msup is="true"><mi is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msup><mrow is="true"><mo is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">n</mi></mrow><mo is="true">]</mo></mrow><mo is="true">=</mo><mspace width="0.33em" is="true"></mspace><msup is="true"><mi is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msup><mrow is="true"><mo is="true">[</mo><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mi is="true">i</mi></mrow><mo is="true">]</mo></mrow><mspace width="0.33em" is="true"></mspace><mo is="true">=</mo><mspace width="0.33em" is="true"></mspace><mi is="true">p</mi></mrow></math> and the value is 0 between the same node, that is Xp¯k[i,j]=0(i=j)<math><mrow is="true"><msup is="true"><mi is="true">X</mi><msub is="true"><mover accent="true" is="true"><mi is="true">p</mi><mo is="true">¯</mo></mover><mi is="true">k</mi></msub></msup><mrow is="true"><mo is="true">[</mo><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo is="true">]</mo></mrow><mo is="true">=</mo><mn is="true">0</mn><mspace width="0.33em" is="true"></mspace><mrow is="true"><mo is="true">(</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mi is="true">j</mi></mrow><mo is="true">)</mo></mrow></mrow></math>.
There are different types of meta-paths in HINs and also the corresponding feature matrix is constructed for each of them. The classification label of the node in HINs will be predicted by the combination of these feature matrixes. The linear weighting method is used to get the combinatorial matrix, which is computed by Eq. (15).(15)X=∑k=1nwp¯k*Xp¯k=wp¯1*[x1,1⋯x1,n⋮xi,j⋮xn,1⋯xn,n]p¯1+⋯+wp¯n*[x1,1′⋯x1,n′⋮xi,j′⋮xn,1′⋯xn,n′]p¯n<math><mtable displaystyle="true" is="true"><mtr is="true"><mtd columnalign="right" is="true"><mi is="true">X</mi></mtd><mtd is="true"><mo is="true">=</mo></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mrow is="true"><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></munderover><msub is="true"><mrow is="true"><mi mathvariant="normal" is="true">w</mi></mrow><msub is="true"><mover is="true"><mrow is="true"><mi is="true">p</mi></mrow><mo stretchy="true" is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">*</mo><msub is="true"><mrow is="true"><mi mathvariant="normal" is="true">X</mi></mrow><msub is="true"><mover is="true"><mrow is="true"><mi is="true">p</mi></mrow><mo stretchy="true" is="true">¯</mo></mover><mi is="true">k</mi></msub></msub><mo is="true">=</mo><msub is="true"><mi is="true">w</mi><msub is="true"><mover is="true"><mrow is="true"><mi is="true">p</mi></mrow><mo stretchy="true" is="true">¯</mo></mover><mn is="true">1</mn></msub></msub><mo is="true">*</mo><msub is="true"><mrow is="true"><mo stretchy="true" is="true">[</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋯</mo></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mi is="true">n</mi></mrow></msub></mstyle></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋮</mo></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋮</mo></mstyle></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mn is="true">1</mn></mrow></msub></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋯</mo></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mi is="true">n</mi></mrow></msub></mstyle></mtd></mtr></mtable><mo stretchy="true" is="true">]</mo></mrow><msub is="true"><mover is="true"><mrow is="true"><mi is="true">p</mi></mrow><mo stretchy="true" is="true">¯</mo></mover><mn is="true">1</mn></msub></msub><mo is="true">+</mo><mo is="true">⋯</mo></mrow></mstyle></mtd></mtr><mtr is="true"><mtd is="true"></mtd><mtd is="true"></mtd><mtd columnalign="left" is="true"><mrow is="true"><mo is="true">+</mo><mspace width="0.16em" is="true"></mspace><msub is="true"><mi is="true">w</mi><msub is="true"><mover is="true"><mrow is="true"><mi is="true">p</mi></mrow><mo stretchy="true" is="true">¯</mo></mover><mi is="true">n</mi></msub></msub><mo is="true">*</mo><msub is="true"><mrow is="true"><mo stretchy="true" is="true">[</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msubsup is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mn is="true">1</mn></mrow><mo is="true">′</mo></msubsup></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋯</mo></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msubsup is="true"><mi is="true">x</mi><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo><mi is="true">n</mi></mrow><mo is="true">′</mo></msubsup></mstyle></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋮</mo></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msubsup is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mo is="true">′</mo></msubsup></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋮</mo></mstyle></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msubsup is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mn is="true">1</mn></mrow><mo is="true">′</mo></msubsup></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><mo is="true">⋯</mo></mstyle></mtd><mtd columnalign="left" is="true"><mstyle scriptlevel="0" displaystyle="true" is="true"><msubsup is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">n</mi><mo is="true">,</mo><mi is="true">n</mi></mrow><mo is="true">′</mo></msubsup></mstyle></mtd></mtr></mtable><mo stretchy="true" is="true">]</mo></mrow><msub is="true"><mover is="true"><mrow is="true"><mi is="true">p</mi></mrow><mo stretchy="true" is="true">¯</mo></mover><mi is="true">n</mi></msub></msub></mrow></mtd></mtr></mtable></math>
The training set VT = {(v1,y1),(v2,y2),…, (vn,yn)}T, where vi = (vi1,vi2,…, vin) and yi is the label for node vi. We use Random-Forest to predict the class label of the node. The algorithm selects n samples back and forth from the training set and r features are selected according to the Gini coefficient, which measures the inequality among values of a frequency distribution. The higher degree of the data mixture, the higher of Gini coefficient and it is computed by Eq. (16).(16)Gini=1−∑i=1mP(i)2<math><mrow is="true"><mtext is="true">Gini</mtext><mo is="true">=</mo><mn is="true">1</mn><mo is="true">−</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">m</mi></munderover><mi is="true">P</mi><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">i</mi><mo stretchy="true" is="true">)</mo></mrow><mn is="true">2</mn></msup></mrow></math>where P(i) represents the probability distribution for the class type i. The Gini coefficient has the lowest value of 0 when the dataset VT has only one data type with m value of 1.
The selected features are used to build a decision tree, and the selection of samples and features is repeated to generate t decision trees to form the random forest. The process is shown in Fig. 7. There are 6 test samples V1, V2, ... and V6 used for label prediction.
We use three datasets DBLP (http://dblp.uni-trier.de/db/), IMDB (http://www.imdb.com/) and YELP (https://www.yelp.com/dataset/challenge) to build HINs and evaluate our method further.
DBLP data. It is an academic information network. The objects related to four research areas, including database, data mining, information retrieval and artificial intelligence, are extracted for the experiment setup. The dataset contains 10,728 papers (P), 20 publication venues (V) and 7538 authors (A). The meta-path APA, APAPA, APVPA and AP˜A<math><mrow is="true"><mrow is="true"><mi mathvariant="normal" is="true">A</mi><mover accent="true" is="true"><mi mathvariant="normal" is="true">P</mi><mo is="true">˜</mo></mover></mrow><mi is="true">A</mi></mrow></math> are used to classify authors by research areas.
IMDB data. It is the well-known database about movies which is published by the GroupLens research group. The dataset contains 11,430 actors (A), 5612 movies (M) and 3092 directors (D). The meta-path AMA, AMAMA, AMDMA and AM˜A<math><mrow is="true"><mrow is="true"><mi mathvariant="normal" is="true">A</mi><mover accent="true" is="true"><mi mathvariant="normal" is="true">M</mi><mo is="true">˜</mo></mover></mrow><mi is="true">A</mi></mrow></math> are used to classify the actors by different genres, such as comedy, thriller and action.
YELP data. It is a website talking about business activities. The dataset contains 15,930 users (U), 22,706 reviews (R) and 1370 businesses (B). The meta-path URU, URURU, URBRU and UR˜U<math><mrow is="true"><mi mathvariant="normal" is="true">U</mi><mover accent="true" is="true"><mi mathvariant="normal" is="true">R</mi><mo is="true">˜</mo></mover><mi mathvariant="normal" is="true">U</mi></mrow></math> are used to classify the users according to the different activities they participate in.
For the node classification task in HINs, the feature matrix is generated for different meta-paths which denote multiple semantic correlations. In order to verify the effectiveness of the meta-path based approach for node classification in HINs, five different classifiers are adopted for comparison, namely Random-Forest, Naive-Bayes, K-Neighbors, SVM, and Neural Network. The results on DBLP, IMDB and YELP datasets with different labeled data size are shown in Table. 3.
As it can be observed from Table 3, with the increasing of labeled data size, accuracy of several classifiers is improved. The accuracy of K-Neighbors on DBLP reaches 96.42% when the labeled data increases from 2% to 8% and the improvement is nearly more than 28%. The objects of IMDB and YELP are harder to classify for their complex node relationship, which is reflected by the classifier accuracy values. The best result is 70.98% for IMDB and 70.05% for YELP.
ROC curves of true/false positive rate derived from different datasets are shown in Fig. 8. It is also shown that the classification performance on DBLP (with the largest AUC) is better than IMDB and YELP for predicting class labels.
Multiple paths will benefit for the classification when the labeled training data is small. An HIN can be enriched by adding jump-paths, such as AP˜A<math><mrow is="true"><mi is="true">A</mi><mover accent="true" is="true"><mi is="true">P</mi><mo is="true">˜</mo></mover><mi is="true">A</mi></mrow></math> on DBLP, AM˜A<math><mrow is="true"><mi is="true">A</mi><mover accent="true" is="true"><mi is="true">M</mi><mo is="true">˜</mo></mover><mi is="true">A</mi></mrow></math> on IMDB and UR˜U<math><mrow is="true"><mi is="true">U</mi><mover accent="true" is="true"><mi is="true">R</mi><mo is="true">˜</mo></mover><mi is="true">U</mi></mrow></math> on YELP. The comparison results after incorporating the jump-path are shown in Fig. 9. Nodes in the training set are labeled by 2%, 4%, 6%, 8% and 10% respectively.
As it can be seen from Fig. 9, for small labeled data size, the performance improvement is significant normally after combining the jump-path, which indicates that the jump-path enriches the association between nodes in HINs. The accuracy based on APA+AP ̃A on DBLP is improved by nearly 14% with the comparison of APA under 2% labeled data. Moreover, it is found that combination of multiple meta-paths achieves the best performance on these three datasets shown Fig. 9(c), (f) and (i). More nodes are correlated by multiple meta-paths in order to alleviate data sparseness, which is also a main reason why the improvement by using a jump-path is not very obvious on these path combinations.
It is assumed that a budget B is given, which represents the number of selected nodes during active learning process for labeling. There are n nodes selected to label per iteration and AWLHIN iterates B/n times. After each iteration, the classifiers are re-trained. We compare the performance of our AWLHIN method with another two active learning strategies on DBLP, IMDB and YELP datasets. Uncertainty Sampling (US) (Holub, Perona, & Burl, 2008) is performed to select n nodes for labeling on all unlabeled nodes. Label Selection based on Clustering (LSC) (Pfeffermann, 2009) method selects n / Nc unlabeled nodes randomly to label within Nc clusters.
The classification accuracies under different active learning strategies on DBLP, IMDB and YELP are shown in Fig. 10(a)–(c). We set n = 10 for DBLP and n = 50 for IMDB and YELP as the budget B varies to see the classification accuracy. The initial training dataset contains 2% labeled nodes.
As the results shown in both Fig. 10(a)–(c), classification accuracy is improved with the increasing of budget, because the training data is expanded by more labeled nodes. Moreover, it can be seen that among the three active learning strategies, AWLHIN outperforms LSC and US whatever budget changes, which proves that learning the weight of meta-path is useful. This indicates that AWLHIN has the ability to adjust to the most effective meta-paths in HINs.
As a further evaluation, we also compare the performance of our AWLHIN method with LLGC (Gupta et al., 2017b, Zhou et al., 2003), wvRN (weighted-vote Relational Neighbor) (Macskassy & Provost, 2008), HENPC (Pio et al., 2018) and RandomMT-MrSBC (Serafino et al., 2018), which are shown in Table. 4. The t test results are shown in Table. 5.
As it can be seen from Table. 4, our method AWLHIN generally achieves a better performance than both LLGC and wvRN approach with different labeled data size, which indicates that the improvement brought by the active learning method are additive. RandomMT-MrSBC gets higher accuracy when labeled data increases to 8% on IMDB and YELP datasets, whereas the accuracy of AWLHIN is the highest when only 2% labeled data is kept. Compared with HENPC and RandomMT-MrSBC using meta-path, AWLHIN uses a simpler way on classification tasks and gets a better classification result with fewer labeled data.
The important strategy of the proposed method is based on the multi-semantic meta-path, especially the jump-path. It is found that more nodes are correlated by the combination of multiple meta-paths, which achieves the best performance on three datasets. And the data sparseness problem in HINs has been alleviated. Other meta-path based methods, such as LLGC and wvRN, can employ only one kind of meta-path each time, while our method outperforms them, which has been shown in Tables. 4 and 5. Moreover, the performance improvement is significant normally after combining the jump-path when the labeled data size is small, which indicates that the jump-path enriches the association between nodes in HINs. The results also confirm one of the main assumption which guided our work: a strong relationship between objects by a jump-path possibly implies that their class labels are correlated. On the other hand, the improvement by jump-path is not obvious with more meta-paths and more labeled data, because data sparseness has been alleviated before using jump-path.
Most of HINs are huge and obtaining the class labels of enough objects is costly. To solve this problem, we adopt the active learning strategy to select the objects for expanding the training set. The candidate selection rule is designed and it outperforms the other active learning strategies LSC and US whatever budget changes, which is shown in Fig. 10. The results indicate that our method AWLHIN requires a much smaller budget to achieve the same classification accuracy. The savings are vital for HINs with complex node relationships. So when having a tight labeling budget, it is important to have a good active learning component. Five different classifiers are employed to verify the effectiveness of the proposed method. The accuracy is improved with the increasing of labeled data size. It obtains good classification results on different datasets, which proves that our method does not depend on classifiers.
Furthermore, the comparisons with recent research of HENPC (Pio et al., 2018) and RandomMT-MrSBC (Serafino et al., 2018) also prove that our method AWLHIN gets better classification results with fewer labeled data. Instead of computing the strength of the relationship among nodes of different types, building multi-type cliques and hierarchy of clusters, AWLHIN uses a simpler way on classification tasks by extracting jump-path and active learning strategy to select the candidate objects which are helpful for model training.
Heterogeneous information network contains different node types and link relationships. The meta-path connecting nodes provides important indications for their correlations. However, the data sparseness in HINs is still a problem when the labeled node set is small and the training data is insufficient. To alleviate this problem, we build the jump-path to enrich the path information.
We make use of multiple semantic meta-paths to implement node classification in the heterogeneous information network. The semantic of different meta-paths are varied and they have different impact for node classification in HINs. The feature matrix of different meta-paths are built and used for meta-path weight learning. The larger of the weight value for a meta-path, the more important for correct nodes classification. The experimental results on DBLP, IMDB and YELP dataset show that the better performance is achieved when more meta-paths are utilized which can enrich the association between the nodes in an HIN. Specially, the improvement by using a jump-path is significantly when the labeled training dataset is small. Moreover, our method AWLHIN outperforms previous work LLGC and wvRN with different labeled data size and various meta-paths, and gets a higher accuracy than HENPC and RandomMT-MrSBC when the amount of labeled data is small.
The proposed approach can also be implemented in social networks such as twitter which contains multiple node types and links. We will discover more kinds of meta-path further to mine the node relations in HINs and also the analysis of the content in HINs will be combined for node classification task.
Y. Du proposed the idea and designed the model, and drafted the manuscript. W. Guo improved the model and gave the formal analysis, and revised the manuscript. J. Liu implemented the idea of active learning strategy for node classification. C. Yao reviewed the manuscript and gave valuable advice on how to improve it. All authors read and approved the final manuscript.
This work is supported by the National Key R&D Program of China under grant no. 2018YFC1900800, Research Program of State Language Commission under grant no. YB135-89 and Rich Media Digital Publishing and Knowledge Service Key Laboratory.