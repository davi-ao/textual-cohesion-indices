The word sense disambiguation (WSD) task has been widely studied in the field of Natural Language Processing (NLP) [31]. This task is defined as the ability to computationally detect which sense is being conveyed in a particular context [37]. Although humans solve ambiguities in an effortlessly manner, this matter remains an open problem in computer science, owing to the complexity associated with the representation of human knowledge in computer-based systems [30]. The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [52], word processing [19], information retrieval and extraction [21], [22], [32], [47], [49], [56]. In addition, the resolution of ambiguities plays a pivotal role in the development of the so-called semantic web [13].
Many approaches devised to solve ambiguities in texts employ machine learning methods, in which systems using supervised methods represent the state-of-the-art [37]. These methods usually rely on features extracted from the context of ambiguous (target) words, making contextual information a primordial element in the disambiguation process. However, the learning process in most of these methods only use representations that attempt to grasp the context and little or no explicit modeling of context is made. In this paper, we propose a new representation that explicitly models context and may be used as a underlying structure in the learning process. The representation used here consists of a bipartite network composed only of target and context words, while learning is carried out by a gradient descent method, which learns the relationship between the two types of words, allowing the induction of a model capable of performing supervised WSD.
Although networks/graphs have been employed in general pattern recognition methods [15], [16], [54] and, particularly in the analysis of the semantical properties of texts in several ways [3], [7], [11], [28], [33], [35], [43], [50], the use of network models in the learning process has been restricted to a few works (see e.g. [44]). In fact, most of the current network models emphasize the relationship between all words of the document. As a consequence, a minor relevance has been given to the relationships between feature and target words. As we shall show, the adopted representation/learning method may improve the classification process when compared with well-known traditional/general purpose supervised algorithms hinging on traditional text representations. Remarkably, we have found that our method retains its discriminative power even when a considerable small amount of training instances is available. These results may indicate that the adopted method is an ideal candidate for state-of-the-art methods such as IMS [55], which at its core make use of traditional machine learning methods such as Support Vector Machines. We also applied our algorithm to two popular benchmarks in the area of WSD, namely Senseval-3 English Lexical Sample Task [34] and SemEval-2007 Task 17 English Lexical Sample [40]. Despite of making use of a simple superficial textual representation, in both datasets our method achieved intermediary positions.
The remainder of this paper is organized as follows. We first present a brief review of basic concepts employed in this paper and related works. We then present the details of the representation and algorithm used to tackle the word sense disambiguation task. The details of the experiments and the results concerning the accuracy and robustness of the method is also discussed. Finally, we present some perspectives for further works.
The word sense disambiguation task can be defined as follows. Given a document represented as a sequence of words T={w1,w2,…,wn},<math><mrow is="true"><mi is="true">T</mi><mo is="true">=</mo><mo is="true">{</mo><msub is="true"><mi is="true">w</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">n</mi></msub><mo is="true">}</mo><mo is="true">,</mo></mrow></math> the objective is to assign appropriate sense(s) to all or some of the words wi ∈ T. In other words, the objective is to find a mapping A from words to senses, such that A(wi)⊆SD(wi),<math><mrow is="true"><mi is="true">A</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">⊆</mo><msub is="true"><mi mathvariant="script" is="true">S</mi><mi is="true">D</mi></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math> where SD(wi)<math><mrow is="true"><msub is="true"><mi mathvariant="script" is="true">S</mi><mi is="true">D</mi></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow></mrow></math> is the set of senses encoded in a dictionary D for the word wi, and A(wi) is the subset of appropriate senses of wi ∈ T. One of the most popular approaches to tackle the WSD problem is the use of machine learning, since this task can be seen as a supervised classification problem, where senses represent the classes [37]. The attributes used in the learning methods are usually any informative evidence obtained from context and external knowledge sources. The latter approach is usually not common in practice because the creation of knowledge datasets demands a time-consuming effort, since the change in domains requires the recreation of new knowledge bases.
The generic WSD task can be distinguished into two types: lexical sample and all-words disambiguation. In the former, a WSD system is required to disambiguate a restricted set of target words. This is mostly done by supervised classifiers [37]. In the all-words scenario, the WSD system is expected to disambiguate all open-class words in a text. This task usually requires a wide-coverage of domains, and for this reason a knowledge-based system is usually employed. In this article, only the lexical sample task is considered.
The main step in any supervised WSD system is the representation of the context in which target words occur. The set of features employed typically are chosen to characterize the context in a myriad of forms [37]. The most common types of attributes used for this aim are:
local features: the features of an ambiguous concept are a small number of words surrounding target words. The number of words representing the context is defined in terms of the window size ω. For example, if the context of the target word τω is “p−3<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">−</mo><mn is="true">3</mn></mrow></msub></math> p−2<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msub></math> p−1<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msub></math> τω p+1<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> p+2<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">+</mo><mn is="true">2</mn></mrow></msub></math> p+3<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">+</mo><mn is="true">3</mn></mrow></msub></math>” and ω=2,<math><mrow is="true"><mi is="true">ω</mi><mo is="true">=</mo><mn is="true">2</mn><mo is="true">,</mo></mrow></math> then the words p−2,<math><mrow is="true"><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">−</mo><mn is="true">2</mn></mrow></msub><mo is="true">,</mo></mrow></math> p−1,<math><mrow is="true"><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo></mrow></math> p+1<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> and p+2<math><msub is="true"><mi is="true">p</mi><mrow is="true"><mo is="true">+</mo><mn is="true">2</mn></mrow></msub></math> are used as features.
topical features: the features are defined as topics of a text or discourse, usually denoted in a bag-of-words representation;
syntatical features: the features are syntactic cues and argument-head relations between the target word and other words within the same sentence; and
semantical features: the features of a word are any semantic information available, such as previously established senses or domain indicators.
Using the aforementioned set of features, each word occurrence can be converted to a feature vector, which in turn is used as input in supervised classification algorithms. Typical classifiers employed for this task include decision trees [36], bayesian classifiers [20], [36], neural networks [36] and support vector machines [20], [26]. A well known state of the art system that uses a combination of the presented features is the “It Makes Sense” (IMS) method [55], which uses Support Vector Machines as the standard classifier. This system also makes use of attributes derived from knowledge bases, allowing its application in both all-words and lexical sample tasks.
Another approach that has been used to address the WSD problem consists in the use of complex networks [6], [38], [45], [53] and graphs [35]. For instance, the HyperLex algorithm [50] connects words co-occurring in paragraphs to establish similarity relations among words appearing in the same context. The frequency of co-occurrences is considered according to the following weighting scheme: (1)wij=1−max{P(wi,wj),P(wj,wi)}<math><mrow is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">max</mi><mrow is="true"><mo is="true">{</mo><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo is="true">,</mo><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">}</mo></mrow></mrow></math>where P(wi,wj)=fij/fi,<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mi is="true">f</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">/</mo><msub is="true"><mi is="true">f</mi><mi is="true">i</mi></msub><mo is="true">,</mo></mrow></math> fi is the frequency of word i in the document and fij is the frequency of the co-occurrence of the words i and j. Then, this network is used to create a tree-like structure via recognition of central concepts, which represent all possible senses. To perform the classification, the distance of context words to the central concepts in the tree structure is computed to identify the most likely sense.
Using a different approach, [9] uses the local topological properties of co-occurrence networks to disambiguate target words. In this case, even though a significant performance has been found for particular target words, the optimal discrimination rate was obtained with traditional local features, suggesting thus that the overall discriminability could be improved upon combining features of distinct nature, as suggested by similar approaches [5], [51].
Despite the numerous studies devoted to the WSD problem, this task remains an open problem in NLP, and currently it is considered one of the most complex problems in Artificial Intelligence [30]. Our contribution in this paper is the proposition of a new representation that explicitly models context that is used to perform sense discrimination. Unlike previous studies [9], [50], the learning process takes place in the same structure used for representation, eliminating the need of hand-designed features. Despite its seemingly simplicity, we show that such representation captures, in a artlessly manner, informative properties of target words and their respective senses.
This section presents the approaches to represent the context of target words in a bipartite heterogeneous network. Here we also present the Inductive Model Based on Bipartite Heterogeneous Network (IMBHN) algorithm, which is responsible for inducing a classification model from the structure of a bipartite network [42], [46].
Traditionally, the context of ambiguous words is represented in a vector space model, so that each target word is characterized by a vector. In this representation, each dimension of the vector corresponds to a specific feature. Alternatively, we may represent the data using a bipartite heterogeneous network. In this model, while the first layer comprises only feature words, the second only stores target words. In this paper, we focused on the analysis of local and topical attributes in the form of context, as such data are readily available on (or derivable from) any corpus. Note that, in this case, we have not used any knowledge dataset.
In the proposed strategy based on topical features, we create a set T<math><mi mathvariant="script" is="true">T</mi></math> of topical words. Then, each one becomes a distinct feature. As topical words, we considered the most frequent words of the dataset. The number of topical words, i.e. |T|,<math><mrow is="true"><mo is="true">|</mo><mi mathvariant="script" is="true">T</mi><mo is="true">|</mo><mo is="true">,</mo></mrow></math> is a free parameter. Given T,<math><mrow is="true"><mi mathvariant="script" is="true">T</mi><mo is="true">,</mo></mrow></math> the bipartite network is created by establishing a link between topical and target words whenever they co-occur in the same document.
In the proposed representation based on local features, each feature word surrounding the target word represents an attribute. For each instance of the target word in the text, we select the ω closest surroundings words to become a feature word (see definition in “Related works” section). The selected words are then connected to the target words by weighted edges.
The IMBHN algorithm can be used in the context of any text classification task. If the objective is to classify distinct documents in a given number of classes, the bipartite network can be constructed so that nodes represent both terms and documents. In this general scenario, such representation is used to compute the relevance of specific terms for distinct document classes. In a similar fashion, in this study, we compute the relevance of local/topical features for each target word. Then, this relevance is used to infer word senses.
The algorithm employed for sense identification relies upon a network structure with two distinct layers: (i) a layer representing possible feature words (i.e. local or topical features), and (ii) a layer comprising all occurrences of the target word. The two layers are illustrated in Fig. 1. Edges are established across layers so that context words and distinct occurrences of the target word are connected. In addition, in the network representation, a weight relating each feature word to each target word is also established. The main components of the model are:
wdk,ti<math><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub></math>: the weight of the connection linking the kth target word and the ith feature word. In the strategy based on topical features, this weight is constant along the execution of the algorithm and, for a given instance T, is computed as (2)wdk,ti=1−δ(dk,ti)/l(T),<math><mrow is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">−</mo><mi is="true">δ</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">/</mo><mi is="true">l</mi><mrow is="true"><mo is="true">(</mo><mi is="true">T</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>where δ(dk, ti) denotes the distance between two words (i.e. the number of intermediary words) and l(T) is the length of T (measured in terms of word counts). In the strategy based on local features, the weight of the links is given by the term frequency - inverse document frequency (tf-idf) strategy [31].
fti,cj<math><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math>: let C<math><mi mathvariant="script" is="true">C</mi></math> be the set of possible classes (i.e. word senses). fti,cj<math><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math> represents the current relevance of the ith feature word (ti∈T<math><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">T</mi></mrow></math>) to the jth class (cj∈C<math><mrow is="true"><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">C</mi></mrow></math>). This value is initialized using a heuristic and then is updated at each step of the algorithm.
ydk,cj<math><msub is="true"><mi is="true">y</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math>: represents the actual membership of the kth target word. In other words, this is the label provided in the supervised classification scheme. If cj is the class of the kth target word, then ydk,cj=1<math><mrow is="true"><msub is="true"><mi is="true">y</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>; otherwise, ydk,cj=0<math><mrow is="true"><msub is="true"><mi is="true">y</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>.
ϕdk,cj<math><msub is="true"><mi is="true">ϕ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math>: represents the obtained membership of the kth target word. If cj is the class obtained for the kth target word, then ϕdk,cj=1<math><mrow is="true"><msub is="true"><mi is="true">ϕ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></math>; otherwise, ϕdk,cj=0<math><mrow is="true"><msub is="true"><mi is="true">ϕ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>.
ϵdk,cj<math><msub is="true"><mi is="true">ϵ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math>: denotes the error of the current iteration. It is computed as: (3)ϵdk,cj=ydk,cj−ϕdk,cj.<math><mrow is="true"><msub is="true"><mi is="true">ϵ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">=</mo><msub is="true"><mi is="true">y</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">−</mo><msub is="true"><mi is="true">ϕ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">.</mo></mrow></math>As we shall show, this error is used to update weights in f so that, at each new iteration, the distance between ydk,cj<math><msub is="true"><mi is="true">y</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math> and ϕdk,cj<math><msub is="true"><mi is="true">ϕ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math> decreases.
Note that, in the model illustrated in Fig. 1, we only consider the relationship between feature and target words.
The algorithm can be divided into the three following major steps:
Initialization: there are three possible ways of initializing f, i.e. the vector weights of feature words. The most simple strategy is to initialize weights with zeros or random values. A more informed alternative initializes weights using the a priori likelihood of feature words co-occur with senses. This probability can be computed as (4)Pr=P(fi|dk)=nfi,dk/ndk,<math><mrow is="true"><mspace width="4.pt" is="true"></mspace><mtext is="true">Pr</mtext><mo is="true">=</mo><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">f</mi><mi is="true">i</mi></msub><mo is="true">|</mo><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><msub is="true"><mi is="true">n</mi><mrow is="true"><msub is="true"><mi is="true">f</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub></mrow></msub><mo is="true">/</mo><msub is="true"><mi is="true">n</mi><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub></msub><mo is="true">,</mo></mrow></math>where nfi,dk<math><msub is="true"><mi is="true">n</mi><mrow is="true"><msub is="true"><mi is="true">f</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub></mrow></msub></math> is the number of times that the ith feature word appears in the context of the kth target word and ndk<math><msub is="true"><mi is="true">n</mi><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub></msub></math> is the total number of occurrences of dk. In our experiments, we report the best results obtained among these three alternatives.
Error calculation: In the error calculation step, firstly, the output vector for each target word (ϕ(dk)) is computed. This vector depends upon the presence of the feature word in the context (wdk,ti<math><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub></math>) and its relevance for the class (fti,cj<math><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math>). Mathematically, the class computed at each new iteration is given by (5)C(∑ti∈Twdk,tifti,cj)={1,ifcj=argmaxcl∈C(∑ti∈Twdk,tifti,cj).0,otherwise.<math><mrow is="true"><mi is="true">C</mi><mo is="true">(</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">T</mi></mrow></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">)</mo><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">,</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="4.pt" is="true"></mspace><mtext is="true">if</mtext><mspace width="4.pt" is="true"></mspace><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub><mo is="true">=</mo><mi is="true">arg</mi><munder is="true"><mo movablelimits="false" form="prefix" is="true">max</mo><mrow is="true"><msub is="true"><mi is="true">c</mi><mi is="true">l</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">C</mi></mrow></munder><mo is="true">(</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">T</mi></mrow></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">)</mo><mo is="true">.</mo></mrow></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo></mrow></mtd><mtd columnalign="left" is="true"><mrow is="true"><mspace width="4.pt" is="true"></mspace><mtext is="true">otherwise.</mtext></mrow></mtd></mtr></mtable></mrow></mrow></math>After updating the classes for each target word, the values of fti,cj<math><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub></math> are modified. This update is controlled by the correction rate η: (6)fti,cj(n+1)=fti,cj(n)+η∑dk∈Dwdk,tiϵdk,cj(n),<math><mrow is="true"><msubsup is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></msubsup><mo is="true">=</mo><msubsup is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">+</mo><mi is="true">η</mi><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">D</mi></mrow></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub><msubsup is="true"><mi is="true">ϵ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></msubsup><mo is="true">,</mo></mrow></math>where the superscript (n) in f and ϵ denotes the value of these quantities computed in the nth iteration of the algorithm and D<math><mi mathvariant="script" is="true">D</mi></math> is the set of target words. Note that ϵdk,cj(n)<math><msubsup is="true"><mi is="true">ϵ</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></msubsup></math> is computed as defined in Eq. (3). The process of generating an output vector for each target word, computing the class and performing weight/feature relevance correction is done iteratively until a stop criterion is reached. In our experiments, we have stopped the algorithm when a minimum error ϵmin=0.01<math><mrow is="true"><msub is="true"><mi is="true">ϵ</mi><mi is="true">min</mi></msub><mo is="true">=</mo><mn is="true">0.01</mn></mrow></math> is obtained. If the minimum error is not reached after nmax=1,000<math><mrow is="true"><msub is="true"><mi is="true">n</mi><mi is="true">max</mi></msub><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">000</mn></mrow></math> iterations, the algorithm is stopped.
Classification: in the classification phase, the induced values of f are used in the classification. The word senses for each ambiguous word of the dataset are then obtained by computing the following linear combination: (7)class(dk)=argmaxcj∈C(∑ti∈Twdk,tifti,cj).<math><mrow is="true"><mspace width="4.pt" is="true"></mspace><mtext is="true">class</mtext><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">arg</mi><munder is="true"><mo movablelimits="false" form="prefix" is="true">max</mo><mrow is="true"><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">C</mi></mrow></munder><mo is="true">(</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi mathvariant="script" is="true">T</mi></mrow></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">k</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub></mrow></msub><msub is="true"><mi is="true">f</mi><mrow is="true"><msub is="true"><mi is="true">t</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">c</mi><mi is="true">j</mi></msub></mrow></msub><mo is="true">)</mo><mo is="true">.</mo></mrow></math>
Some aspects of the IMBHN algorithm resemble a neural network, namely the use of weights to represent the relevance of features in the classification process and the use of a similar optimization strategy to learn weights. However, the underlying IMBHN network structure completely differs from a neural network because the former learns a bipartite structure to represent the relationship of two distinct types of entities (terms and senses). Another distinctive difference of the IMBHN structure concerns its ability to direct propagate the information through neighbors. Note that a single layer network, conversely, performs the selection of relevant information via activation functions.
The experimental evaluation of the algorithm was performed in two stages. In the first step, we assessed the performance of the algorithm by comparing to other state-of-the-art inductive classification algorithms. In the second stage, the IMBHN algorithm was applied to two WSD corpora previously used in WSD shared tasks, allowing thus the comparison of our method with state-of-the-art WSD systems. Both corpora are presented in the next section.
The minimal corpus is composed of 4 words (interest, line, serve and hard), which were used in similar works [17], [24], [25]. This corpus comprises documents from distinct sources, including the San Jose Mercury News Corpus and the Penntreebank portion of the Wall Street Journal. The corpus encompasses 15,225 instances of short texts representing the context surrounding ambiguous words, where words are tagged with their respective part-of-speech. In this corpus, the correct senses conveyed by ambiguous words were manually annotated. The number of senses and the number of instances of each word used in our experiments is shown in Table 1. In the evaluation process, these four words were considered as the target words. In particular, to characterize the contexts, we have removed stopwords and punctuation marks as such elements do not convey any semantical meaning and, therefore, do not improve the characterization of contexts.
The Senseval-3 and SemEval-2007 corpora here presente refer, respectively, to the corpora used in the Senseval-3 English Lexical Sample Task [34] and in the SemEval-2007 Task 17 English Lexical Sample [40]. Both datasets provide instances of short texts representing the context of ambiguous words. The Senseval-3 is composed of 57 ambiguous words and 11,804 instances (7,860 for train and 3,944 for test). The words were extracted from the British National Corpus. The SemEval-2007 comprises 100 ambiguous words in 27,132 instances (22,281 for train and 4,851 for test). The data used in this corpus was extracted from both the Wall Street Journal and the Brown Corpus.
In this experiment the results obtained by the IMBHN algorithm were compared with four inductive classification algorithms: Naive Bayes (NB) [18], J48 (C4.5 algorithm) [41], IBk (k-Nearest Neighbors) [1] and Support Vector Machine via sequential minimal optimization (SMO) [39]. The parameters of these algorithms have been chosen using the methodology described in [8]. For the IMBHN algorithm, we used the error correction rates η={0.01,0.05,0.10,0.50}<math><mrow is="true"><mi is="true">η</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">0.01</mn><mo is="true">,</mo><mn is="true">0.05</mn><mo is="true">,</mo><mn is="true">0.10</mn><mo is="true">,</mo><mn is="true">0.50</mn><mo is="true">}</mo></mrow></math>. The number of topical features used in the experiments were |T|={100,200,300}<math><mrow is="true"><mo is="true">|</mo><mi mathvariant="script" is="true">T</mi><mo is="true">|</mo><mo is="true">=</mo><mo is="true">{</mo><mn is="true">100</mn><mo is="true">,</mo><mn is="true">200</mn><mo is="true">,</mo><mn is="true">300</mn><mo is="true">}</mo></mrow></math>. Finally, the window size for the local features were ω={1,2,3}<math><mrow is="true"><mi is="true">ω</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">}</mo></mrow></math>. The evaluation process was performed via 10-fold cross-validation [23].
To analyze the behavior and accuracy of the IMBHN algorithm, we first studied the WSD task using topical features to characterize the context of target words of our dataset. The obtained results are shown in Table 2. When the number of topical features |T|<math><mrow is="true"><mo is="true">|</mo><mi mathvariant="script" is="true">T</mi><mo is="true">|</mo></mrow></math> is set with |T|=100,<math><mrow is="true"><mo is="true">|</mo><mi mathvariant="script" is="true">T</mi><mo is="true">|</mo><mo is="true">=</mo><mn is="true">100</mn><mo is="true">,</mo></mrow></math> the best results occurred for the SMO and J48 techniques. In three cases, the IMBHN performed worse than the best results achieved with competing techniques.
In general, the performance of the classifiers tend to improve when the number of topical features (|T|<math><mrow is="true"><mo is="true">|</mo><mi mathvariant="script" is="true">T</mi><mo is="true">|</mo></mrow></math>) increases from 100 to 300. This is clear when one observes that e.g. the best accuracy rate for the word “interest” goes from 79.77% to 84.71%. The same behavior can be observed for the other target words of the dataset, however, in a minor proportion. Concerning the performance of the IMBHN technique when |T|={200,300},<math><mrow is="true"><mo is="true">|</mo><mi mathvariant="script" is="true">T</mi><mo is="true">|</mo><mo is="true">=</mo><mo is="true">{</mo><mn is="true">200</mn><mo is="true">,</mo><mn is="true">300</mn><mo is="true">}</mo><mo is="true">,</mo></mrow></math> in most cases, the IMBHN method is outperformed by the SMO technique, which provided the best results for the words “interest”, “line” and “serve”. The best results for the word “hard” was achieved with the J48 classifier.
When analyzing the performance of the classifiers induced with local features, a different pattern of accuracy has been found, as shown in Table 3. For the words “interest”, “line”and “serve” the IMBHN classifier yielded the best results, for ω={1,2,3}<math><mrow is="true"><mi is="true">ω</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">}</mo></mrow></math>. Conversely, if we consider the word “hard”, the decision tree based algorithm, J48, outperformed all other methods. However, the performance achieved with J48 was very similar to the one obtained with the IMBHN: the maximum difference of accuracy between these two classifiers was 1.09%, when ω=3<math><mrow is="true"><mi is="true">ω</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>. This observation confirms the suitability of the method for the problem, as optimized results have been found for virtually all words of the dataset.
The best results obtained with topical and local features are summarized in Table 4. The IMBHN algorithm for representing texts and discriminating senses outperformed other methods when considering also distinct types of features. In special, the IMBHN performed significantly better than the SMO method for the word “line” and “serve”. A minor gain in performance has been observed for “interest”. With regard to the word “hard”, the best performance was obtained with the J48 (with topical features). However, a similar accuracy was obtained with the IMBHN (with local features, as shown in Table 3). All in all, these results show, as a proof of principle, that the proposed algorithm may be useful to the word sense disambiguation problem, as optimal or near-optimal performance has been found in the studied corpus. Given the superiority of the local feature strategy, we also provide in Table S2 of the Supplementary Information results for additional words, which also confirm the effectiveness of the IMBHN algorithm.
State of the art WSD methods do not only use machine learning for classification purposes, but also a combination of heuristics, domain specific information and deep resources such as thesaurus and lexical datasets (e.g. the WordNet) [37]. The combination of distinct techniques and resources explains the reason why the IMBHN appears in a intermediary rank when compared to other methods relying upon more semantic information. We should note that the only information used by this method is the co-occurrence information present in the text, therefore no external information is used. Given the superiority of the IMBHN over SMO in some scenarios, it could be interesting to explore, in future works, the performance of other state of the art systems (such as the IMS) by using the IMBHN as the main machine learning algorithm (note that the IMS originally uses the SVM as main machine learning method).
A disadvantage associated to the use of supervised methods to undertake the word sense disambiguation problem is the painstaking, time-consuming effort required to build reliable datasets [37]. For this reason, it becomes relevant to analyze the performance of WSD systems when only a few labelled instances are available for training [37]. In this sense, we performed a robustness analysis of the proposed algorithm to investigate how performance is affected when smaller fractions of the dataset are provided for the algorithm. To perform such a robustness analysis the following procedure was adopted. We defined a sampling rate S,<math><mrow is="true"><mi mathvariant="script" is="true">S</mi><mo is="true">,</mo></mrow></math> representing the percentage of disregarded instances from the original dataset. For each sampling rate, we computed the accuracy Γ(S) relative to the sampled dataset. The relative accuracy rate for a given S was computed as (8)Γ˜(S)=Γ(S)Γ(0),<math><mrow is="true"><mover accent="true" is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mo is="true">˜</mo></mover><mrow is="true"><mo is="true">(</mo><mi is="true">S</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mo is="true">(</mo><mi is="true">S</mi><mo is="true">)</mo></mrow><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mo is="true">(</mo><mn is="true">0</mn><mo is="true">)</mo></mrow></mfrac><mo is="true">,</mo></mrow></math>which quantities the percentage of the original accuracy which is preserved when the original dataset is sampled with sampling rate S. For each sampling rate, we generated 50 sampled subsets. The obtained results for the IMBHN in its best configuration (i.e. using local features and ω=3<math><mrow is="true"><mi is="true">ω</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>) are shown in Fig. 2. The best scenario occurs for the word “hard”, as even when 90% of the original is ignored, in average, more than 95% of the original accuracy (i.e. Γ(S=0)<math><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mo is="true">(</mo><mi is="true">S</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">)</mo></mrow></math>) is recovered. Concerning the other words, a good performance was also observed when only a small fraction was available. This is the case of “serve”: when 90% of the dataset is disregarded, 85% of the original accuracy is kept. These results suggest that the IMBHN could be successfully applied in much smaller datasets without a significative loss in performance. We have found similar robustness results for other configurations of parameters (ω) of the IMBHN (results not shown), which reinforces the hypothesis that the resiliency of the method with regard to the total amount of instances in the training phase is stable with varying parameter values. Note that such a robustness, although strongly desired in practical problems, does not naturally arise in all pattern recognition methods. This is evident e.g. when the robustness SMO is verified for “serve” and “interest”, as shown in Fig. 3. Note that when S=0.9,<math><mrow is="true"><mi is="true">S</mi><mo is="true">=</mo><mn is="true">0.9</mn><mo is="true">,</mo></mrow></math> the accuracy drops to about 60% of its original value. The results confirmed that only a minor decrease in performance is observed when labelled data is scarce in the IMBHN algorithm. Such a robustness suggests that the algorithm might not only be useful for the WSD task, but also for semi-supervised related problems [12].
In the proposed model, as the number of training examples increases, the connectivity patterns between feature and target words tend to become constant (i.e. each word tends to keep the same number of links). However, the number of links for each feature/target word depends on the ambiguous word being analyzed, so there is no simple clear pattern that can be explained with the degree of the bipartite networks. The same idea holds for other measurements such as those dependent on link weights. We note that topological features of networks, however, have already been used for the WSD task, with different network formations (see e.g. [9]). So we think that it would be interesting to explore in future works if there is any fact of the solution of the WSD problems that can be explained with features of bipartite networks.
In this experiment, the IMBHN algorithm was applied in two WSD corpora that were previously used in Senseval-3 and SemEval-2007, allowing thus the comparison with state-of-the-art WSD systems that participated of the shared tasks. Only local features are considered in this experiment because, in the previous experiment, the best results of our method were obtained with these features. Since in both shared tasks the evaluation of WSD systems was performed using recall, precision and F-score, we chose to use the F-score because it consolidates recall and precision in a single quality index, simplifying the comparison between systems. The parameters of the algorithm were chosen in accordance with the previous experiment, being the error correction rate η=0.10<math><mrow is="true"><mi is="true">η</mi><mo is="true">=</mo><mn is="true">0.10</mn></mrow></math> and the window size for the local features ω={1,2,3,4,5,6,7,8,9,10}<math><mrow is="true"><mi is="true">ω</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">,</mo><mn is="true">4</mn><mo is="true">,</mo><mn is="true">5</mn><mo is="true">,</mo><mn is="true">6</mn><mo is="true">,</mo><mn is="true">7</mn><mo is="true">,</mo><mn is="true">8</mn><mo is="true">,</mo><mn is="true">9</mn><mo is="true">,</mo><mn is="true">10</mn><mo is="true">}</mo></mrow></math>. We also considered as the context all words in the sentence where the ambiguous word occurs.
Table 5 shows the best result obtained by variations of the IMBHN, together with the baseline (i.e. the Most Frequent Sense) and a sample of the systems that participated in Senseval-3. The systems were evaluated in two variants, which considered fine and coarse grained senses (according to WordNet 1.7.1 and Wordsmyth). The ranking os systems was generated considering only their performance in fine grained senses. In this assessment, our system exceeded the baseline by 8.4% (fine) and 4.4% (coarse), and had very close results to the systems that were in 25th and 26th places (among 48 systems).
In the SemEval-2007 task, only coarse grained senses were considered (based on WordNet 2.1), since the identification of fine grained senses is a hard task even for human annotators [40]. Table 6 shows the result of the best variation of the IMBHN along with a sample of systems that participated in the SemEval-2007. We also show the performance of the baseline based on the most frequent sense. In this evaluation, our system outperformed the baseline by a margin of 5.2%. The IMBHN also displayed a similar performance of systems ranked in 6th and 7th places (among 14 systems).
In both datasets our algorithm did not exceed the best results, but managed to overcome the baseline and got better results than about half of the systems that participated in both tasks. Arguably, most of the participating systems have made the use of multiple features while we focused only statistical, superficial features. These results suggest that our system performs well if we consider that any linguistic, deeper information regarding senses was used to create the classifier. Another point of interest is that a large part of the best performing systems made use of the SVM as a core classifier. For this reason, we argue that such systems could benefit from the IMBHN to handle local features, since our algorithm is able to overcome the SVM in some cases, as discussed in the “Experiment1” section.
The accurate discrimination of word senses plays a pivotal role in information extraction and document classification tasks [4], [14]. This task is important to improve other systems such as machine translators and search engines [37]. While methods based on deep paradigms may perform well in very specific domains, statistical methods based mainly on machine learning have proved useful to undertake the word sense disambiguation task in more general contexts. In this article, we have devised a statistical model to both represent contexts and recognize patterns in written texts. The model hinges on a bipartite network, with layers representing feature words and target words, i.e. words conveying two or more potential senses. We have shown, as a proof of principle, that the proposed model presents a significant performance, mainly when contextual features are modelled via extraction of local words to represent semantical contexts. We have also observed that, in general, our method performs well even if a relatively small amount of data is available for the training process. This is an important property as it may significantly reduce both time and effort required to construct a corpus of labelled data. Concerning its performance compared to state-of-the-art WSD systems, our method was competitive although not exceed the best methods that participated of the Senseval-3 English Lexical Sample Task and SemEval-2007 Task 17 English Lexical Sample. We note here that no deep linguistic information was used in our system, which makes it more suitable when the existence of such information is limited or absent. Even though our method does not present the lowest processing time, we highlight that the technique can take advantage of specific hardware, which may substantially improve the efficiency of the method in a practical scenario.
As future work, we intend to explore further generalizations of the algorithm. Owing to the power of word adjacency networks in extracting relevant semantical features of texts [9], we intend to use such models to improve the characterization of the studied bipartite networks. The word adjacency model could be used, for example, to better represent the relationship between feature and target words by using network similarity measurements [10], [27], [29]. We also intend to extend the present model to consider topological and dynamical measurements of word adjacency networks as local features [9]. While in the current model we explored only the relationship between feature words and target words, we could also consider the inner-relationships between feature words or target words. The relationship between features words, e.g. can be considered using other networked models, such as co-occurrence networks [44].
E.A.C. Jr. and D.R.A. acknowledge financial support from Google (Google Research Awards in Latin America grant). D.R.A. thanks São Paulo Research Foundation (FAPESP) for support (14/20830-0, 16/19069-9 and 17/13464-6). A.A.L. acknowledges support from FAPESP (11/22749-8 and 15/14228-9) and CNPq(Brazil) (302645/2015-2).
Download : Download Acrobat PDF file (180KB)Supplementary Data S1. Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/