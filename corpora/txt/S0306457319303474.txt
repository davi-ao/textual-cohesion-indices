With the advancement of technology, a huge amount of information is floating around the Web which is expressed in natural language. Detecting the sense of a word is a primary step for natural language understanding. Researchers have tried to detect the sense of words from a given corpus in both supervised and unsupervised ways. One stream of work deals with the task of word sense induction (WSI), the goal of which is to automatically induce different senses of a given word, generally in the form of an unsupervised learning task with senses represented as clusters of words. The word sense disambiguation (WSD) task opens up another stream of literature, where a fixed sense inventory is assumed to exist, and the senses of a given word are disambiguated using the sense inventory as a reference. However, in both of these tasks, the assumption is that the number of senses that a word has is static. In addition, the senses do exist in the sense inventory to compare with for these tasks. They attempt to detect or induce one of these senses depending on the context.
Natural language, however, is dynamic and is constantly evolving as per the users’ needs which leads to continuous changes of word meanings over time. For example, by late 20th century, the word ‘virus’ has come up with the ‘technology’ related sense whereas the word ‘cool’ has emerged with ‘smart, calm personality’ related sense. How to automatically detect such changes? Given sufficient time-stamped data, can one design efficient algorithms to detect known as well as unknown shifts in word meanings highly precisely? Such automation can directly benefit people such as librarians, historians or linguists who work with digitized texts from different time periods. The variation in the sense of a word could either be attributed to the emergence of a completely new sense of the word or change of usage of a well-established sense of the word. This semantic evolution of a word over time is of great interest to historical linguistics. Besides, lexicography is also expensive; compiling, editing and updating sense inventory entries frequently remains cumbersome and labor-intensive.
In general, detecting time-specific knowledge would make word meaning representations more accurate and hence, is a worthy problem to study for specialists such as etymologists, librarians, general public and NLP researchers. A well constructed semantic representation of a word is useful for many natural language processing or information retrieval systems like machine translation, semantic search, disambiguation, Q&A, etc. Taking into account the newer senses of a word can increase the relevance of the query result leading to a better semantic search. Similarly, a sense disambiguation engine informed with the newer senses of a word can increase the efficiency of disambiguation; it can recognize senses not available in sense inventory that would otherwise be wrongly assigned to any of the available senses from the inventory. Above all, a system having the ability to perceive the novel sense of a word can help in an automatic sense inventory update by taking into account the temporal scope of senses.
Investigations of individual words or phrases were very limited due to the need for huge human involvement until now. Recently, with the arrival of large-scale collections of historic texts and online libraries such as Google books, a new paradigm has been added to this research area, whereby the prime interest is in identifying the temporal scope of a sense (Gulordava, Baroni, 2011, Jatowt, Duh, 2014, Lau, Cook, McCarthy, Gella, Baldwin, 2014, Tahmasebi, Risse, Dietze, 2011) which, in turn, can give further insights to the phenomenon of language evolution. Some recent attempts (Eger, Mehler, 2016, Frermann, Lapata, 2016, Hamilton, Leskovec, Jurafsky, 2016a, Hamilton, Leskovec, Jurafsky, 2016b, Kulkarni, Al-Rfou, Perozzi, Skiena, 2015) have been made to model the dynamics of language in terms of word senses.
One of the studies in this area has been presented by Mitra et al. (2014) where the authors show that at earlier times, the sense of the word ‘sick’ was mostly associated to some form of illness; however, over the years, a new sense associating the same word to something that is ‘cool’ or ‘crazy’ has emerged. Their study is based on a unique network representation of the corpus called a distributional thesauri (DT) network built using Google books syntactic n-grams. They have used unsupervised clustering techniques to induce a sense of a word and then compared the induced senses of two time periods to get the new sense for a particular target word.
While Mitra et al. (2014) reported a precision close to 0.6 over a random sample of 49 words, we take another random sample of 100 words separately and repeat manual evaluation. When we extract the novel senses by comparing the DTs from 1909–1953 and 2002–2005, the precision obtained for these 100 words is as low as 0.32. Similarly, if we extract the novel senses comparing the DTs of 1909–1953 with 2006–2008, the precision stands at 0.23. We then explore another unsupervised approach presented in Lau et al. (2014) over the same Google books corpus,1 apply topic modeling for sense induction and directly adapt their similarity measure to get the new senses. Using a set intersecting with the 100 random samples for Mitra et al. (2014), we obtain the precision values of 0.21 and 0.28, respectively. Clearly, none of the precision values are good enough for reliable novel sense detection. Thus, the primary motivation of this work is to devise an approach that is able to boost the precision values to an acceptable range. However, the idea is not to build the entire framework from scratch but to carefully re-engineer the algorithm presented by Mitra et al. (2014). Precisely, we show how we can take extreme advantage of the differences in certain properties of the networks that the authors had originally built to detect sense changes. Note that these properties were completely overlooked in the original approach and our main contribution is to unfold their tremendous benefit in improving the precision of novel sense detection.
It is well known that network science has proved to be very effective in addressing problems related to various complex phenomena including the structure and dynamics of the human brain, the functions of genetic pathways, the social behavior of humans in the online and offline world and many more. Some recent work shows that complex network concepts are being applied to understand human languages as well (Antiqueira, Nunes, Oliveira Jr, Costa, 2007, Ferrer i Cancho, Capocci, Caldarelli, 2007). The ability to access embedded knowledge makes complex networks extremely promising for natural language processing which normally requires deep knowledge representation. Many works exist where network properties are applied to natural language processing tasks, which lead to elegant solutions to the problem. Examples include word co-occurrence network (Ferrer i Cancho & Solé, 2001), word association network (Bonneau, Just, & Matthews, 2010), syntactic dependency network (Ferrer i Cancho, 2004), etc. Some other applications of complex networks in NLP include ways to evaluate machine-generated summaries (Pardo, Antiqueira, Nunes, Oliveira Jr, & da Fontoura Costa, 2006), detection of ambiguity in a text (Dorow et al., 2004), etc. These works constitute our prime motivation to apply network science methods to enhance the precision of novel word sense detection.
We propose a supervised method based on the network features to reduce the number of false positives and thereby, increase the overall precision of the method proposed by Mitra et al. (2014). As per Mitra et al. (2014), the basic idea is to prepare Distributional Thesaurus networks from a corpus that covers old and new time periods, cluster the network around target word to obtain the sense clusters in both the time periods and then do a cluster comparison to find out the sense cluster from the new time period to have a ‘birth’ sense for the target word which is not present in the old time period. Now, if a target word qualifies as having a new sense (‘birth’) as per their method, we construct two induced subgraphs of those words that form the cluster corresponding to this ‘birth’ sense, from the corresponding distributional thesauri (DT) networks of the two time periods. Next, we compare the following three network properties: (i) the edge density (ED), (ii) the structural similarity (SS) and (iii) the average path length (APL) (Turnu, Marchesi, Tonelli, 2012, Wasserman, Faust, 1994) of the two induced subgraphs from the two time periods. A remarkable observation is that although this is a small set of only three features, for the actual ‘birth’ cases, each of them has a significantly different value for the later time point and are therefore very discriminative indicators. In fact, the features are so powerful that even a small set of training instances is sufficient for making highly accurate predictions. We pose this problem as a binary classification task where we get the best results for Support Vector Machine (SVM) classifier fed with the fractional change of ED, SS, and APL over time as features.
Preparation of gold standard dataset: In order to evaluate our model, we prepare a gold standard dataset through human annotations. As far as we are aware of the literature, there are no such gold standard datasets (even âǣsilver annotationsâǥ) available, which in turn makes the evaluation task difficult for this particular problem of novel sense detection. This concern is also discussed in detail in the recent surveys (Kutuzov, Øvrelid, Szymanski, Velldal, 2018, Tahmasebi, Borin, Jatowt, 2018). In most of the studies dealing with this task, researchers take some example words known to have emerged with new sense and try to model their characteristics over time. On the other hand, the datasets for evaluating tasks like word sense disambiguation or word sense induction do not contain the time information. Considering all these issues, we introduce a carefully prepared gold standard dataset. Note that this gold standard dataset consists of 365 words (nouns extracted from the torso region of word frequency as per Google books corpus), 184 for 1909–1953 vs 2002–2005 and 181 for 1909–1953 vs 2006–2008. This dataset is one of our contributions as well which will help the community to move further in this otherwise difficult task.
Results: Evaluation using this gold standard dataset shows that this classification achieves an overall precision of 0.86 and 0.74 for the two time point pairs over the same set of samples, in contrast with the precision values of 0.32 and 0.23 by the original method. Note that we would like to stress here that an improvement of more than double in the precision of novel sense detection that we achieve has the potential to be the new stepping stone in many NLP and IR applications that are sensitive to novel senses of a word.
In addition to exploring the usefulness of network measures obtained from the DT networks, we also investigate the effect of using network representation learning methods. For that purpose we produce network embeddings to map the DT from the network space to the vector space using Deepwalk (Perozzi, Al-Rfou, & Skiena, 2014) and node2vec (Grover & Leskovec, 2016). We propose two measures – Intra-cluster Average Similarity (IAS) and Average Similarity with Target word (AST) computed from the network embeddings, and apply fractional change of these two measures as features in the classifier. We find that using these two features or a combination of these features along with the network features (ED, SS, APL) in the classifier helps to improve precision in some cases but is not able to beat the overall F-measure values of the classifier fed with only network features (ED, SS, APL).
We further investigate the usefulness of word embeddings obtained using FastText (Bojanowski, Grave, Joulin, & Mikolov, 2017) trained on the corpus from two different time periods. We use the same two measures as discussed before – Intra-cluster Average Similarity (IAS) and Average Similarity with Target word (AST) but computed using FastText embeddings, and apply fractional change of these two measures as features in the classifier. In this scenario as well, we observe that using these two features or a combination of these features along with the network features (ED, SS, APL) in the classifier helps to improve precision in some cases but is not able to beat the overall F-measure values of the classifier fed with only network features (ED, SS, APL).
Further, we also investigate the robustness of our approach by analyzing the ability to capture known historical shifts in meaning. Preparing a list of words that have been suggested by different prior works as having undergone sense change, we see that 80% of those words get detected by our approach. We believe that the ability to detect such diachronic shifts in data can significantly enhance various standard studies in natural language evolution and change.
The work presented here is an extension of Jana, Mukherjee, and Goyal (2019). The novel aspects and contributions of this paper with respect to the conference version are (a) it is an extended version of the conference paper with a detailed explanation of the baselines, dataset description, proposed methodology, extensive feature analysis along with illustrative examples and (b) in addition to applying only complex network measures, we also investigate the applicability of network representation learning methods and FastText embedding methods for the task of precise novel sense detection.
Word sense induction and word sense disambiguation are the broad problems that deal with the detection of word senses. Some of the first attempts were made by Mihalcea and Moldovan (1999) where they proposed an approach to automatically generate an arbitrarily large corpus for word senses using the information provided in WordNet and the information gathered from Web using existing search engines. Sahlgren (2002) tried to build a model of semantic knowledge using random indexing, which is able to acquire ambiguous semantic information in an unsupervised fashion from unstructured text data. Recently, Wu and Giles (2015) proposed a multi-prototype model for word representation using Wikipedia, namely SaSA, that could give more accurate sense-specific representation to words with multiple senses. Another stream of literature focuses on inducing senses of words by using clustering approaches with different context representations. Pantel and Lin (2002) presented a clustering algorithm, CBC (Clustering By Committee) to automatically discover senses from the text. In a similar line, Dorow and Widdows (2003) came up with an iterative clustering-based method on the word-relation graph. Bordag (2006) proposed a triplet-based clustering algorithm that instantiated the ‘one sense per collocation’ observation. Pedersen and Bruce (1998) presented a corpus-based approach to word-sense disambiguation that only requires information that can be automatically extracted from the untagged text. Some of the recent attempts to word sense disambiguation have been made by Raviv, Markovitch, and Maneas (2012). They introduced Concept-Based Disambiguation (CBD), a novel framework that utilizes recent semantic analysis techniques to represent both the context of the word and its senses in a high-dimensional space of natural concepts. Baskaya and Jurgens (2016) presented a new approach to build a semi-supervised WSD system that combines a small amount of sense-annotated data with information from WSI. Some researchers (Erk & Pado, 2007) even observed that the senses of a word are not completely disjoint and attempted to propose a graded representation of word sense.
On the other hand, researchers have also tried to develop data-driven models of language dynamics. One of the first attempts was made by Erk (2006), where the author tried to model this problem as an instance of outlier detection, using a simple nearest neighbor-based approach. Gulordava and Baroni (2011) study the change in the semantic orientation of words using Google book n-grams corpus from different time periods. In another work, Mihalcea and Nastase (2012) attempted to quantify the changes in word usage over time and came up with the intuition that changes in usage frequency and word senses contribute to these differences in usages. In similar lines, Jatowt and Duh (2014) used the Google n-grams corpus from two different time periods and proposed a method to identify semantic change based on the distributional similarity between the word vectors. Tahmasebi et al. (2011) attempted to track sense changes from a newspaper corpus containing articles between 1785 and 1985. Even though the interest for automatic identification of new word senses has grown, the research has been limited by the availability of appropriate evaluation resources. Efforts have been made by Cook, Lau, McCarthy, and Baldwin (2014) to prepare the largest corpus-based dataset of diachronic sense differences. Attempts have been made by Lau, Cook, McCarthy, Newman, and Baldwin (2012) where they first introduced topic modeling based word sense induction method to automatically detect words with emergent novel senses. In subsequent work, Lau et al. (2014) extended this task by leveraging the concept of predominant sense. The first computational approach to track and detect statistically significant linguistic shifts of words has been proposed by Kulkarni et al. (2015). Researchers (Kenter, Wevers, Huijnen, & De Rijke, 2015) also attempted to solve a variant of this problem which deals with monitoring shifts in vocabulary over time. Recently, Hamilton, Leskovec, and Jurafsky (2016b) proposed a method to quantify semantic change by evaluating word embeddings against known historical changes. In another work, Hamilton et al. (2016a) categorized the semantic change into two types and proposed different distributional measures to detect those types. An attempt has also been made to analyze the time-series model of embedding vectors as well as time-indexed self-similarity graphs in order to hypothesize the linearity of semantic change by Eger and Mehler (2016). Dynamic Bayesian model of diachronic meaning change has been proposed by Frermann and Lapata (2016) where they have shown novel sense detection task as one of their applications. The probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time has also been tried out (Bamler & Mandt, 2017). Recently, researchers have also tried to investigate the reasons behind word sense evolution and have come up with computational models based on chaining (Ramiro, Srinivasan, Malt, & Xu, 2018). Researchers also attempt to apply dynamic word embeddings as well to detect language evolution. Rudolph and Blei (2018) develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. Yao, Sun, Ding, Rao, and Xiong (2018) develop a dynamic statistical model to learn time-aware word vector representation. Researchers also attempt to analyze temporal word analogy by effectively modeling with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space (Szymanski, 2017). In a recent study, Di Carlo, Bianchi, and Palmonari (2019) proposed a new heuristic to train temporal word embeddings based on the word2vec model which talks about using atemporal vectors as a reference, i.e., as a compass, when training the representations specific to a given time interval and they evaluated this heuristic for temporal word analogy task.
As per the surveys made in this stream of literature (Kutuzov, Øvrelid, Szymanski, Velldal, 2018, Tahmasebi, Borin, Jatowt, 2018, Tang, 2018), researchers have tried to formulate the task of tracking semantic shifts differently. Given a corpora containing texts from different time periods, some researchers have tried to locate words with different meaning in different time periods (Cook, Lau, McCarthy, Baldwin, 2014, Lau, Cook, McCarthy, Newman, Baldwin, 2012, Mitra, Mitra, Riedl, Biemann, Mukherjee, Goyal, 2014), some attempted to trace the dynamics of the relationship between the words (Erk, Pado, 2007, Gulordava, Baroni, 2011, Jatowt, Duh, 2014, Mihalcea, Nastase, 2012) whereas some tried to discover the general trends in semantic shifts depicting probable linguistic reasons (Eger, Mehler, 2016, Hamilton, Leskovec, Jurafsky, 2016a, Hamilton, Leskovec, Jurafsky, 2016b, Kulkarni, Al-Rfou, Perozzi, Skiena, 2015). Recently, a new line of research problem which deals with detecting temporal word analogy has also been attempted (Di Carlo, Bianchi, Palmonari, 2019, Szymanski, 2017). Researchers have not only tried to attempt this problem of semantic shift of words from different perspectives with different goals, but they have also tried to formulate solutions to such problems using different methodologies. Starting from simple corpus statistics based approaches (Gulordava, Baroni, 2011, Tahmasebi, Risse, Dietze, 2011), researchers have gradually moved toward probabilistic approaches (Bamler, Mandt, 2017, Erk, 2006, Frermann, Lapata, 2016, Mihalcea, Nastase, 2012), topic modelling based approaches (Cook, Lau, McCarthy, Baldwin, 2014, Lau, Cook, McCarthy, Newman, Baldwin, 2012), and network based approaches (Mitra et al., 2014). Recently, a trend of using dynamic neural embeddings has been observed among the researchers (Di Carlo, Bianchi, Palmonari, 2019, Eger, Mehler, 2016, Jatowt, Duh, 2014, Kulkarni, Al-Rfou, Perozzi, Skiena, 2015, Szymanski, 2017, Yao, Sun, Ding, Rao, Xiong, 2018).
Nevertheless, in majority of these previous works, researchers tried to model the way a word’s sense changes over time and validated their models using words known to have undergone sense change. In some attempts, authors tried to find out how or why the semantic shifts happen as well. In contrast to most of these attempts, we pose the problem as detecting the set of words which have come up with a novel sense between old time point (told) and new time point (tnew) with high precision, provided we have large text corpus from both told and tnew. Therefore we point out two such baselines (Lau, Cook, McCarthy, Gella, Baldwin, 2014, Mitra, Mitra, Riedl, Biemann, Mukherjee, Goyal, 2014) with a similar objective. We describe these baselines in Section 3.
In this section, we describe the two baselines that are relevant to our work.
In Mitra et al. (2014), the authors proposed an unsupervised and automated method to identify word sense changes. Their analysis is only focused on noun words. A brief summary of their work is described below for the ease of the readability of this paper.
The authors used the Google books corpus, consisting of texts from over 3.4 million digitized English books. These books were published between 1520 and 2008, mostly after 1800. The authors constructed distributional thesauri (DT) networks from the Google books syntactic n-grams data (Goldberg & Orwant, 2013). The DT network contains, for each word, a list of words that are similar with respect to their bigram distribution (Riedl & Biemann, 2013). In particular, they first extracted each word and a set of its context features like part-of-speech tag, neighboring set of words, frequency, etc. Next, they calculated the lexicographer’s mutual information (LMI) (Kilgarriff, Rychly, Smrz, & Tugwell, 2004)2 between a word and its features and took the top 1000 ranked features for each word. In the network, each word is a node and there is a weighted edge between a pair of words where the weight of the edge is defined as the number of features that these two words share in common. A snapshot of the DT is shown in Fig. 1. To study word sense changes over time, they divided the dataset across eight time periods; accordingly DT networks for each of these time periods were constructed separately. The basic idea is that if a word undergoes sense change, it can be detected by comparing its senses from two different time periods. The unsupervised method for inducing word senses in each time period is described below.
In order to get the induced sense clusters in an unsupervised way, Chinese Whispers algorithm (Biemann, 2006) has been used. The algorithm produces a set of clusters for each target word by decomposing its neighborhood in the DT network. The hypothesis is that different clusters signify different senses of a target word. The clusters for a target word ‘float’ is shown in Fig. 2. The authors then compare the sense clusters extracted across two different time periods to obtain suitable signals of sense change.
Let us assume that the Chinese Whispers algorithm is run over DTs corresponding to two different time periods, ti and tj. Now, assume that for a given word w, the algorithm gives two different sets of clusters, Ci and Cj, such that m sense clusters are obtained in ti and n sense clusters are obtained in tj. Accordingly, let Ci = si1,<math><mrow is="true"><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><mn is="true">1</mn></msub></msub><mo is="true">,</mo></mrow></math> si2,<math><mrow is="true"><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><mn is="true">2</mn></msub></msub><mo is="true">,</mo></mrow></math>..., sim<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><mi is="true">m</mi></msub></msub></math> and Cj = sj1,<math><mrow is="true"><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><mn is="true">1</mn></msub></msub><mo is="true">,</mo></mrow></math> sj2,<math><mrow is="true"><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><mn is="true">2</mn></msub></msub><mo is="true">,</mo></mrow></math>..., sjn,<math><mrow is="true"><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><mi is="true">n</mi></msub></msub><mo is="true">,</mo></mrow></math> where skz<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">k</mi><mi is="true">z</mi></msub></msub></math> denotes zth sense cluster for word w during time interval tk. There are four types of sense changes that can happen for a target word – split, join, birth and death. These sense changes are defined below.
split: A sense cluster siz<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><mi is="true">z</mi></msub></msub></math> in ti splits into two (or more) sense clusters, sjp1<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><msub is="true"><mi is="true">p</mi><mn is="true">1</mn></msub></msub></msub></math> and sjp2<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><msub is="true"><mi is="true">p</mi><mn is="true">2</mn></msub></msub></msub></math> in tj.
join: Two sense clusters siz1<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><msub is="true"><mi is="true">z</mi><mn is="true">1</mn></msub></msub></msub></math> and siz2<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><msub is="true"><mi is="true">z</mi><mn is="true">2</mn></msub></msub></msub></math> in ti join to make a single cluster sjp<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><mi is="true">p</mi></msub></msub></math> in tj.
birth: A new sense cluster sjp<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">j</mi><mi is="true">p</mi></msub></msub></math> appears in tj, which was absent in ti.
death: A sense cluster siz<math><msub is="true"><mi is="true">s</mi><msub is="true"><mi is="true">i</mi><mi is="true">z</mi></msub></msub></math> in ti dies out and does not appear in tj.
A sense cluster is considered as ‘birth’ if at least 80% words of that cluster are novel, i.e., they do not appear in any of the clusters of the old time periods. For example, in Fig. 2, the programming related cluster of the target word ‘float’ represents a ‘birth’ sense. For split, each split cluster should have at least 30% words of the source cluster and the total intersection of all the split clusters should be > 80%. For join and death, the same parameters are used with the interchange of the source and the target clusters.
Note that, as our main focus is to detect novel sense of a word, we are concerned with only ‘birth’ cases for our study.
The authors then apply multi-stage filtering in order to obtain meaningful candidate words.
Stage 1: They apply Chinese Whisper three times over the two different time periods. They obtain the candidate word lists using their algorithm for the three runs, then take the intersection to output those words and clusters, which came up in all the three runs. Being non-deterministic in nature, the Chinese Whisper algorithm might produce different clustering in different runs. Therefore running it multiple times and taking an intersection is helpful to reduce the effect of non-determinism.
Stage 2: As they focus only on nouns, they keep the candidate words tagged with ‘NN’ or ‘NNS’.
Stage 3: They sort the target words based on their frequency counts and consider only the middle 60% of the list which is the most informative part for this type of analysis. Note that, the authors remove the words in the low-frequency range as there may not be sufficient evidence in the dataset to detect a sense change and rare words usually only have a single sense. On the other hand, words in the high-frequency range tend to be less topic-oriented and thus, appear in very different contexts even when conveying the same (mostly abstract) sense (Kwong, 1998, Luhn, 1958). For evaluation, the authors selected 49 candidate ‘birth’ words from a total of 2789 candidate ‘birth’ words while comparing 1909–1953 DT with the 2002–2005 DT. Using manual evaluation, 31 words were found to be true positives and 18 words were false positives. In our work, we take these 49 candidate words and show that network features can be useful to discriminate the true positives from the false positives.
The authors proposed an unsupervised approach based on topic modeling for sense induction and showed novel sense identification as one of its applications. For a candidate word, Hierarchical Dirichlet Process (Teh, Jordan, Beal, & Blei, 2006) is run over a corpus containing occurrences of that word to induce topics. The induced topics are represented as word multinomials, and are expressed by the top-N words in descending order of conditional probability. Each topic is represented as a sense of the target word. The words having the highest probability in each topic represent the sense clusters. The authors treated the novel sense detection task as identifying those sense clusters, which did not align with any of the recorded senses in a sense repository. They used Jensen-Shannon (JS) divergence measure to compute the alignment between a sense cluster and a synset. First, they computed JS divergence between the multinomial distribution (over words) of the topic cluster and that of the synset, and converted the divergence value into a similarity score. Similarity between topic cluster tj and synset si is defined as(1)sim(tj,si)=1−JS(T∥S)<math><mrow is="true"><mi is="true">s</mi><mi is="true">i</mi><mi is="true">m</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">t</mi><mi is="true">j</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">s</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mn is="true">1</mn><mo linebreak="goodbreak" is="true">−</mo><mi is="true">J</mi><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><mi is="true">T</mi><mo is="true">∥</mo><mi is="true">S</mi><mo is="true">)</mo></mrow></mrow></math>where T and S are the multinomial distributions over words for topic tj and synset si, respectively, and JS(X∥Y) is the Jensen-Shannon divergence for the distribution X and Y. Since we define novel senses while comparing sense clusters across two time periods, we use the same JS measure to detect novel sense of a target word. A sense cluster in the newer time period denotes a new sense (‘birth’) only if its maximum similarity with any of the clusters in older time period is below a threshold, which we have set to 0.35 based on empirical observation.
In Mitra et al. (2014), the authors reported a total of 2789 candidate ‘birth’ words while comparing 1909–1953 DT with 2002–2005 DT and 2468 candidate ‘birth’ words while comparing 1909–1953 DT with 2006–2008 DT. We take all these reported ‘birth’ cases as target words and apply our approach to improve the quality of the detected ‘birth’ senses. We have also followed the procedure described above for Lau et al. (2014) over the same set of candidate words. Then we take 100 random samples from each of these two separate set-ups(1909–1953 vs 2002–2005 and 1909–1953 vs 2006–2008) and compare all three approaches in terms of precision-recall measure over this set using manual evaluation of the reported results.
Mitra et al. (2014) manually evaluated 49 candidate ‘birth’ words from a total of 2789 candidate ‘birth’ words while comparing 1909–1953 DT with the 2002–2005 DT among which 31 words were found to be having come up with novel sense and 18 words are not having novel sense. We first study these 49 candidate ‘birth’ words and show that network features can be useful to discriminate the true positives from the false positives. For each of these candidate words w, we take the ‘birth’ cluster from 2002 to 2005, which is represented by a set of words S. According to our hypothesis, if the words in set S together represent a new sense for w in 2002–2005 which is not present in 1909–1953, the network connection among these words (including w) would be much stronger in the 2002–2005 DT than the 1909–1953 DT. The strength of this connection can be measured if we construct induced subgraphs of S from the two DTs and measure the network properties of these subgraphs; the difference would be more prominent for the actual ‘birth’ cases (true positives) than for the false ‘birth’ signals (false positives). Note that by definition, the nodes in an induced subgraph from a DT will be the words in S and there will be an edge between two words if and only if the edge exists in the original DT; we ignore the weight of the edge henceforth. Thus, the difference between the two subgraphs (one each from the older and newer DTs) will only be in the edge connections. Fig. 3 shows one true positive (‘register’) and one false positive (‘quotes’) word from the set of 49 words and shows the induced subgraphs obtained by a subset of their ‘birth’ clusters across the two time periods. Note that, the target words are not present in the figure. This figure basically depicts, how the network connections among the words in the birth cluster (signifying the sense of target word) change over time. We can clearly see that connections among the words in S are much stronger in the newer DT than in the older ones in the case of ‘registers’, indicating the emergence of a new sense. In the case of ‘quotes’, however, the connections are not very different across the two time periods. We choose three cohesion indicating network properties, (i) the edge density, (ii) the structural similarity and (iii) the average path length, to capture this change.
Let S={w1,w2,…,wn}<math><mrow is="true"><mi is="true">S</mi><mo linebreak="goodbreak" is="true">=</mo><mo is="true">{</mo><msub is="true"><mi is="true">w</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">n</mi></msub><mo is="true">}</mo></mrow></math> be the ‘birth’ cluster for w. Once we construct a graph induced by S from the DT, these network properties are measured as follows:
Edge Density (ED): ED is given by(2)ED=Na/Np<math><mrow is="true"><mi is="true">E</mi><mi is="true">D</mi><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mi is="true">N</mi><mi is="true">a</mi></msub><mo linebreak="goodbreak" is="true">/</mo><msub is="true"><mi is="true">N</mi><mi is="true">p</mi></msub></mrow></math>where Na denotes the number of actual edges between w1,w2,…,wn<math><mrow is="true"><msub is="true"><mi is="true">w</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">n</mi></msub></mrow></math> and Np denotes the maximum possible edges between these, i.e., n(n−1)2<math><mfrac is="true"><mrow is="true"><mi is="true">n</mi><mo is="true">(</mo><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow><mn is="true">2</mn></mfrac></math>. This measure indicates how densely the neighbours of the target word are connected among themselves.
Structural Similarity (SS): For each pair of words (wi, wj) in the cluster S, the structural similarity SS(wi, wj) is computed as:(3)SS(wi,wj)=Ncdeg(wi)*deg(wj)<math><mrow is="true"><mi is="true">S</mi><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">N</mi><mi is="true">c</mi></msub><msqrt is="true"><mrow is="true"><mi is="true">d</mi><mi is="true">e</mi><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">*</mo><mi is="true">d</mi><mi is="true">e</mi><mi is="true">g</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow></mrow></msqrt></mfrac></mrow></math>where Nc denotes the number of common neighbors of wi and wj in the induced graph and deg(wk) denotes the degree of wk in the induced graph, for k=i,j<math><mrow is="true"><mi is="true">k</mi><mo linebreak="goodbreak" is="true">=</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></math>. The average structural similarity for the cluster S is computed by averaging the structural similarity of all the word pairs. This measure indicates the cohesiveness among the neighborhood of the target word.
Average Path Length (APL): To compute average path length of S, we first find the shortest path length between w and the words wi, in the induced graph of S. Let spli denote the shortest path distance from w to wi. The average path length is defined as:(4)APL=∑ispli/n<math><mrow is="true"><mi is="true">A</mi><mi is="true">P</mi><mi is="true">L</mi><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">∑</mo><mi is="true">i</mi></munder><mi is="true">s</mi><mi is="true">p</mi><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">/</mo><mi is="true">n</mi></mrow></math> where n is the number of words in the cluster S. This measure captures how the distance between the target word and the words in the birth cluster changes over time in the network. If the words in the birth cluster come close over time, it signifies that the target word is emerging with a sense captured by the birth cluster.
Table 1 notes the values obtained for these network properties for the induced subgraphs of the reported ‘birth’ clusters for ‘registers’ and ‘quotes’ across the two time periods. The fractional changes observed for the three network properties show a clear demarcation between the two cases. Fractional change (Δ) of any network measure P is defined as,(5)Δ(P)=(P(t2)−P(t1))/P(t1)<math><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Δ</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">P</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mrow is="true"><mo is="true">(</mo><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">t</mi><mn is="true">2</mn></msub><mo is="true">)</mo></mrow><mo linebreak="badbreak" is="true">−</mo><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">t</mi><mn is="true">1</mn></msub><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">/</mo><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">t</mi><mn is="true">1</mn></msub><mo is="true">)</mo></mrow></mrow></math> where t1 and t2 are old and new time periods respectively. The change observed for the ‘birth’ cluster of ‘registers’ is significantly higher than that in ‘quotes’3.
We now compute these parameter values for all the 49 candidate cases. The mean values obtained for the true positives (TP) and false positives (FP) are shown in Table 2. The findings are consistent with those obtained for ‘registers’ and ‘quotes’.
We, therefore, use the fractional changes in the three network properties over time as three features to classify the remaining candidate ‘birth’ words into true positives (actual ‘birth’) or false positives (false ‘birth’). We use different classifiers like Naive Bayes, Support Vector Machine (SVM), Random Forest, etc. for this purpose and report the result of SVM, which was the best performing classifier.
In addition to the proposed three metrics inspired by network science, we also explore the applicability of network representation learning methods in this task. Recall that, we have the DT networks both from the old (DTold) and the new (DTnew) time periods. First, we apply network representation learning methods to obtain continuous feature representations for nodes in networks for both the time periods. As discussed in Section 4, for each candidate word w, we have the ‘birth’ cluster from 2002 to 2005, which is represented by a set of words S. Intuitively, if the words in set S together represent a new sense for w in 2002–2005 which is not present in 1909–1953, the relative similarity in the vector space between the words in S among themselves as well as with the target words would be much higher in the 2002–2005 DT in comparison to the 1909–1953 DT. In order to measure these similarities, we propose the following two metrics.
Intra-cluster Average Similarity (IAS): To compute this metric for S, we first find the pairwise cosine similarity (CSim) between the words in S. Then we consider the average of these cosine similarities. IAS is defined as:(6)IAS=2|S|*|S−1|*∑wi,wj∈S,i<jCSim(wi,wj)<math><mrow is="true"><mi is="true">I</mi><mi is="true">A</mi><mi is="true">S</mi><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mn is="true">2</mn><mrow is="true"><mo is="true">|</mo><mi is="true">S</mi><mo is="true">|</mo><mo is="true">*</mo><mo is="true">|</mo><mi is="true">S</mi><mo is="true">−</mo><mn is="true">1</mn><mo is="true">|</mo></mrow></mfrac><mo linebreak="goodbreak" is="true">*</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi is="true">S</mi><mo is="true">,</mo><mi is="true">i</mi><mo linebreak="badbreak" is="true">&lt;</mo><mi is="true">j</mi></mrow></munder><mi is="true">C</mi><mi is="true">S</mi><mi is="true">i</mi><mi is="true">m</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow></mrow></math>
Average Similarity with Target word (AST): To compute this metric for S, we first find the cosine similarity (CSim) between the target word w and the words in S. Then, we consider the average of these cosine similarities. AST is defined as:(7)AST=1|S|*∑wi∈SCSim(w,wi)<math><mrow is="true"><mi is="true">A</mi><mi is="true">S</mi><mi is="true">T</mi><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mo is="true">|</mo><mi is="true">S</mi><mo is="true">|</mo></mrow></mfrac><mo linebreak="goodbreak" is="true">*</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">S</mi></mrow></munder><mi is="true">C</mi><mi is="true">S</mi><mi is="true">i</mi><mi is="true">m</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow></mrow></math>
We use the fractional change (Δ) (as described in Eq. (5)), of these two measures as features to classify the candidate ‘birth’ words into true positives (actual ‘birth’) or false positives (false ‘birth’).
Now, from the DT network of each time period, in order to prepare the vector representations for each node, we explore two state-of-the-art network representation learning models as discussed below.
Deepwalk: Deepwalk (Perozzi et al., 2014) learns social representations of a graph’s vertices by modeling a stream of short random walks. Social representations signify latent features of the vertices that capture neighborhood similarity and community membership. Using local information from the truncated random walks as input, this method learns a representation that encodes structural regularities.
node2vec: node2vec (Grover & Leskovec, 2016) is an algorithmic framework for scalable feature learning in networks, that maximizes the likelihood of preserving network neighborhoods of nodes in a d-dimensional feature space. This algorithm can learn representations that organize nodes based on their network roles and/or communities they belong to by developing a family of biased random walks, which efficiently explore diverse neighborhoods of a given node.
We further investigate the usefulness of embeddings obtained directly from corpus compared to the embeddings obtained using network embeddings from network representation of corpus. We obtain fastText embeddings (Bojanowski et al., 2017) from both the Google book corpus of old and new time periods. We use the same metrics (IAS, AST) as proposed in Section 4.1 as features to be plugged into a classifier, but in order to compute those two metrics, we use the fastText embeddings of words in place of network embeddings (Deepwalk, node2vec). We use the vector dimension of 128 to be consistent with the analysis using network embeddings.
For experimental evaluation, we start with the ‘birth’ cases reported by Mitra et al. (2014) – 2740 cases (after removing the 49 cases used in training) for 1909–1953 - 2002–2005 (T1)and 2468 cases for 1909–1953 - 2006–2008 (T2). We first discuss how the gold standard dataset is prepared in Section 5.1. In Section 5.2, we run Lau et al. (2014) over these birth cases to detect ‘novel’ sense as per Lau et al.’s algorithm. Separately, we also apply the proposed SVM classification model as a filtering step to obtain ‘filtered birth’ cases. This helps in designing a comparative evaluation of these algorithms as follows. From both the time period pairs (T1 and T2), we take 100 random samples from the birth cases reported by Mitra et al. (2014) and get them evaluated against gold standard dataset prepared by human annotators. For the same 100 random samples, we now use the outputs of Lau et al. (2014) and the proposed approach and estimate the precision as well as recall of these. Note that, for our proposed SVM based model, even though we use these random 100 samples from both T1 and T2 for testing, the training set is fixed to the 49 birth cases taken from T1 provided by Mitra et al. (2014). This comparative evaluation shows, how much we achieve by applying our proposed model on top of the approach of Mitra et al. (2014). We also investigate how good our model performs if we apply it independent of Mitra et al. (2014). Next, we do an extensive feature analysis to explore the importance of each of the network measures. In Section 5.3 we analyze the usefulness of features obtained using Deepwalk and node2vec independently as well as in combination with the proposed network features. Next, in Section 5.4 we do the same analysis using word embeddings obtained using fastText. As a final step, we perform an error analysis of our proposed approach in Section 5.5.
As far as we are aware of the literature, there is a scarcity of gold standard datasets, which in turn makes the evaluation task difficult for this particular problem of novel sense detection. This concern is also discussed in detail in the recent survey papers (Kutuzov, Øvrelid, Szymanski, Velldal, 2018, Tahmasebi, Borin, Jatowt, 2018). Therefore, we prepare a gold standard dataset using human annotations and perform all the evaluations against this gold standard dataset. Each of the candidate words is judged by three evaluators. These evaluators are graduate/post-graduate students, having a good background in Natural Language Processing and well versed with the English language. They are unaware of each other, making the annotation process completely blind and independent. Evaluators are shown the detected ‘birth’ cluster from the newer time period and all the clusters from the older time period. They are asked to make a binary judgment as to whether the ‘birth’ cluster indicates a new sense of the candidate word, which is not present in any of the sense clusters of the previous time point.4 Majority decision is taken in the disagreement. In total, we prepared annotations for a set of as large as 365 words (only nouns taken from Distributional Thesaurus)5 which we believe is significant given the tedious manual judgment involved. In this process of manual annotation, we obtain an inter-annotator agreement (Fleiss’ κ (Fleiss, 1971)) of 0.745,<math><mrow is="true"><mn mathvariant="bold" is="true">0</mn><mo is="true">.</mo><mn mathvariant="bold" is="true">745</mn><mo is="true">,</mo></mrow></math> which is substantial (Viera, Garrett et al., 2005). Table 3 shows three example words from T1, their ‘birth’ clusters as reported in Mitra et al. (2014) and the manual evaluation result. The first three cases belong to computer or technology related sense of ‘sender’, ‘directories’ and ‘float’, which were absent from time point 1909–1953. On the other hand, the ‘birth’ clusters of ‘celebrity’ and ‘quiz’ represent an old sense which was also present in 1909–1953. Similarly, Table 4 shows manual evaluations results for three example cases, along with their novel sense as captured by Lau et al. (2014). This gold standard dataset is also one of our significant contributions and we make it publicly available to facilitate further research.
Only 32 and 23 words out of the 100 random samples from two time point pairs are evaluated to be actual ‘birth’s, respectively, thus giving precision scores of 0.32 and 0.23 for Mitra et al. (2014). Evaluation results for the same set of random samples after applying the approach outlined in Lau et al. (2014) are presented in Table 5. Since the reported novel sense cluster can in principle be different from the ‘birth’ sense reported by the method of Mitra et al. (2014) for the same word, we get the novel sense cases manually evaluated by 3 annotators (42 and 28 cases for the two time periods, respectively). Note that for these 100 random samples (that are all marked ‘true’ by Mitra et al. (2014)), it is possible to find an upper bound on the recall of Lau et al. (2014)’s approach automatically. While the low recall might be justified because this is a different approach, even the precision is found to be in the same range as that of Mitra et al. (2014).
Table 6 presents the evaluation results for the same set of 100 random samples after using the proposed SVM filtering. We see that the filtering using SVM classification improves the precision for both the time point pairs (T1 and T2) significantly, boosting it from the range of 0.23-0.32 to 0.74-0.86. Note that, as per our calculations, indeed the recall of Mitra et al. (2014) would be 100% (as we are taking random samples for annotation from the set of reported ‘birth’ cases by Mitra et al. (2014) only). Even then Mitra et al. (2014)’s F-measure ranges from 0.37-0.48 while ours is 0.67-0.68. Table 7 represents some of the examples which were declared as ‘birth’ by Mitra et al. (2014) but SVM filtering correctly flagged them as ‘false birth’. The feature values in the third column clearly show that the network around the words in the detected ‘birth’ clusters did not change much and therefore, the SVM approach could correctly flag these. Considering the small training set (49 reported ‘birth cases’ by Mitra et al. (2014)), the results are highly encouraging. We also obtain decent recall values for the two time point pairs, giving an overall F-measure of 0.67-0.68.
We, therefore, move onto further feature analysis of the proposed approach. To validate the usefulness of all three proposed features, we first check the Pearson’s correlation among the three features and then perform feature leave-one-out experiments. Table 8 represents the feature correlation matrix which shows that the three features are not significantly correlated. Next, we observe the results of feature leave-one-out experiment for T1 and T2 in Table 13 and Table 14 respectively. We find that the F1-score drops as we leave out one of the features. While {ED, SS} turns out to be the best for precision, {SS, APL} gives the best recall. Table 11 provides three examples to illustrate the importance of using all three features. For the word ‘newsweek’, using {ED, APL} and for the word ‘caring’, using {ED, SS} could not detect those as ‘birth’. Only when all the three features are used, these cases are correctly detected as ‘birth’. Edge density, on the other hand, is very crucial for improving precision. For instance, when only {SS, APL} are used, words like ‘moderators’ are wrongly flagged as ‘true’. Such cases are filtered out when all the three features are used.
We first take 60 random samples each from the filtered ‘birth’ cases reported by the SVM filtering for the two time period pairs, T1 (from 318 cases) and T2 (from 329 cases). The precision values of this evaluation are found to be 0.87 (52/60) and 0.75 (45/60) respectively, quite consistent with those reported in Table 6. We do another experiment in order to estimate the performance of our model for detecting novel sense, independent of the method of Mitra et al. (2014). We take 100 random words from the two time point pairs (T1 and T2), along with all the induced clusters from the newer time period and run the proposed SVM filtering approach to flag the novel ‘birth’ senses. According to our model, for T1 and T2 respectively, 13 out of predicted 24 words and 13 out of 21 predicted words are flagged to be having novel sense achieving precision values of 0.54 and 0.62 on manual evaluation, which itself is quite decent. Note that, for some cases, multiple clusters of a single word have been flagged as novel senses and we observe that these clusters hold a similar sense. For both of these experiments we use the same set of 49 reported ‘birth cases’ by Mitra et al. (2014) to train the SVM classifier (Tables 9 and 10).
We explore the effect of fractional change of IAS and AST over time by plugging them as features into the SVM classifier. We first try to find out the best hyper-parameter settings for the feature combination of IAS and AST by grid search. We try out different values of the primary hyper-parameters (dimension of vectors, number of random walks to start at each node, length of random walk starting at each node) to obtain the best settings. We experiment with dimension of vectors: (d) = {80, 128, 300}; number of random walks to start at each node (nrw) = {10, 20}; length of random walk starting at each node (lrw) = {10, 20, 40, 80}. From the results presented in Table 12, we see for d=128,nrw=10,lrw=40<math><mrow is="true"><mi is="true">d</mi><mo linebreak="goodbreak" is="true">=</mo><mn is="true">128</mn><mo is="true">,</mo><mi is="true">n</mi><mi is="true">r</mi><mi is="true">w</mi><mo linebreak="goodbreak" is="true">=</mo><mn is="true">10</mn><mo is="true">,</mo><mi is="true">l</mi><mi is="true">r</mi><mi is="true">w</mi><mo linebreak="goodbreak" is="true">=</mo><mn is="true">40</mn></mrow></math> we get the best consistent performance over T1 and T2 and hence we continue with this hyper-parameter setting (BDHS) for further analysis.
We observe from the first rows of Tables 13 and 14, that for both the time period pairs (T1 and T2, respectively), using only features obtained using Deepwalk (with best possible hyper-parameter setting) - IAS and AST, does not help improving the performance when compared to the settings in which only network measures are used (Table 6). We next try to combine the simple network features (ED, SS and APL) with IAS and AST as well in different combinations and present the results in Tables 13 and 14 for T1 and T2, respectively. We observe that even though the combination of ED, SS, APL, AST gives the best overall performance beating the performance of the model using only network measures (ED, SS, APL) for T1, it shows inconsistent performance for T2 producing poorer performance while compared with the model where only network measures (ED, SS, APL) are used.
We further investigate the usefulness of IAS and AST while those are computed from the network embeddings obtained from node2vec. The results for both time period pairs (T1 and T2) are presented in Table 15 and Table 16, respectively. We observe that even though appending AST to (ED, SS, APL) improves the precision for both the time period pairs (T1 and T2), it leads to decrease of F-measure while compared with using only the network measures. We obtain these results for the default hyper-parameter settings of node2vec as follows: number of random walks to start at each node = 10; length of random walk starting at each node = 80; skipgram window size = 10; Inout = 1; Return = 1. We also try with the hyper-parameter settings for which we get the best results for Deepwalk (BDHS) by setting skipgram window size = 5 and the length of random walk starting at each node = 40, the results of which are presented in the last rows of Tables 15 and 16 for T1 and T2, respectively. We observe that even in this hyper-parameter setup, node2vec produces poor F-measure score compared to the model using only complex network measures.
We see in all the experiments using network embeddings (for both Deepwalk and node2vec) that no combination of features produces the best performance consistently for both T1 and T2. As we are mapping network to vector space, there is a chance that we are missing some information which leads to inconsistent feature values for T1 and T2, causing inconsistent performances. On the other hand, simple network measures perfectly capture the change in the networks which leads to consistent feature values producing a consistent performance for T1 and T2.
So far we have discussed the usefulness of features obtained using Deepwalk and node2vec independently. In this section, we try to further combine the two features (IAS, AST) obtained both using Deepwalk and node2vec (naming them IASD, ASTD and IASN, ASTN respectively) along with simple network measures (ED, SS, APL). To validate the usefulness of all seven features, we first check the Pearson’s correlation among the seven features, the heatmap of which is presented in Fig. 4. Then we perform feature leave-one-out experiments for network embedding features with the proposed three network measures taken into consideration for all the combinations. From the results presented in Tables 17 and 18, we do not find any single feature combination which beats the performance of simple network measures (ED, SS, APL) consistently for both T1 and T2.
All these analysis using network embeddings shows that even though network representation learning methods are supposed to capture network properties better by embedding the nodes in the network to a vector space, for a task like novel word sense detection explicit complex network features prove to be more useful, despite they are more efficient to compute. We see the same trend even when we try to obtain the embeddings directly from the corpus using fastText (described in Section 5.4), leading to the fact that a carefully curated small set of network measures can be more useful than high dimensional embeddings for this task of novel sense detection.
In this analysis we investigate whether embeddings obtained directly from corpus using fastText (Bojanowski et al., 2017) can help boosting the performance for this task. The results for T1 and T2 are presented in Table 19 and Table 20, respectively. We see that using only IAS and AST does not help to improve the F-measure compared to using only the network measures, whereas when both these types of features are merged, it helps to boost the performance in different combinations for different time period pairs (T1 and T2).
We further analyze the cases, which are labeled as ‘true birth’ by the SVM but are evaluated as ‘false’ by the human evaluators. We find that in most of such cases, the sense cluster reported as ‘birth’ contained many new terms (and therefore, the network properties have undergone change) but the implied sense was already present in one of the previous clusters with very few common words (and therefore, the new cluster contained > 80% new words and is being reported as ‘birth’ in Mitra et al. (2014)). Two such examples are given in Table 21. The split-join algorithm proposed in Mitra et al. (2014) needs to be adapted for such cases.
We also analyze the ‘true negatives’ cases, which are labeled as ‘false birth’ by the SVM filtering but are evaluated as ‘true’ by the human evaluators. Two such examples are given in Table 22. By looking at the feature values of these cases, it is clear that the network structure of the induced subgraph is not changing much, yet they undergo sense change. The probable reason could be that the target word was not in the network of the induced subgraph in the old time point and enters into it in the new time point. Our SVM model is unable to detect this single node injection in a network so far. Handling these cases would be an immediate future step to improve the recall of the system.
We also observe that even though for time point T1, we get good results for feature combination Δ(ED, SS, APL, AST) while using Deepwalk, it fails to improve the performance for T2. Therefore, we dig into the reason behind the fall in the performance for T2 and do error analysis. Note that, the training set is from T1 and while the mean of Δ(AST) for ‘true birth’ cases is 1.02, the value is 0.52 for ‘false birth’ cases. However, there are examples like ‘tans’, ‘guitarist’, ‘conformist’, etc., which are correctly predicted as ‘false’ by the classifier model using only complex network measures (Δ(ED, SS, APL)) as features, but are wrongly flagged as ‘true’ when we add Δ(AST) to the feature set. On analysis, we find that the Δ(AST) values for these cases are 1.93, 2.33 and 1.68, respectively. Similarly, target words like ‘regularization’, ‘rewarming’ are correctly predicted as ‘true’ by the classifier model using only the complex network measures (Δ(ED, SS, APL)) as features, but are wrongly flagged as ‘false’ when we add Δ(AST) to the feature set, because of the low values, 0.59 and 0.44, respectively. Network embedding frameworks attempt to put nodes with similar network properties close in the vector space and the properties include both the local and global neighborhood structures. As the training set is from T1 and test set is from T2, it seems that the network pattern, especially the global pattern which is captured by network embedding feature (AST), does not change in the same way for T1 and T2 leading to different range of feature values for both the classes in training and test set whereas the local neighborhood changing pattern captured by complex network measures are close for T1 and T2, leading to decent performance.
So far, we have reported experiments on discovering novel senses from data and measured the accuracy of our method using manual evaluation. In this section, we evaluate the diachronic validity of our method on another task of detecting known shifts. We test whether our proposed method is able to capture the known historical shifts in meaning. For this purpose, we create a reference list L of 15 words that have been suggested by prior work (Eger, Mehler, 2016, Hamilton, Leskovec, Jurafsky, 2016a, Hamilton, Leskovec, Jurafsky, 2016b) as having undergone a linguistic change and emerging with a novel sense. Note that, we focus only on nouns that emerge with a novel sense between 1900 and 1990. The goal of this task is to find out the number of cases for which our method is able to detect a novel sense from the list L, which in turn would prove the robustness of our method.
Data: Consistent with the prior work, we use the Corpus of Historical American (COHA).6 COHA corpus is carefully created to be genre balanced and is a well constructed prototype of American English over 200 years, from the time period 1810 to 2000. We extract the raw text data of two time slices: 1880–1900 and 1990–2000 for our experiment.
Experiment details and results: We first construct distributional thesauri (DT) networks (Riedl & Biemann, 2013) for the COHA corpus at two different time periods, 1880–1900 and 1990–2000. We apply Chinese Whispers algorithm (Biemann & Bosch, 2011) to produce a set of clusters for each target word in the DT network. The Chinese Whispers clusters for the target word ‘web’ are shown in Fig. 5. Note that we have reported only some of the representative words for each cluster. Each of the clusters represents a particular sense of the target. We now compare the sense clusters extracted across two different time periods to obtain the suitable signals of sense change following the approach proposed in Mitra et al. (2014). After getting the novel sense clusters, we pick up 50 random samples, of which 25 cases are flagged as ‘true birth’ while the other 25 cases are flagged as ‘false birth’ by manual evaluation. We use these 50 samples as our training set for classification using SVM. Some of the examples of this training set are presented in Table 23. We ensure that none of the words in the list L is present in the training set. Using this training set for our proposed SVM classifier, we are successfully able to detect 80% of the cases (12 out of 15) from the list L as having a novel sense. Table 24 presents all of these detected words along with the novel senses and the discriminative network feature. Our method is unable to detect three cases -‘gay’, ‘guy’ and ‘bush’. For ‘gay’, since there is no sense cluster in the older time period with ‘gay’ being a noun, cluster comparison does not even detect the ‘birth’ cluster of ‘gay’. The ‘birth’ sense clusters for ‘guy’, ‘bush’ in the new time period, as detected by split-join algorithm contain general terms like “someone, anyone, men, woman, mother, son” and “cloud, air, sky, sunlight” respectively. As the network around these words did not change much over time, our method found it difficult to detect. Note that even though COHA corpus is substantially smaller than the Google n-grams corpus, our approach produces promising results, showing the usability of the proposed method with corpora of limited size as well.
In this study, we dealt with the task of improving the performance of novel sense detection task by borrowing concepts from complex network theory, which is an attempt of first of its kind. In order to improve the precision of detecting words evolved with a new sense over time, we demonstrated how the change in the network properties of the induced subgraphs from a sense cluster can be used. In addition, to investigate the superiority of complex network measures in this task, we explore the measures computed from the network representation learning framework as well. Manual evaluation over two different time period pairs shows that the proposed SVM classification approach boosts the precision values from 0.23–0.32 to 0.74–0.86 with a decent F-measure value of 0.67–0.68 when fed with only complex network measures. Even though the combination of complex network measures and network embedding measures improve the precision further to 0.76–0.91 in different scenarios, the F-measure falls significantly proving the superiority of complex network measures over network embedding measures. Note that, using only network embedding measures as features to the classification model leads to very poor performance compared to the model using only complex network measures. This study also shows that if we can intelligently apply complex network theory to come up with some intuitive measures to be used in a problem dealing with only local network structures, it can produce better or comparable performance than the network embedding measures which is otherwise computationally heavy to compute. Finally, from the experiments on the COHA corpus, we have also shown that our approach can reliably detect the words known to have sense shifts. The gold standard dataset prepared by us for validating novel sense detection is one of our main contributions and is made available publicly7.
In future, we plan to apply our methodology to different genres of corpus, like social network data, several product or movie reviews data which are becoming an increasingly popular source for opinion tracking, to identify short-term changes in word senses or usages. These analyses would also provide insights into the evolution of language in a short span of time. Our ultimate goal is to prepare a generalized framework for accurate detection of sense change across languages and investigate the triggering factors behind language evolution as well.
Download : Download XML file (361B)Supplementary Data S1. Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/