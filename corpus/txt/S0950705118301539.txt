Bipartite networks comprise a particular class of network models in which the set of vertices is split into two disjoint subsets, with edges connecting vertices placed in different sets. Also known as two-layer networks, they provide a powerful representation of relationships in many real-world systems, including document-word [1], protein-ligand [2], actor-movie [3], georeferenced user-location [4] and paper co-authorship or citation networks [5]. Bipartite network models have been widely employed in hard combinatorial optimization problems that require finding a minimum (or maximum) cost, wherein the number of possible states is finite and usually exponential. Many such problems, e.g., biclique, matching, vertex cover, community structure, traveling salesman and network coloring [6] have proven to be NP<math><mi mathvariant="script" is="true">NP</mi></math>-complete or NP<math><mi mathvariant="script" is="true">NP</mi></math>-hard.
Multilevel techniques are being investigated as a global strategy to handle decision-making and optimization problems in a variety of application domains. We refer the reader interested in applications to management problems to recent literature on the topic [7], [8], [9]. In this paper we investigate the multilevel strategy to handle computationally expensive optimization problems in bipartite networks. In this context, the approach consists of iteratively coarsening an original network into a hierarchy of smaller sized approximations. A starting solution is obtained in the coarsest network and successively projected back and refined over the inverse sequence of coarsened networks, until the original one. Previous studies demonstrated the strategy enables running computationally expensive algorithms on large networks with no significant loss in solution quality [10], [11], [12], [13], [14], [15], [16]. Sciences [17] and Walshaw [18] argued over the relevance and feasibility of the multilevel strategy for solving combinatorial optimization problems. Empirical evidence has been shown by Walshaw [18] that the coarsening process filters the solution space by gradually removing irrelevant high-cost solutions and drastically reducing the search space, and hence, optimization convergence times.
Multilevel algorithms have been applied to many classic network problems, including network coloring [19], traveling salesman [20], network drawing [19], network partitioning [21] and computation of centrality measures [22]. However, current multilevel approaches are not directly applicable to bipartite networks and, to the best of our knowledge, the multilevel strategy has not been considered in this context.
We address this gap and introduce a novel multilevel optimization approach applicable to bipartite networks. Furthermore, we describe an implementation of this approach as a general-purpose multilevel framework that incorporates two novel efficient matching algorithms, as well as novel contracting and uncoarsening algorithms.
In order to illustrate its potential, we employed the framework to handle two distinct problems defined in bipartite networks, namely community detection and dimensionality reduction. In the community detection problem, tests on a large set of synthetic networks demonstrated that, combined with a proper local search strategy, it yields good speedups and preserves solution quality. When employed to perform dimensionality reduction in text classification it yielded encouraging results in terms of both runtime and accuracy as compared with a standard dimensionality reduction technique.
The remainder of the paper is organized as follows: Section 2 reviews some basic concepts on networks and provides an overview of the standard multilevel approach. Section 3 discusses previous work and application of multilevel strategies on combinatorial optimization problems. Section 4 introduces a multilevel formulation for bipartite networks and its implementation. Section 5 reports our empirical results, which include (i) an analysis of how the multilevel representation impacts the topological features of a real bipartite network; (ii) an empirical study of instantiating the framework to solve the community detection problem on a large synthetic test suite; and (iii) an empirical study of its application to dimensionality reduction in text classification. Section 6 briefly discusses how the general framework can be tuned for application in other types of problems. Finally, Section 7 summarizes our findings and discusses future work.
Let G=(V,E,σ,ω)<math><mrow is="true"><mi is="true">G</mi><mo is="true">=</mo><mo is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo is="true">,</mo><mi is="true">σ</mi><mo is="true">,</mo><mi is="true">ω</mi><mo is="true">)</mo></mrow></math> be an undirected weighted network, where V={1,⋯,n}<math><mrow is="true"><mi is="true">V</mi><mo is="true">=</mo><mo is="true">{</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><mi is="true">n</mi><mo is="true">}</mo></mrow></math> denotes the set of vertices and E⊆V x V denotes the set of edges, such that (v,u)={(u,v)=(v,u)∣u,v∈V}<math><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo><mo is="true">=</mo><mo is="true">(</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">u</mi><mo is="true">)</mo><mo is="true">∣</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">∈</mo><mi is="true">V</mi><mo stretchy="true" is="true">}</mo></mrow></mrow></math>. Let n=|V|<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mo is="true">|</mo><mi is="true">V</mi><mo is="true">|</mo></mrow></math> be the total number of vertices and m=|E|<math><mrow is="true"><mi is="true">m</mi><mo is="true">=</mo><mo is="true">|</mo><mi is="true">E</mi><mo is="true">|</mo></mrow></math> be the total number of edges, where operator “|.|” stands for the cardinality of a set. The weight of an edge (u, v) is represented by ω(u, v) with ω:VxV→R*<math><mrow is="true"><mi is="true">ω</mi><mo is="true">:</mo><mi is="true">V</mi><mtext is="true">x</mtext><mi is="true">V</mi><mo is="true">→</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mo is="true">*</mo></msup></mrow></math> and the weight of a vertex v is represented by σ(v) with σ:V→R*<math><mrow is="true"><mi is="true">σ</mi><mo is="true">:</mo><mi is="true">V</mi><mo is="true">→</mo><msup is="true"><mi mathvariant="double-struck" is="true">R</mi><mo is="true">*</mo></msup></mrow></math>.
A network G=(V,E,σ,ω)<math><mrow is="true"><mi is="true">G</mi><mo is="true">=</mo><mo is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo is="true">,</mo><mi is="true">σ</mi><mo is="true">,</mo><mi is="true">ω</mi><mo is="true">)</mo></mrow></math> is bipartite (two-layer network) if V is partitioned into two sets V1 and V2, such that V1∩V2=∅<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">∩</mo><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">=</mo><mi is="true">∅</mi></mrow></math> and E⊆V1 x V2. Hereafter, each vertex subset is called a layer. A bipartite network thus has two layers so that vertices in the same layer are not connected.
The degree of a vertex v ∈ V, denoted κv, is given by the total weight of its adjacent edges, i.e. κv=∑u∈Vw(v,u)<math><mrow is="true"><msub is="true"><mi is="true">κ</mi><mi is="true">v</mi></msub><mo is="true">=</mo><msub is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">u</mi><mo is="true">∈</mo><mi is="true">V</mi></mrow></msub><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">u</mi><mo is="true">)</mo></mrow></mrow></math>. The h-hop neighborhood of v, denoted Γh(v), is formally defined as the vertices in set Γh(v)={u|<math><mrow is="true"><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mi is="true">h</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><mi is="true">u</mi><mspace width="0.33em" is="true"></mspace><mo is="true">|</mo></mrow></mrow></math> there is a path of length h between v and u}. Thus, the 1-hop neighborhood of v, Γ1(v), is the set of vertices adjacent to v; the 2-hop neighborhood, Γ2(v), is the set of vertices 2-hops away from v, and so forth.
A similarity score S(u, v) can be computed from a pair of vertices u and v. A fundamental structural similarity function between a pair of vertices is given by the number of common neighbors, defined as Scn(u,v)=|Λ(u,v)|,<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">|</mo><mstyle mathvariant="normal" is="true"><mi is="true">Λ</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">|</mo></mrow><mo is="true">,</mo></mrow></math> Λ(u,v)={Γ1(u)∩Γ1(v)}<math><mrow is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Λ</mi></mstyle><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mo is="true">{</mo><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">∩</mo><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">}</mo></mrow></math>. Alternatively, a weighted common neighbors similarity function can be defined by Eq. (1), where the term log(1+s(z))<math><mrow is="true"><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><mo is="true">(</mo><mn is="true">1</mn><mo is="true">+</mo><mi is="true">s</mi><mo is="true">(</mo><mi is="true">z</mi><mo is="true">)</mo><mo is="true">)</mo></mrow></math> is used to prevent negative scores [23].(1)Swcn(u,v)=∑z∈Λ(u,v)ω(u,z)+ω(v,z)log(1+s(z))s(u)=∑z∈Γ1(u)ω(u,z)<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">z</mi><mo is="true">∈</mo><mstyle mathvariant="normal" is="true"><mi is="true">Λ</mi></mstyle><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo></mrow></munder><mfrac is="true"><mrow is="true"><mi is="true">ω</mi><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">z</mi><mo is="true">)</mo><mo is="true">+</mo><mi is="true">ω</mi><mo is="true">(</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">z</mi><mo is="true">)</mo></mrow><mrow is="true"><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><mo is="true">(</mo><mn is="true">1</mn><mo is="true">+</mo><mi is="true">s</mi><mo is="true">(</mo><mi is="true">z</mi><mo is="true">)</mo><mo is="true">)</mo></mrow></mfrac><mspace width="1em" is="true"></mspace><mi is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">z</mi><mo is="true">∈</mo><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow></mrow></munder><mi is="true">ω</mi><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">z</mi><mo is="true">)</mo></mrow></mrow></math>
The local clustering coefficient of a vertex is given by the probability of its neighbors being connected [3], [24]. This statistics is closely related to transitivity, which measures the relative frequency of triangles in the vertex neighborhood. The clustering coefficient is defined by Eq. (2):(2)cc(v)=2tr(v)|Γ1(v)|(|Γ1(v)|−1),<math><mrow is="true"><mi is="true">c</mi><mi is="true">c</mi><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">2</mn><mi is="true">t</mi><mi is="true">r</mi><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">|</mo><mo is="true">(</mo><mo is="true">|</mo></mrow><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">|</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></mfrac><mo is="true">,</mo></mrow></math>where tr(u) denotes the number of edges (v, z) ∈ E, such that v, z ∈ Γ1(u) and cc relies on the enumeration of the triangles in the network. However, as triangles do not occur in bipartite networks this definition is not valid. An equivalent metrics for bipartite graphs was introduced by Latapy et al. [25], defined in Eq. (3), which captures the overlap between vertex neighborhoods in the same layer.(3)ccb(v,u)=|Γ1(u)∩Γ1(v)||Γ1(u)∪Γ1(v)|ccb(v)=∑v∈Γ2(u)ccb(v,u)|Γ2(u)|,<math><mrow is="true"><mi is="true">c</mi><msub is="true"><mi is="true">c</mi><mi is="true">b</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">∩</mo><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">|</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">∪</mo><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">1</mn></msub><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">|</mo></mrow></mrow></mfrac><mspace width="2.em" is="true"></mspace><mi is="true">c</mi><msub is="true"><mi is="true">c</mi><mi is="true">b</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">v</mi><mo is="true">∈</mo><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">2</mn></msub><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow></mrow></msub><mi is="true">c</mi><msub is="true"><mi is="true">c</mi><mi is="true">b</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">u</mi><mo is="true">)</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">2</mn></msub><mrow is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">)</mo></mrow><mo is="true">|</mo></mrow></mrow></mfrac><mo is="true">,</mo></mrow></math>
A multilevel optimization is formally defined as a meta-heuristics that combines different heuristics to guide, modify and possibly fix a solution obtained from a target algorithm (or operations of the subordinate heuristics, local search or global search) and refines this solution over multiple iterations. It operates in three phases, namely coarsening, solution finding and uncoarsening. In the coarsening phase, the network size is successively reduced to obtain coarser network representations; in the solution finding phase a starting solution is obtained applying the target algorithm in the coarsest representation; in the uncoarsening phase, the starting solution is successively projected back to the intermediate networks and refined, until obtaining the final solution.
Fig. 1 illustrates such a process, considering an initial network G0 (in which the original problem instance is defined), where GL denotes the coarsest network obtained after L coarsening steps (levels), SL denotes the starting solution obtained in GL, and S0 denotes the final refined solution obtained in G0.
The coarsening phase constructs a hierarchy of coarsened networks Gl from the initial network G0, yielding intermediate network approximations on multiple levels-of-detail. The process requires two algorithms, namely matching, which defines which vertices will be merged, and contracting, which builds the reduced representation, given the matching. Let Gl=(Vl,El,σl,ωl)<math><mrow is="true"><msub is="true"><mi is="true">G</mi><mi is="true">l</mi></msub><mo is="true">=</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">V</mi><mi is="true">l</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">E</mi><mi is="true">l</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">σ</mi><mi is="true">l</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">ω</mi><mi is="true">l</mi></msub><mo is="true">)</mo></mrow></mrow></math> be the network model coarsened at level l, with |V0|>|V1|>…>|Vl|<math><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mi is="true">V</mi><mn is="true">0</mn></msub><mrow is="true"><mo is="true">|</mo><mo is="true">&gt;</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mrow is="true"><mo is="true">|</mo><mo is="true">&gt;</mo><mo is="true">…</mo><mo is="true">&gt;</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">V</mi><mi is="true">l</mi></msub><mrow is="true"><mo is="true">|</mo></mrow></mrow></math>.
Coarsening starts with the matching step. According to some given restriction, in general, pairs of vertices are selected for matching, producing a set of unordered pairs called vertex matching, independent edge set or simply matching. Formally, a matching M consists of a set of pairwise non-adjacent edges. Heavy-edge matching is a popular algorithm for this purpose, which attempts to find a matching of maximal weight [26].
At any level, the coarsening of a network must preserve its topological features, implying that vertex and edge weights of the reduced network must reflect the connectivity of its parent. This will be guaranteed by a proper choice of matching strategy, which is a key component of effective multilevel optimizations. A matching strategy inadequate to support the solution finding phase will impair the quality of the solution derived by a multilevel algorithm and its performance.
Once the matching is defined, a contracting algorithm constructs the coarsened network, by joining matched vertex pairs into a single super-vertex (sV). A child network Gl+1<math><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> will inherit the non-joined vertices from its parent. In order for Gi+1<math><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> to be a good proxy to its parent, given a super-vertex sV={v,u}∈Vi+1<math><mrow is="true"><mi is="true">s</mi><mi is="true">V</mi><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><mi is="true">v</mi><mo is="true">,</mo><mi is="true">u</mi><mo is="true">}</mo></mrow><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math> its weight σ(sV) is computed as the sum of weights σ(v) and σ(u), {u, v} ∈ Vi. Furthermore, the edges incident to vertices {u, v} ∈ Vi are joined to obtain the so-called super-edges incident to sV.
This phase employs the target algorithm to solve the problem on the smallest network Gl. Let S be the set of all possible solutions in the coarsest problem instance. Given an objective (cost) function f:S→R(orN<math><mrow is="true"><mi is="true">f</mi><mo is="true">:</mo><mi is="true">S</mi><mo is="true">→</mo><mi mathvariant="double-struck" is="true">R</mi><mspace width="4.pt" is="true"></mspace><mtext is="true">(or</mtext><mspace width="4.pt" is="true"></mspace><mi mathvariant="double-struck" is="true">N</mi></mrow></math>) that assigns a cost to each solution in S, the aim is to find a state s ∈ S with minimum (or maximum) cost. For instance, in the traveling salesman problem f(s) expresses the length of tour s, whereas in a network community detection problem it denotes some measure of community quality. Since the coarsest network is possibly very small, it becomes feasible to employ computationally expensive target algorithms to find a starting solution [11].
The uncoarsening (also known as solution projection) phase successively transfers the solution available at a current level to the upper level in the hierarchy, i.e., the solution obtained in the coarsest network Gl is successively projected through intermediate networks Gl−1,Gl−2,⋯,G1<math><mrow is="true"><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">G</mi><mn is="true">1</mn></msub></mrow></math> up to the original network G0.
Solution Sl is constructed from Sl+1<math><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> simply by assigning vertices {u, v} ∈ Vl to the same set of their parent sV∈Vl+1<math><mrow is="true"><mi is="true">s</mi><mi is="true">V</mi><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math>. Although Sl is a local minima of f in Gl, this may not be the case of solution Sl−1,<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo></mrow></math> derived for the upper level Gl−1,<math><mrow is="true"><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo></mrow></math> with respect to Gi. Therefore, a refinement heuristics can be applied to avoid local minima and improve solution quality. Local operations can move the solution towards a lower cost neighboring solution in the search space; for instance, in a community detection problem vertices can be moved between adjacent communities to improve a target quality measure.
Early studies of multilevel optimization were mostly designed to speed up the recursive bisection problem, as its high computational cost prevents wider applicability [27]. One of the first theoretical analysis was presented by Karypis and Kumar [11], who demonstrated multilevel approaches can find high-quality communities in a variety of networks. Later, Karypis and Kumar [26] introduced the now widely adopted matching algorithms HEM (Heavy Edge Matching), LEM (Light Edge Matching) and MCH (Modified Edge Matching). Other studies relevant for the development and expansion of the multilevel approach were conducted by Walshaw and Cross [28], who presented a theoretical multilevel formulation of the Kernighan–Lin method for mesh partitioning, and by Korosec et al. [29], who introduced a new multilevel colony optimization applicable to several optimization problems. Sciences [17] and Walshaw [18] provided compelling evidence the multilevel framework is an extremely useful addition to combinatorial optimization toolkits, although it can not be considered a panacea.
Previous work on multilevel optimization in complex networks can be broadly organized into four categories. A first category encompasses studies that explore features of network domains; e.g., Abou–Rjeili and Karypis [30] studied scale-free networks (with power-law degree distribution), Oliveira and Seok [31] designed a multilevel spectral approach that explores features of biological networks (protein complexes) and Valejo et al. [14] considered properties of social networks, such as high transitivity and assortativity. A second group comprises contributions focused on scalability, e.g., investigating parallel and distributed paradigms that improve the performance of the coarsening and refinement phases [16], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44]. A third category of studies is concerned with the optimization of a target objective function, e.g., several contributions focused on improvements in modularity [13], [16], [45], [46], [47], [48], [49], [50]. Finally, there are significant contributions in applications, including (and not limited to) graph coloring [51], traveling salesman problem [20], graph drawing [19], biomedical feature selection [52], covering design [53], DNA sequencing [54], vehicle routing [55], semi-supervised learning [56], partitioning or community detection [21] and computation of centrality measures [22].
We are not aware of any previous effort concerned with the usage of multilevel strategies to solve optimization problems in bipartite networks, despite the relevance of this kind of network to real-world modeling problems. Current multilevel methods cannot be directly applied in bipartite models without adaptations, e.g., they do not consider vertex types, whereas the layers in a bipartite network usually represent distinct types of entities that must be handled independently. For the sake of illustration, suppose a text document collection modeled as a document-word bipartite network. For a start, matching vertices that represent words and vertices that represent documents (i.e., matching of vertices of different layers) would not be meaningful in most application scenarios. Moreover, as the number of words is typically much higher, coarsening the word layer may be sufficient to reduce the asymptotic convergence of a target algorithm.
This section introduces a multilevel optimization framework designed to handle bipartite networks. Bearing in mind the previous discussion, we have defined two restrictions that establish the major distinction between current multilevel methods and the one proposed here:
Vertices are only allowed to match their set of two-hop neighbors.
The matching algorithm must operate on vertices of the same layer.
Such restrictions support cost-effective implementations of multilevel strategies in bipartite networks. From the definitions of bipartite networks and two-hop neighborhoods, the first restriction implies adjacent vertices are not matched. Furthermore, vertices can only match others in their two-hop neighborhood set, rather than any non-adjacent vertex. Considering, for instance, the network in Fig. 2, vertex u1 can match vertices u2 and u3, which are non-adjacent and are in Γ2(u1).
The first restriction alone does not enforce independent handling of layers, which is guaranteed by the second restriction, which states layers (either one of them or both) will be processed independently in the coarsening phase. Therefore, the coarsening will not match vertices of different kinds and distinct coarsening and refinement algorithms, or distinct parameterizations of the same algorithm may be adopted in processing each layer. Such a restriction also favors the adoption of distributed or parallel processing strategies.
Algorithm 1 summarizes the general multilevel optimization framework for bipartite networks (MOb). Similarly to the standard multilevel approach, it comprises the phases of coarsening (lines 1–4), solution finding (line 5) and uncoarsening (lines 7–8). It takes as inputs the initial bipartite network G=(V1∪V2,E,σ,ω),<math><mrow is="true"><mi is="true">G</mi><mo is="true">=</mo><mo is="true">(</mo><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">∪</mo><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mi is="true">E</mi><mo is="true">,</mo><mi is="true">σ</mi><mo is="true">,</mo><mi is="true">ω</mi><mo is="true">)</mo><mo is="true">,</mo></mrow></math> and for each layer a maximum number of coarsening levels L and a layer reduction factor rf.
The coarsening is applied to each layer (line 1), level by level until the desired reduction factor is attained, by calls to a matching algorithm (line 3) and a contracting algorithm (line 4). The matching algorithm considers the above restrictions in selecting the vertex pairs to produce the list of independent edges (line 3). The reduction factor (rf ∈ [0, 0.5]) is multiplied by the number of vertices to determine the maximum matching number. If rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math> (the maximum reduction factor), each coarsening iteration will (potentially) reduce the number of vertices by a factor of two, yielding a logarithmic decrease in network size along the process. This is not guaranteed if the network is disconnected or highly sparse, since in this case there may be an insufficient number of edges. The call to the contracting algorithm in the next step (line 4) is responsible for creating the coarsened bipartite network, in which any matched vertex pairs have been merged into super-vertices.
The target algorithm is then executed in the coarsest network Gl to obtain a starting solution Sl (line 5). Finally, in the subsequent uncoarsening phase this solution is projected back, up to G0, through the space of intermediate solutions Sl−1,Sl−2,…,S1,S0<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">2</mn></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">S</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">S</mi><mn is="true">0</mn></msub></mrow></math> (lines 7–9), possibly refining the solutions at each level (line 8). Although we do not investigate refinement algorithms in this work, the rationale is to apply a local search strategy to improve the current solution, i.e., algorithms should explore small regions of the solution space, in order to reduce impact on performance and scalability. We do introduce novel algorithms for matching, contracting and uncoarsening.
We propose a straightforward matching algorithm called random greedy matching for bipartite networks (RGMb), described in Algorithm 2, where S(u, v) denotes a similarity function.
A vertex u is picked randomly (Line 4) at each iteration, and as long as it has not yet been matched, one of its unmatched two-hop neighbors v so that S(u, v) is maximal is chosen (Line 5). Vertices u and v are marked as matched and removed from the list of unmatched vertices (Line 7). The process iterates until no more vertices can be eliminated from the list. For unweighted edges, a random neighbor is selected for matching, otherwise, the heaviest adjacent edge is selected.
One may consider different similarity functions, e.g., common neighbors similarity Scn or weighted common neighbors similarity Swcn. For illustration, consider a graph G=(V1∪V2,E),<math><mrow is="true"><mi is="true">G</mi><mo is="true">=</mo><mo is="true">(</mo><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">∪</mo><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mi is="true">E</mi><mo is="true">)</mo><mo is="true">,</mo></mrow></math> shown in Fig. 3, with V1={u1,u2,u3,u4,u5}<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">3</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">4</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">5</mn></msub><mo is="true">}</mo></mrow></mrow></math> and V2={v1,v2,v3,v4,v5},<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">v</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">3</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">4</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">5</mn></msub><mo is="true">}</mo></mrow><mo is="true">,</mo></mrow></math> Γ2(u2)={u1,u3,u4}<math><mrow is="true"><msub is="true"><mstyle mathvariant="normal" is="true"><mi is="true">Γ</mi></mstyle><mn is="true">2</mn></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">3</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">4</mn></msub><mo is="true">}</mo></mrow></mrow></math> and u1, u2, u3 and u4 unmatched. Suppose vertex u2 has been randomly chosen for matching. Adopting Scn as the similarity function, then Scn(u2,u1)=2,<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">2</mn><mo is="true">,</mo></mrow></math> Scn(u2,u3)=3<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">3</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">3</mn></mrow></math> and Scn(u2,u4)=1,<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">4</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> therefore, pair {u2, u3} would be included in the matching, as illustrated in Fig. 3(a). Alternatively, for a choice of Swcn as similarity function, Swcn(u1,u2)=11,<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">11</mn><mo is="true">,</mo></mrow></math> Swcn(u1,u3)=7<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">3</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">7</mn></mrow></math> and Swcn(u2,u4)=2.5<math><mrow is="true"><msub is="true"><mi is="true">S</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">4</mn></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mn is="true">2.5</mn></mrow></math> and pair {u1, u2} would be included, as illustrated in Fig. 3(b).
Algorithm RGMb does not ensure an optimal choice over all possible matchings. An alternative strategy would choose from a list of vertex pairs sorted in decreasing order of similarity scores, which may be kept in an appropriate data structure, e.g., a heap or a priority queue. An alternative algorithm that implements this strategy is called greedy sorted matching for bipartite networks (GMb) (Algorithm 3): it selects the best possible match for a vertex from its two-hop neighborhood (line 4).
GMb constructs a priority queue of potential matches ranked by similarity (Lines 3–5) and uses it to retrieve optimal matching choices (Line 8). In case of a tie, it makes a random choice, in order to favor further exploration of the solution space over multiple iterations. If a vertex is selected that has already been matched, it is skipped. GMb, albeit slower than its random search counterpart RGMb, is more robust and yields better performance (see Section 5).
For illustration, consider the graph G=(V1∪V2,E)<math><mrow is="true"><mi is="true">G</mi><mo is="true">=</mo><mo is="true">(</mo><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">∪</mo><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mi is="true">E</mi><mo is="true">)</mo></mrow></math> depicted in Fig. 4, with V1={u1,u2,u3,u4,u5}<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">3</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">4</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mn is="true">5</mn></msub><mo is="true">}</mo></mrow></mrow></math> and V2={v1,v2,v3,v4,v5}<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">v</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">3</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">4</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">5</mn></msub><mo is="true">}</mo></mrow></mrow></math>. Suppose the matching is being performed on layer V1. Adopting the Scn similarity, pairs {u2, u3} and {u4, u5} would be included in the matching, as illustrated in Fig. 4(a). Alternatively, for a choice of Swcn similarity, pairs {u1, u2} and {u4, u5} would be included, as shown in Fig. 4(b).
Variations of the general matching algorithm can be obtained depending on the combined choices of matching strategy and similarity function S(u, v). We report four variants, namely RGMbcn and RGMbwcn for a choice of random matching with, respectively, Scn and Swcn similarity; GMbcn and GMbwcn for the equivalent combinations with greedy matching.
Algorithm 4 (Cb, contracting of a bipartite network from a matching) describes how to create a coarsened bipartite network from a matching M. It takes as input a bipartite network Gl and a corresponding matching M. First, vertex pairs {u, v} ∈ M are mapped into a successor vector (lines 5–6) and joined into a single super-vertex sV∈Gl+1<math><mrow is="true"><mi is="true">s</mi><mi is="true">V</mi><mo is="true">∈</mo><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math> (lines 4–9). The successor vector will be accessed in the uncoarsening phase to assign pair {u,v}∈Gl+1<math><mrow is="true"><mrow is="true"><mo is="true">{</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">}</mo></mrow><mo is="true">∈</mo><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math> to the same subset of its super-vertex sV. Any vertices not included in M are inherited by Gl+1<math><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> (line 10). After the mapping, adjacent edges in El are joined and added to El+1<math><msub is="true"><mi is="true">E</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> (lines 11–18), i.e., each pair (u, v) ∈ El holds its successors (w, z) and if edge (w, z) already exists in El+1<math><msub is="true"><mi is="true">E</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> its weight is increased by ωl(u, v) (lines 15–16); otherwise, a new edge (w, z) with weight ωl(u, v) is inserted into El+1<math><msub is="true"><mi is="true">E</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> (lines 17–18).
Fig. 5 illustrates the contracting process. Considering, for instance, the previous example, in which pairs {u2, u3} and {u4, u5} have been included in the matching, as illustrated in Fig. 5(a), the resulting coarsened network is depicted in Fig. 5(b).
The capability of handling layers with distinct parameter settings (rf and L) and coarsening algorithms is a compelling feature of the proposed framework. This is advantageous in many situations, since layers in many real network models are highly unbalanced in size, as in the already mentioned document-word networks. Coarsening just one of the layers is also a useful feature, for example, in dimensionality reduction problems, e.g., to reduce the space of words in document-word networks, as illustrated in Section 5.3.
The solution finding phase executes the target algorithm to obtain a starting solution SL. The representation of SL depends on the problem being handled. In a community detection problem, it is described as a partitioning of the vertex set into non-empty partitions Pk with ∪Pk=SL,<math><mrow is="true"><mo is="true">∪</mo><msub is="true"><mi is="true">P</mi><mi is="true">k</mi></msub><mo is="true">=</mo><msub is="true"><mi is="true">S</mi><mi is="true">L</mi></msub><mo is="true">,</mo></mrow></math> Pk⊆VL. Each iteration of the uncoarsening phase projects the current solution, obtained in network Gl, to network Gl−1<math><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub></math> as described in Algorithm 5 Ub (uncoarsening of bipartite networks). The rationale is, for each vertex u∈Vl−1<math><mrow is="true"><mi is="true">u</mi><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub></mrow></math> in network Gl−1<math><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">l</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></msub></math> (line 2), to obtain its corresponding successor vertex w ∈ Vl (line 3) and assign vertex u to its partition (lines 4–5). An implementation can keep the successor vector as a global variable, or it may be provided as a network attribute.
As discussed later (Section 6), the algorithm can be adapted with little effort to handle other types of problems and representations, e.g., in link prediction, S would be a set of predicted edges Ep ∉ E. In some problems the solution finding and uncoarsening phases are omitted, e.g., in dimension reduction the coarsest bipartite network is itself the final solution (see Section 5.3).
Consider the multilevel strategy applied to both layers of a network. Computing a matching requires, for each selected vertex u, to find its h-hop neighborhood Γh(u), which takes O(n⟨κ⟩h) time, where ⟨κ⟩ is the average network degree and h specifies the number of distance hops. Most real networks are sparse, i.e., ⟨κ⟩ ≪ n and m ≈ n. This study only considers two-hop neighborhoods (h=2<math><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math>), which leads to notable computational savings, since Γ2 can be reached in nearly linear time. Appropriate data structures and parallelism can be employed to maximize the efficiency of the matching step [57], [58], [59]. The subsequent contracting step relies directly on the matching and can be very fast in sparse networks. A coarsened network can be mapped and built in time O(|E|), since the process is iterated over all edges of its parent network.
If the coarsening parameter is set to its maximum value (rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math>), at each iteration the current network size will be reduced at most by a factor of two relative to its parent. In this case, given 1 < L < n, the coarsening steps would produce networks of sizes (n,n2,⋯,nn),<math><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">2</mn></mfrac><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mi is="true">n</mi></mfrac><mo is="true">)</mo><mo is="true">,</mo></mrow></math> yielding a reduction factor O(log2(n)) over the L iterations. For instance, departing from a network with n=100<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">100</mn></mrow></math> vertices, rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math> and L=2,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn><mo is="true">,</mo></mrow></math> potentially 50 vertex pairs will be merged in the first iteration (from G0 to G1) and 25 in the second (from G1 to G2), a reduction factor of 4. In most real-world problems, such a reduction factor would suffice to produce a manageable model to run the optimization algorithm. As the cost of coarsening decreases drastically at each level, the number of levels L may be neglected in the estimation of the computational cost.
The complexity of the solution finding phase is determined by the target algorithm. If Ttg is the cost of the target algorithm on a bipartite network with n vertices, and Tss is the cost to obtain the starting solution, it is not too difficult to show that Tss=Ttg/(2L)<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mrow is="true"><mi is="true">s</mi><mi is="true">s</mi></mrow></msub><mo is="true">=</mo><msub is="true"><mi is="true">T</mi><mrow is="true"><mi is="true">t</mi><mi is="true">g</mi></mrow></msub><mo is="true">/</mo><mrow is="true"><mo is="true">(</mo><msup is="true"><mn is="true">2</mn><mi is="true">L</mi></msup><mo is="true">)</mo></mrow></mrow></math>. Insofar as the hierarchy of approximations produced by coarsening, L=1<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math> yields (n,n2),<math><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">2</mn></mfrac><mo is="true">)</mo><mo is="true">,</mo></mrow></math> L=2<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math> yields (n,n2,n4),<math><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">2</mn></mfrac><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">4</mn></mfrac><mo is="true">)</mo><mo is="true">,</mo></mrow></math> L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> yields (n,n2,n4,n8)<math><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">2</mn></mfrac><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">4</mn></mfrac><mo is="true">,</mo><mfrac is="true"><mi is="true">n</mi><mn is="true">8</mn></mfrac><mo is="true">)</mo></mrow></math> and so on. Hence, the number of vertices in the coarsest network is approximately n(2L)<math><mfrac is="true"><mi is="true">n</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mn is="true">2</mn><mi is="true">L</mi></msup><mo is="true">)</mo></mrow></mfrac></math> and therefore Tss=Ttg(2L)<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mrow is="true"><mi is="true">s</mi><mi is="true">s</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">T</mi><mrow is="true"><mi is="true">t</mi><mi is="true">g</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msup is="true"><mn is="true">2</mn><mi is="true">L</mi></msup><mo is="true">)</mo></mrow></mfrac></mrow></math>. Parameter L can thus be chosen to control the cost deemed acceptable to obtain an initial solution.
As an illustration, consider a network with n=100<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">100</mn></mrow></math> vertices and two target algorithms, X and Y with TX=O(n)<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mi is="true">X</mi></msub><mo is="true">=</mo><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><mi is="true">n</mi><mo is="true">)</mo></mrow></mrow></math> and TY=O(n2),<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mi is="true">Y</mi></msub><mo is="true">=</mo><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><msup is="true"><mi is="true">n</mi><mn is="true">2</mn></msup><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math> hence, TX=O(100),<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mi is="true">X</mi></msub><mo is="true">=</mo><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><mn is="true">100</mn><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math> TY=O(10,000)<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mi is="true">Y</mi></msub><mo is="true">=</mo><mi is="true">O</mi><mrow is="true"><mo is="true">(</mo><mn is="true">10</mn><mo is="true">,</mo><mn is="true">000</mn><mo is="true">)</mo></mrow></mrow></math> and TX=TY<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mi is="true">X</mi></msub><mo is="true">=</mo><msqrt is="true"><msub is="true"><mi is="true">T</mi><mi is="true">Y</mi></msub></msqrt></mrow></math>. Consequently, if one wishes to execute algorithm Y on a network with n=10,000<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">10</mn><mo is="true">,</mo><mn is="true">000</mn></mrow></math> with a runtime similar to that of algorithm X, the network size should be reduced to n=100,<math><mrow is="true"><msqrt is="true"><mi is="true">n</mi></msqrt><mo is="true">=</mo><mn is="true">100</mn><mo is="true">,</mo></mrow></math> which requires setting L ≈ 7 and rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math>. Of course, the asymptotic complexity of algorithm Y remains quadratic, but the coarsening allows its execution with a runtime equivalent to the linear one.
Analogous to the solution finding phase, the complexity of the uncoarsening phase depends on the target problem (target algorithm), and also on the choice of refinement algorithm, if one is used. In problems such as community detection and classification, it is not necessary to project the solution along the intermediate levels, rather it can be projected directly to the original network, since successors have been stored in an array (or other data structure). In these cases, this phase as implemented in Algorithm 5 would have linear cost in the number of vertices, O(n). As we do not investigate refinement algorithms, an analysis of their asymptotic complexity is suppressed.
We assessed the proposed framework guided by the following research questions:
How does coarsening affect a network’s topological features at each level?
Can the framework improve the efficiency of optimization algorithms without incurring significant losses in the solution quality?
How does the framework impact on the scalability of the local search strategy?
To which extent can it enable the running of high-cost algorithms on large networks?
Is it sufficiently general for handling different combinatorial optimization problems?
We conducted three experimental studies to find answers to the questions. A first study investigated the impact of the coarsening process on the topological properties of a well-known author-paper network. In a second study, the framework was employed in the context of community detection, which is a prototypical application of multilevel strategies. Finally, in a third study, it was employed in dimension reduction in a text classification scenario.
The experiments were executed in a Linux machine with 8-core processor with 3.7 GHz CPU and 64 GB main memory. The framework1 was implemented in Python with igraph library.2 We report average values obtained from 30 executions for algorithms that rely on random strategies.
We considered the scientific collaboration network Cond-Mat,3 which describes co-authorships of preprints posted from 1995 to 1999 in the Condensed Matter section of the arXiv repository, to address the first research question. It has 38,742 vertices (representing authors and papers) and 58,595 edges (co-authorship relations) (additional information can be found elsewhere [60], [61]). Our interest was to observe how a progressive coarsening affects the intrinsic topological properties of a network.
The Cond-Mat network has been extensively analyzed and is known to have characteristic features regarding degree distribution and clustering coefficients, as depicted in Fig. 6. According to Fig. 6(a), the degree distribution follows a power-law relationship characteristic of scale-free networks, with a vast majority of low-degree vertices and a few vertices of very high-degree, i.e., the so-called “hubs”.
An inverse relation between vertex clustering coefficients and vertex degrees is also evident in Fig. 6(b). This is a particular feature of this network: the hubs, i.e., authors with many collaborators, have low-clustered neighborhoods and they tend to interact with collaborators from distinct groups, who are not usually collaborators themselves. In contrast, authors with few collaborators have highly-clustered neighborhoods, i.e., they often interact within smaller and more restrict research groups whose members also collaborate with each other.
We investigated how the scale-free properties of degree distribution and two-mode clustering coefficient are affected as the network is progressively coarsened by matching algorithms RGMb and GMb, with input parameters set as L=10<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">10</mn></mrow></math> and rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math>. Fig. 7 shows the reduction factor on |V|=n<math><mrow is="true"><mo is="true">|</mo><mi is="true">V</mi><mo is="true">|</mo><mo is="true">=</mo><mi is="true">n</mi></mrow></math> and |E|=m<math><mrow is="true"><mo is="true">|</mo><mi is="true">E</mi><mo is="true">|</mo><mo is="true">=</mo><mi is="true">m</mi></mrow></math> at each level of the coarsening hierarchy, i.e., starting from the original network (level 0) to the coarsest one (level 10). The curves depict the percentages of remaining vertices (left) and edges (right) relative to the initial numbers.
Figs. 8 and 9 show curves for degree distribution and clustering coefficient, respectively, in each coarsened network, from levels 1 to 10 (the top two rows refer to RGMb and the bottom two rows refer to GMb). Both algorithms yielded very similar distributions of degree and clustering coefficient, which suggests the random exploration of the solution space adopted by RGMb has no strong impact on the topological features of intermediate networks, in comparison with GMb.
The characteristic behavior of degree distribution observed in the original network is reasonably preserved in the coarsened models down to level 3, i.e., coarsened networks at levels 1, 2 and 3 still contain few hubs and many low-degree vertices, a behavior that gradually changes from level 4 onwards.
Particularly from level 8, vertex degrees become more homogeneous, until the original topological features have been completely lost in the final network at level 10. A similar pattern is observed in the clustering coefficient, i.e., again, the original behavior of neighborhoods of hub authors is preserved up to level 3. From level 5 onwards, most vertices converge to lower clustering coefficients and the network’s characteristic behavior is completely lost from level 8.
At the initial coarsening levels (1, 2 and 3) only vertices with many two-hop common neighbors can compose a matching. The newly formed super-vertices thus preserve the neighborhood properties of their predecessors, hence, the dominant topological properties of the parent network. In other words, it is likely that authors matched at the early coarsening levels indeed have many common collaborators. However, at later levels, authors with few common collaborators may be forced to join, with corrupts the original topological relations in the network. Notice the sizes of networks at levels 9 and 10 were reduced to nearly 10% of the original network (see Fig. 7), which implies the coarsest networks are mostly formed by heavy-weight super-vertices ( ≫ σ(sV)) sparsely connected. Such super-vertices of low-degree form clustered structures with few triangles.
Results from this analysis indicate a limited reduction of an initial network, up to two or three levels, can preserve its relevant topological features. Furthermore, it is evident that choosing the appropriate coarsening level is critical, and depends on properties of the target application and dataset. Establishing a suitable trade-off between accuracy and runtime may require empirical verification in each case. This inherent limitation can be associated to the well-known overfitting/underfitting problem; however, the coarsening phase in our multilevel process “generalizes” the data instead of an objective function. This may explain why, in some cases, better solutions were obtained on the coarsened networks. In general, extensive coarsening reduces execution times of a target algorithm, but it can lead to excessive generalization of the data with significant degradation of topological features, and possibly algorithm accuracy. In contrast, limited coarsening preserves topological features and accuracy, at the expense of higher execution times.
Research efforts on multilevel optimization have been strongly motivated by community detection (or graph partitioning) problems, which makes this a benchmark problem. Algorithms for community detection in networks split the vertices into disjoint groups (or communities), so as to minimize the number of edges between distinct communities [62]. Barber’s modularity optimization [63] is often employed to identify community structures in bipartite networks. Formally, it quantifies the extent of communities formed in both layers relative to a null bipartite network model. Beckett [64] introduced the LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> algorithm,4 which maximizes Barber’s modularity through label propagation in weighted bipartite networks and showed it has competitive performance compared with state-of-the-art methods. However, it is a computationally costly algorithm and becomes unfeasible in large-scale networks.
In order to address the second research question, the MOb framework was tested considering Beckett’s algorithm LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> as a target algorithm. Therefore, MOb (Algorithm 1) performs the coarsening, runs LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> to find the community structure in the coarsest network, and projects the solution to obtain the community structure in the original network. Our goal was to investigate whether MOb can yield solutions statistically equivalent in quality to the standard LPAwb+,<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo><mo is="true">,</mo></mrow></math> whilst increasing its scalability. Results are compared with those obtained with the standard LPAwb+,<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo><mo is="true">,</mo></mrow></math> used as baseline.
We investigated the performance of the four instances of MOb, listed in Table 1. They were executed with parameter settings rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math> and L=[1,2,3]<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mo is="true">[</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">]</mo></mrow></math> in a set of 15 synthetic weighted bipartite networks, identified as R1-R15 (hereafter each MOb instance is referred to by its name).
Synthetic networks were obtained with the community model described in [64], which creates unbalanced and randomly positioned community structures. Networks of sizes n=|V1+V2|<math><mrow is="true"><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">+</mo><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mrow is="true"><mo is="true">|</mo></mrow></mrow></math> were generated within the range [1, 000; 15, 000] at increments of 1,000 and the number of communities was set to 0.01*n. Edge weights were randomly assigned from a skewed negative binomial distribution and noise was introduced in the connection patterns by reconnecting a percentage of the edges between and within communities.
Performance was measured in terms of accuracy, by means of the NMI (normalized mutual information), which compares the solution found by a selected algorithm with the baseline [65], we also measured execution times. Table 2 shows the NMI accuracy values in the 15 networks. The highest values are in bold and values equal to or higher than baseline LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> are highlighted with a gray background. The best performances were achieved by MOb−GMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> with one level of coarsening (L=1<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math>) on 11 out of the 15 networks. Baseline LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> yielded the best performance in three networks, whereas MOb−RGMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> with L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> yielded the worst results.
Limited coarsening levels (mainly L=1<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math>) yielded higher accuracy values, reinforcing that a controlled coarsening can filter the solution space by joining promising vertex pairs and removing irrelevant high-cost solutions, while preserving important topological features. In contrast, accuracy deteriorated with more extensive coarsening (L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>), which does not necessarily preserve the original topological properties and is likely to blur the boundaries between adjacent communities. The effect of parameter L depends on network size, i.e.; differences in algorithm accuracy are likely to decrease as the network sizes increase, hinting that higher values of L might be successfully adopted in handling larger networks.
Although LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> achieved the best performance in three networks out of the fifteen, the corresponding accuracy values attained by the MOb instances are very close for these networks. Indeed, in these specific cases accuracy values differ up to 0.006 in R8, up to 0.005 in R9 and up to 0.001 in R12. Interestingly, all MOb instances yielded similar accuracy values and more stable results than the standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. The accuracy values obtained with MOb−GMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> and L=1,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> for instance, are within the range [0.994, 0.996], whereas for the standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> they are within the range [0.918, 0.999], as shown in Figs. 10 and 11.
Fig. 10 depicts the averages and standard deviations of the accuracy values, whereas Fig. 11 shows the dispersion of their distribution and outliers, considering in both cases the alternative settings of parameter L. The bar plots in Fig. 10(a) reveal superior performance and stability of the four MOb instances when L=1,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> confirmed by their higher average accuracies and narrower standard deviations. When L=2,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn><mo is="true">,</mo></mrow></math> only MOb−GMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> and MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> yielded a slightly superior solution in comparison to LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. For L=3,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn><mo is="true">,</mo></mrow></math> LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> yielded better results than any MOb instance, a consequence of the extensive network reduction.
The box plots in Fig. 11(a) reveal that for L=1<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math> all MOb instances yielded accuracy values within a narrower distribution and higher averages than LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>.
We can conclude MOb instances yielded, in general, higher accuracy and improved stability in comparison to LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. In summary, the experimental evidence regarding solution quality (average, standard deviation and dispersion of the accuracy values) suggests the multilevel framework stabilizes and improves the performance of the algorithm.
A Nemenyi post-hoc test [66] was applied to the results in Table 2 to verify statistical differences in the algorithms performances and the results are shown in Fig. 12 for (a) L=1,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> (b) L=2<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math> and (c) L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>. The critical difference (CD) is indicated at the top of each diagram and the algorithms’ average ranks are placed on the horizontal axes, with the best ranked algorithms to the left. A black line connects algorithms if no significant difference has been detected among them. According to the Nemenyi statistics, the critical value for comparing the mean-ranking of two different algorithms at 95 percentile is 1.58.
Let us consider the outcome of the post-hoc test for L=1,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> i.e., when the number of vertices n is reduced by a factor of two, shown in Fig. 12(a). MOb−GMBcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">B</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> was ranked best, followed by MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> and MOb−RGMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> and then LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. Furthermore, MOb−GMBcn,<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">B</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub><mo is="true">,</mo></mrow></math> MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> and MOb−RGMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> presented statistically significant differences compared with standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. Interestingly, for L=2<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math> (Fig. 12(b)), MOb−GMBcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">B</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> and MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> remain ranked first, however, no statistically significant difference was observed in relation to LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. Finally, for L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> (Fig. 12(b)), LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> was ranked first, with no statistically significant difference observed in relation to MOb−GMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> or MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math>. However, parameter settings L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> and rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math> implied, in this case, in reducing the original size by a factor of 75%, which explains the poor performance of all MOb instances.
We also assessed the scalability of the MOb instances to investigate the third and fourth research questions. Their performance was analyzed considering each individual network and the total time spent in the experiments. Table 3 shows the absolute execution times (in seconds) on each network - values refer to the average times from 30 executions of each scenario.
The longest execution time of standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> was 302,442 s (time to process the largest network) and the shortest was 14 s (time to process the smallest one). MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> was the most expensive MOb instance, consuming (L=1<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math>) 25,674 s on the largest network and 3 s on the smallest one. Therefore, regarding its maximum and minimum execution times, respectively, MOb−GMbwcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">w</mi><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> run 11.8 to 4.6 times faster than the standard LAPwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">A</mi><mi is="true">P</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>. The maximum and minimum running times of the least expensive MOb instance, MOb−RGMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> (L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>), were 1,310 s and 1 s, respectively. Therefore, MOb−RGMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> run 230 to 14 times faster than LAPwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">A</mi><mi is="true">P</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>.
The analysis in Section 5.1 revealed that a network coarsened at level 3 has roughly 25% of its original size. Let us consider, for example, network R15 (n=15,000<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">15</mn><mo is="true">,</mo><mn is="true">000</mn></mrow></math>): we know it has been reduced to 3750 vertices at level 3 and algorithm MOb−RGMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> processed it in 1,310 s (see Table 3). This is close to the execution time of standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> on network R4 (of size n=4,000,<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">4</mn><mo is="true">,</mo><mn is="true">000</mn><mo is="true">,</mo></mrow></math> similar to the size of R15 coarsened at level 3), i.e., 904 s. As MOb−RGMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> executes the coarsening/uncoarsening steps, the actual time spent running LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> to find the solution is roughly similar in both cases, but MOb−RGMbcn<math><mrow is="true"><mi is="true">M</mi><mi is="true">O</mi><mi is="true">b</mi><mo is="true">−</mo><mi is="true">R</mi><mi is="true">G</mi><mi is="true">M</mi><msub is="true"><mi is="true">b</mi><mrow is="true"><mi is="true">c</mi><mi is="true">n</mi></mrow></msub></mrow></math> is handling a network nearly four times larger.
The total time spent running the experiments was 1,267.307 s, or nearly 352 h. Fig. 13 shows the contribution of each algorithm to the total time, considering both absolute values (seconds) (a) and relative values (percentages) (b).
In the best case, the MOb instances reduced execution time from 891,875 s (nearly 208.8 h) required by the standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> to 4,552.4 s (1.26 h), which implies LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> was nearly 195 times slower than its MOb instantiations. Executing LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math> consumed over 70% of the time spent in the experiments, whereas roughly 6% of the time was spent running the MOb instances.
Finally, we assessed the impact of each phase (coarsening, target algorithm on coarsest network and uncoarsening) on the execution time of the multilevel process; we analyzed algorithm behavior separately on each network and then in relation to the total time of the experiments.
The relative contributions of each multilevel phase for each network are shown in Fig. 14 (for legibility, we show bars for 12 out of the 15 networks). On the smallest network (n=1,000<math><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">000</mn></mrow></math>) the coarsening phase consumed nearly 52% of the total execution time and the local search step consumed nearly 45%. On the other hand, as the networks increase, the time spent on the coarsening relative to the solution finding gradually decreases. On the larger networks, the coarsening phase consumed roughly 1% of the total execution time, in contrast to roughly 99% of the solution finding phase. The time spent on the uncoarsening phase was negligible.
Fig. 15 shows the relative contribution of each multilevel phase to the total time of the experiments. In general, the coarsening phase consumed less than 1% of the total execution time, the solution finding phase (executing LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>) consumed over 99%, and the time spent in the uncoarsening phase was negligible. These results indicate coarsening and uncoarsening exerted no significant influence on the scalability of the multilevel process and provide empirical evidence the multilevel strategy is a promising approach to scale network optimization algorithms.
From this empirical investigation we conclude: (i) the proposed MOb approach yielded more accurate and stable results compared to the standard LPAwb+<math><mrow is="true"><mi is="true">L</mi><mi is="true">P</mi><mi is="true">A</mi><mi is="true">w</mi><mi is="true">b</mi><mo is="true">+</mo></mrow></math>; (ii) although solution quality degrades as the network is progressively coarsened, runtime drops drastically at each additional coarsening level; hence, a successful solution requires establishing a suitable trade-off between accuracy and execution time.
We illustrate how MOb framework can be adapted to perform dimensionality reduction in the context of text classification, having the k-Nearest Neighbor classifier (kNN) as the target algorithm, in order to exemplify its application in a different kind of optimization problem.
Documents are often represented, in text classification tasks, as multidimensional feature vectors, in which each dimension maps a particular term. As the dimensionality of the representation space has strong impact in classification performance, such tasks are often preceded by a dimension reduction step. Specifically, a kNN classifier that employs a naÏve search strategy has time complexity O(ndk) for a fixed k, where n is the cardinality of the training set and d denotes the dimensionality of the document representation.
Alternatively, a document corpus can be represented as a bipartite network G=((V1∪V2),E,σ,ω),<math><mrow is="true"><mi is="true">G</mi><mo is="true">=</mo><mo is="true">(</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">∪</mo><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">)</mo></mrow><mo is="true">,</mo><mi is="true">E</mi><mo is="true">,</mo><mi is="true">σ</mi><mo is="true">,</mo><mi is="true">ω</mi><mo is="true">)</mo><mo is="true">,</mo></mrow></math> where V1={d1,⋯,dr}<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">d</mi><mn is="true">1</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mi is="true">r</mi></msub><mo is="true">}</mo></mrow></mrow></math> is the set of documents and V2={t1,⋯,ts}<math><mrow is="true"><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mo is="true">=</mo><mrow is="true"><mo is="true">{</mo><msub is="true"><mi is="true">t</mi><mn is="true">1</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">t</mi><mi is="true">s</mi></msub><mo is="true">}</mo></mrow></mrow></math> is the set of terms. An edge (u, v) exists if term tu occurs in document dv, and the term’s frequency determines the corresponding edge weight w(u, v). Such a network is represented as a bi-adjacency matrix Arxs, where r=|V1|,<math><mrow is="true"><mrow is="true"><mi is="true">r</mi><mo is="true">=</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">V</mi><mn is="true">1</mn></msub><mrow is="true"><mo is="true">|</mo><mo is="true">,</mo></mrow></mrow></math> s=|V2|<math><mrow is="true"><mrow is="true"><mi is="true">s</mi><mo is="true">=</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">V</mi><mn is="true">2</mn></msub><mrow is="true"><mo is="true">|</mo></mrow></mrow></math> and Au,v=w(u,v)<math><mrow is="true"><msub is="true"><mi is="true">A</mi><mrow is="true"><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi></mrow></msub><mo is="true">=</mo><mi is="true">w</mi><mrow is="true"><mo is="true">(</mo><mi is="true">u</mi><mo is="true">,</mo><mi is="true">v</mi><mo is="true">)</mo></mrow></mrow></math> if (u, v) ∈ E. Dimensionality reduction is aimed at obtaining a lower dimensional matrix Ar′xs′′,<math><mrow is="true"><msubsup is="true"><mi is="true">A</mi><mrow is="true"><msup is="true"><mi is="true">r</mi><mo is="true">′</mo></msup><mtext is="true">x</mtext><msup is="true"><mi is="true">s</mi><mo is="true">′</mo></msup></mrow><mo is="true">′</mo></msubsup><mo is="true">,</mo></mrow></math> with r′=r<math><mrow is="true"><msup is="true"><mi is="true">r</mi><mo is="true">′</mo></msup><mo is="true">=</mo><mi is="true">r</mi></mrow></math> and s′ <  < s.
The proposed solution is described in Algorithm 6, multilevel dimensionality reduction (Mdr), which only requires a coarsening phase (lines 1–3). It takes as inputs the initial bipartite network G, the term layer tl to be coarsened, the desired maximum number of levels L and reduction factor rf, and returns the bi-adjacency matrix of the coarsest network (line 4).
We adopted RGMbcn as the contracting algorithm in the Mdr implementation, and used Mdr in combination with a kNN classifier, assuming k=3<math><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> and the Euclidean distance as similarity measure, hereafter referred to as Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math>. The results obtained considering a reduction factor rf=0.5<math><mrow is="true"><mi is="true">r</mi><mi is="true">f</mi><mo is="true">=</mo><mn is="true">0.5</mn></mrow></math> and L varying in the range [1,2,3,4,5,6,7,8,9,10] were compared to an equivalent kNN setting that employed PCA (Principal Components Analysis) [67] for dimensionality reduction (PCA−kNN<math><mrow is="true"><mi is="true">P</mi><mi is="true">C</mi><mi is="true">A</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math>). The complexity of PCA is O(d2n+d3),<math><mrow is="true"><mi is="true">O</mi><mo is="true">(</mo><msup is="true"><mi is="true">d</mi><mn is="true">2</mn></msup><mi is="true">n</mi><mo is="true">+</mo><msup is="true"><mi is="true">d</mi><mn is="true">3</mn></msup><mo is="true">)</mo><mo is="true">,</mo></mrow></math> where n denotes the number of samples and d the data dimensionality.
Implementations Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> and PCA−kNN<math><mrow is="true"><mi is="true">P</mi><mi is="true">C</mi><mi is="true">A</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> were used to classify thirteen real document-term networks available from the literature, described in Table 4, considering a cross-validation with permutation testing and ten-fold cross-validation to estimate classification error. The training set was randomly split into ten equal-sized subsets, so the classification model was trained in nine subsets and tested on the remaining one.
Fig. 16 shows the performance of the Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> classifier as a function of the number of levels in the coarsening hierarchy, where L=0<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math> refers to classification with no dimensionality reduction. Fig. 16(a)–(c) show accuracy, dimensionality of the representation (number of terms) and execution times, respectively.
A moderate decrease in accuracy values is observed up to level L=5<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">5</mn></mrow></math>. In general, accuracy is stable at the early coarsening levels, which suggests the first coarsening iterations have limited impact in solution quality, as it is more likely that highly correlated terms are being matched. Fig. 16(a) shows accuracies up to level 5. Moreover, since from level 5 onwards the coarsening yields no significant reduction in the number of terms it may be interrupted at this point. Also, a sharp reduction in execution times is observed only up to this level. We remind the analysis presented in Section 5.1, which showed a network coarsened at level 5 has roughly 15% its original size.
We compared the classification accuracies of kNN without dimension reduction and Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> (with coarsening at L=[1,3,5]<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mo is="true">[</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">3</mn><mo is="true">,</mo><mn is="true">5</mn><mo is="true">]</mo></mrow></math>) and those of PCA−kNN<math><mrow is="true"><mi is="true">P</mi><mi is="true">C</mi><mi is="true">A</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> (with equivalent dimensionality reduction). The results are shown in Table 5. The highest accuracy values are shown in bold and values equivalent or superior to baseline kNN are shown with a gray background. Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> achieved the best performance in five out of the thirteen networks; PCA−kNN<math><mrow is="true"><mi is="true">P</mi><mi is="true">C</mi><mi is="true">A</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> performed best in seven networks and standard kNN performed best in only one of them.
A Nemenyi post-hoc test was applied to the results in Table 5 in order to detect statistical differences among the algorithms. Demsar post-hoc test requires each algorithm and dataset to be independent, therefore, we perform the dimensionality reduction at each level separately. The results are shown in Fig. 17 for (a) L=1,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> (b) L=2<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math> and (c) L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>. According to Nemenyi statistics, in all diagrams, the critical value for comparing the mean-ranking of two different algorithms at 95 percentile is 0.92. No significant difference was observed between the algorithms, therefore, they are connected by a bold line in each diagram. Albeit differences are not significant, Fig. 17(a) shows Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> was ranked best for L=1,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> whereas for L=2<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">2</mn></mrow></math> and L=3<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> PCA−kNN<math><mrow is="true"><mi is="true">P</mi><mi is="true">C</mi><mi is="true">A</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> was ranked best (Fig. 17(b) and (c)). Therefore, Mdr−kNN<math><mrow is="true"><mi is="true">M</mi><mi is="true">d</mi><mi is="true">r</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> proved competitive in terms of accuracy, in comparison to PCA−kNN<math><mrow is="true"><mi is="true">P</mi><mi is="true">C</mi><mi is="true">A</mi><mo is="true">−</mo><mi is="true">k</mi><mi is="true">N</mi><mi is="true">N</mi></mrow></math> and baseline kNN.
This case study has been presented as a preliminary investigation on the feasibility of extending the proposed multilevel framework to other combinatorial problems beyond community detection. The Mdr algorithm deserves further consideration and could incorporate additional capabilities, e.g., it would be convenient to reduce the feature space to a target dimensionality, rather than by a given reduction factor. Furthermore, the coarsening algorithm could take into account intrinsic characteristics of specific kinds of document-term networks by means of customized matching algorithms. Moreover, Mdr can be employed in connection with other classification algorithms.
It is relatively straightforward to apply MOb to several combinatorial optimization problems beyond community detection. For instance, in Section 5.3 we illustrated its application to handle a dimensionality reduction problem over a bipartite network in which the two vertex layers represent objects and features, respectively. In this context, only the feature layer was coarsened and the reduced feature space is given directly by the adjacency matrix of the coarsest network. Likewise, it can be easily instantiated to handle overlapping or fuzzy community detection or classification problems.
In overlapping community detection, for each decomposed sV∈Vl+1,<math><mrow is="true"><mi is="true">s</mi><mi is="true">V</mi><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">,</mo></mrow></math> its original vertices {u, v} ∈ Vl are assigned to the same set of communities as the corresponding super-vertex. If the coarsened network has a fuzzy structure, the strength of a vertex’ pertinence to a community will be equal to that of its super-vertex. Similarly, in a classification problem original vertices {u, v} ∈ Vi should be assigned to the same class of their super-vertex. Therefore, instantiating the framework to handle either problem would require just minor modifications in the projection algorithm (Algorithm 5).
MOb might also be useful to support interactive visualization of large-scale bipartite networks, by means of navigation over a hierarchy of coarsened networks, which would demand a data structure to keep these intermediate networks.
Instantiation to other scenarios is not necessarily as straightforward, and may require further modifications in the proposed algorithms. For example, in the edge clustering problem (also called link communities) the contracting algorithm used in coarsening phase would require adjustments. Whereas in community detection a vertex inherits the same group assignment of its super-vertex, here each edge must inherit the connections from its super-edge. Therefore, an edge e∈El+1<math><mrow is="true"><mi is="true">e</mi><mo is="true">∈</mo><msub is="true"><mi is="true">E</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math> incident to sV∈Vl+1<math><mrow is="true"><msub is="true"><mi is="true">s</mi><mi is="true">V</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">V</mi><mrow is="true"><mi is="true">l</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math> must refer to the edges incident to vertices {u, v} ∈ Vl. Algorithm 4 does not implement this function; however, it could be done by keeping an additional data structure similar to the successor vector.
Another possible application is in link prediction or recommendation problems, albeit this poses a more complex scenario for generalization. Link prediction methods rely on similarity between vertices, since similar vertices are likely to share common links. However, such information is not explicitly given for the super-vertices in a coarsened bipartite network. Nonetheless, the framework could be employed to reduce the number of required operations, i.e. super-vertices might be created grouping vertices with shared h-hop neighbors, thus filtering the search space. Link prediction could be performed in the uncoarsening phase through the decomposed super-vertices, and the solution finding and uncoarsening phases would be executed simultaneously.
As a final consideration, the matching algorithm used for coarsening should be carefully designed to incorporate the specific characteristics of each problem and context.
Inspired by the potential of general-purpose multilevel strategies to scale optimization algorithms we have introduced algorithms of a novel multilevel optimization framework (MOb) for bipartite networks, and illustrated its application on two combinatorial optimization problems. Our framework accounts for the specificities of bipartite networks and provides a powerful tool for handling a variety of problems.
We investigated three empirical scenarios to illustrate strengths and limitations of the proposed MOb framework. A first study has shown that a controlled coarsening preserves relevant topological features of a network. A second study described an application in community detection, showing that MOb combined with a proper local search strategy can drastically improve speedup of a classic community detection algorithm while preserving solution quality. Finally, in a third study we considered text classification to illustrate how the general framework can be instantiated to handle different combinatorial optimization problems.
Our results provide compelling evidence that MOb offers a competitive approach to scale existing methods while preserving solution quality, and reinforce its usefulness for handling combinatorial optimization problems in large bipartite networks. Furthermore, the framework is flexible and can be adjusted to incorporate alternative and novel coarsening methods targeted at specific applications. We also discuss some general guidelines for future applications of combinatorial problems, such as link prediction, edge clustering, and interactive graph visualization.
Identifying the level of coarsening that will yield a suitable trade-off between accuracy and execution times is a critical issue in applying the proposed multilevel strategy. Currently, this is done by means of empirical investigation in each application problem and dataset, but it certainly deserves further investigation.
We also plan as future work to extend the framework to handle problems defined in heterogeneous networks, where edges connect vertices of multiple types. It would be applicable, e.g., to document-word networks indicating associations of the type document-word, word-word, and document-document; or networks describing relations between words, documents and authors. We are also interested in investigating distributed or parallel paradigms, as well as in application of MOb to supervised and unsupervised classification tasks. An implementation of the general framework is currently available and can be downloaded from https://www.github.com/alanvalejo/mob.
Author A. Valejo is supported by a scholarship from the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES) grants PROEX-6513393/D. This work has been partially supported by the State of São Paulo Research Foundation (FAPESP) grants 15/14228-9 and 17/05838-3; and the Brazilian Federal Research Council (CNPq) grants 302645/2015-2 and 301847/2017-7.