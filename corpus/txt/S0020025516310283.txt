Natural language is one of the most vivid examples of complex systems [18], where the term more is different [4] like no other succinctly defines its features. Indeed, the relatively small number of elementary items, the phonemes and letters, allow one to create more complex elements: the words. They form references to everything that a human can name and describe. However, the words alone do not constitute the whole essence of language and another complex entity is a prerequisite here: the sentence [8]. The sentential structure is a standard feature of almost all written languages. Only at this level the semantics in its whole richness and with a variety of carriers emerges: words, syntax, phrases, clauses, and punctuation in written language.
Statistical analyses of language samples that were carried out since over a century ago [9], [27] revealed the existence of laws that describe language quantitatively. Classical statistical study comprises, among others, the empirical word frequency distribution that is compared with the power-law model known as the Zipf law [28] or its generalized form known as the Zipf–Mandelbrot law [21], [24], and the functional relation between the length of a text and the number of unique words used to compose it, modelled by the Heaps law [11], [13], [14]. A relatively new approach is a description of language in the network formalism [6], [10], [12], [22], [23] that, among others, reveals that certain network representations of the lexical structure of texts (e.g. the word co-occurrence) belong to the scale-free class, similar to the semantic networks constructed based on the meaning of words [2], [3], [19].
Writing requires the use of punctuation; otherwise some expressions might be ambiguous and deceptive. Punctuation also allows one to denote separate logical units into which any compound message can be divided. From this perspective, the punctuation marks are something more than merely technical signs serving to allow a reader to comprehend the consecutive pieces of texts more easily. If put in between the words, they also acquire meaning and become meaningful not less than, for example, some words playing mainly grammatical role as conjunctions and articles. For example, even though the full stops do not have clear phonetic expression, they define the length of sentences and thus they can influence a reader’s subjective perception of the message content: the speed of events, the descriptive complexity of a given situation, etc. Our recent study shows additionally that punctuation carries long-range correlations in narrative texts [8]. This brings us more quantifiable evidence that punctuation, even though “silent”, is no less important than words.
Thus, it might seem intuitively natural to include such marks in any analysis, in which the ordinary words are considered: the rank-frequency, the word co-occurrence, and other types of the statistical analyses [5]. It is sometimes done so in the engineering sciences like natural language processing due to practical reasons [15], but without any deeper linguistic justification. On the other hand, such an inclusion might not be recommended if the statistical properties of the punctuation marks were significantly different from the corresponding properties of the ordinary words as it would actually mean that the punctuation marks were something different than words. So, this issue appears to be rather a complex one. In order to resolve it, in this work we study the rank-frequency distributions and the word-adjacency networks in the corpora, in which the punctuation marks are treated as words, and compare the results for the punctuation marks with the results for the ordinary words. We argue that these results, which are complementary to the earlier ones published in [8], can provide one with indication on how to improve reliability of the statistical calculations based on large corpora of the written language samples.
A literary form that is relatively the closest to the spoken language - prose - is expected to reflect the statistical properties of language. In order to analyse it, we selected a set of well-known novels written in one of six Indo-European languages belonging to the Germanic (English and German), Romance (French and Italian), and Slavic (Polish and Russian) language groups. Our selection criterion was the substantial length of each text sample, i.e., at least 5000 sentences, which we have already verified to be sufficient for a statistical analysis [8]. The texts were downloaded from the Project Gutenberg website [26]. Apart from the individual texts, we also created 6 monolingual corpora by merging together at least 5 texts written in the same language so that each corpus consisted of about one million words - a volume that was sufficient for our statistical analysis (see Appendix for a list of texts).
Some redundant words residing outside the sentence structure of texts (such as chapter, part, epilogue, etc.), footnotes, page numbers, and typographic marks (quotation marks, parentheses, etc.) were deleted. All standard abbreviations specific to a given language (like Mrs. and Dr. in English) were cleaned of dots and counted as separate words. The following marks were considered the full stops that end a sentence: dots, question marks, exclamation marks, and ellipses. Apart from the full stops, our analysis also included commas, colons, and semicolons.
Moreover, the notion of the punctuation marks may be generalized in such a way that it includes new chapters, new parts, and new paragraphs (that are recognized as the separators stronger than a full stop), as well as new lines (that may further be divided into: comma-new line, colon-new line, etc.). While the division into parts is too sparse to be meaningful in our analysis and the localization of all new paragraphs and new lines is too demanding to be easily done here, we extended our analysis over the chapters. In each text we found the places, in which new chapters begin, and introduced them into the texts as an additional punctuation mark (denoted as #chap). We preferred not to consider any specific word as a separator in this context, because different ways of denoting new chapters are used in different texts: the word “chapter”, the Roman or the Hindu-Arabic numerals, the asterisks, or even just the voids. One issue should be kept in mind, however. While the standard punctuation can be viewed as an inherent part of the natural language that helps one to understand the message, the division of texts into paragraphs, chapters, and parts is purely a writing technique not necessary from the point of view of the language organization.
Our first analysis was based on the frequency of word occurrence in a sample, which is a standard approach. It allowed us to check for possible statistical similarities between the punctuation marks and the ordinary words. It also aimed at testing whether these additional elements obey the well-known empirical Zipf law. Next, in a word-adjacency network representation, where nodes represent words and connections represent the words’ adjacent positions, the punctuation marks were taken into account like usual words. Doing so has practical importance for the consistency of the network creation process: otherwise there might be a problem whether the node representing a word ending a sentence and the node representing a word that starts the subsequent sentence may be connected to each other. On the one hand, such words are more loosely related semantically than the words within the same sentence are, but, on the other hand, leaving those nodes unconnected can lead to the formation of a disconnected network, for which many useful network measures cannot be well-defined. Identification of the punctuation marks as words thus allowed us to overcome this difficulty and to apply all the standard network measures effectively.
All calculations were performed in Mathematica and C++<math><mrow is="true"><mo is="true">+</mo><mo is="true">+</mo></mrow></math> environments independently. For better comparison between the corresponding results, all respective figures are shown in the same scale ranges.
The primary characteristics of natural language samples describing its quantitative structure is the Zipf distribution. It states that the probability P(R) of encountering the Rth most frequent word scales according to P(R)∼R−α<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">∼</mo><msup is="true"><mi is="true">R</mi><mrow is="true"><mo is="true">−</mo><mi is="true">α</mi></mrow></msup></mrow></math> for α ≈ 1. The Zipfian scaling in its original formulation holds for the majority of ranks except for a few highest ones, where the power law breaks and the corresponding plots are deflected towards lower frequencies than those expected from the pure power law. Therefore a better agreement with the empirical data one can obtain using the so-called Zipf–Mandelbrot law (shifted power-law): (1)P(R)∼(R+c)−α,<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">∼</mo><msup is="true"><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">+</mo><mi is="true">c</mi><mo is="true">)</mo></mrow><mrow is="true"><mo is="true">−</mo><mi is="true">α</mi></mrow></msup><mo is="true">,</mo></mrow></math>where c is the parameter responsible for the above-mentioned deflection. There are different hypotheses on the origin of the Zipf law, with the principle of least effort [28] and the communication optimization [20] among them. It should be noted that this situation occurs only if a language sample is created in the unconstrained and spontaneous conditions. Existing aberrations from a power-law regime have appropriate justifications that have their source in an intellectual disability [25] or in sophisticated creative workshops [17].
After calculating the frequency of words, a set of words that are present in almost every sample is selected. As it turns out, for a sufficiently large sample they are always the words having grammatical functions. Regardless of the topics covered by a sample text, these words occupy the first ranks in the Zipf distribution. Additionally, we count the occurrence numbers of different punctuation marks in each sample and include them in the corresponding Zipf distributions as if they were ordinary words. The main plots in Fig. 1 show such distributions with distinguished punctuation marks (the special division): dot (#dot), question mark (#qu), exclamation mark (#ex), ellipsis (#ell), semicolon (#scol), colon (#col), comma (#com), and new chapter (#chap). In the insets to Fig. 1, all the marks that can end sentences are counted together as full stops (#fs).
Commas and the different types of full stops (except for ellipses) appear in the same region of the Zipf distribution where the highest-ranked words reside, i.e., the function words, like conjunctions (especially in the Slavic languages), articles (the Romance and Germanic languages), and prepositions. In all the considered languages, comma has R=1,<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo></mrow></math> while the rank of dot is typically R=2,<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">2</mn><mo is="true">,</mo></mrow></math> except for Italian and English (R=3<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math>). The question and exclamation marks as well as semicolons and colons have considerably lower ranks that vary among the languages but in general can be found in the interval 10 < R < 30 (#qu and #ex) and in the interval 10 < R < 50 (#scol and #col). Ellipses can behave as lexical words with their ranks sometimes being lower than R=100<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">100</mn></mrow></math>. This refers even more to the new chapter marks whose frequency varies strongly from text to text and their rank can be as low as R ≈ 1000 for particular books. For the general division, the unified full stop becomes the second most frequent object after comma in all languages except for English, where it occupies rank R=3<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math> (after comma and the). The most interesting observation regarding the plots is that all the punctuation marks in both divisions are placed together with the regular words in the regime that is close to a power-law. This means that adding the punctuation marks to the Zipf analysis results in a substantial improvement of the scaling of the rank-frequency plots in that part (R < 10) that in a standard analysis deviates from a power-law towards the lower frequencies and that is described by the Zipf–Mandelbrot distribution. From this point of view, the punctuation marks act towards restoring of the Zipf distribution. This effect can be seen in Fig. 1, where the Zipfian power law fitted within the range [101, 104] is geometrically extended over the highest ranks. For all the languages the corresponding points are closer to the power law and for French, Polish, and Russian they are placed exactly in the scaling regime. For a comparison, in Fig. 2 we present analogous Zipf plots for two texts where the punctuation differs from the standard pattern (a lack of the sentence structure of the texts). However, except for the distant location or even the absence of #dot, the overall statistical properties of the remaining punctuation marks are normal.
To express the above observation in a quantitative form, we fit the Zipf–Mandelbrot (Eq. (1)) distribution to the rank-frequency plots constructed for words and for words together with the punctuation marks and estimate the corresponding values of the parameter c responsible for a deflection from the pure power law (c=0)<math><mrow is="true"><mo is="true">(</mo><mi is="true">c</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">)</mo></mrow></math>. Fig. 3 shows such fits for all the considered languages. In each case, the inclusion of the punctuation marks results in the significantly lower values of c than those in the case, in which only the words are considered, with the strength of this decrease depending on a language. It is the strongest for the Slavic languages (essentially c=0<math><mrow is="true"><mi is="true">c</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>) and the weakest, but still sizeable, for the Germanic ones. This provides a quantitative evidence that the punctuation marks included in a Zipfian plot largely restore its scaling, indeed.
Fig. 4 shows three stages of a word-adjacency network development. The network was created based on a growing sample of text of length s. The adopted representation allows us to check the adjacency relation between words and punctuation marks. In Table 1 the chosen network parameters are shown for the corpora.
A weighted work-adjacency network can be easily created from a text sample. The number of word co-occurrences may be understood as the weight of a connection between the respective nodes. The basic local parameter of the ith node is the number of edges attached to it, called a node degree kiw<math><msubsup is="true"><mi is="true">k</mi><mrow is="true"><mi is="true">i</mi></mrow><mi is="true">w</mi></msubsup></math>. It is roughly equal to doubled frequency fi of the corresponding word in the sample. For a binary network, a node degree ki≡kiu<math><mrow is="true"><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><mo is="true">≡</mo><msubsup is="true"><mi is="true">k</mi><mrow is="true"><mi is="true">i</mi></mrow><mi is="true">u</mi></msubsup></mrow></math> refers to the number of unique connections from the ith node to other nodes, where fi is the larger with respect to ki, the more connections with other nodes this node has. In Fig. 5 the difference between fi and ki is shown for the most frequent items in a proper order starting from the left-hand side.
In English (Fig. 5(top)), these differences for all the considered words are substantial and roughly similar in size on logarithmic scale. This means that there exists a simple relation: fi ≃ a(i)ki with 1/5 < a(i) < 1/3. The most frequent English words often form 2-grams that are repeated many times throughout the corpus, which significantly lowers the degrees of the corresponding nodes. There is also no significant difference observed between comma, full stop and the other common words. In the remaining five languages, more significant variability among different items is observed. In German, the pronouns: er, sie, and ich are represented by larger differences between both quantities (1/6 < a(i) < 1/4) than the punctuation marks and the other considered words (1/3 < R < 1/2). In French, the pronouns/articles: le, la, il show large differences up to a(i) ≈ 1/8, the pronoun les, the preposition á, and the conjunction et show small differences (a(i) ≈ 3/5), and the punctuation marks present moderate behaviour (Fig. 5(middle)). In Italian, all the considered objects except for full stop are characterized by small and steady difference between their frequency and degree. What is important, in contrast to the Germanic languages, there are comparable, rather small differences between fi and ki for the corresponding words in French and Italian.
More significant differences betweenfiandkiare observed for Polish and Russian (Fig. 5(bottom)). The smallest difference is for a Polish conjunctioni(a(i) ≈ 3/4) since this word does not have any special collocation with other words. On the other hand, the punctuation marks can be collocated with specific words and this property is reflected in the largest difference betweenfiandki(a(i) ≈ 1/3), but nevertheless this difference does not exceed those observed for other words much. In Russian the variability between the items is also strong with the pronouns
and
exhibiting the largest differences (1/6 <a(i) < 1/3), while the conjunction
and the prepositions:
,cexhibiting the smallest ones (a(i) ≈ 3/5). The properties of the punctuation marks in both languages are alike.
For further calculations, two other local measures are used, that is, the average shortest-path length (ASPL) for a specific node ℓi and the local clustering coefficient Ci. ASPL for a node i refers to the average distance from a particular node to other nodes in the network and it is defined as follows: (2)ℓi=1n−1∑jnd(i,j),<math><mrow is="true"><msub is="true"><mi is="true">ℓ</mi><mi is="true">i</mi></msub><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mrow is="true"><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></mfrac><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi></mrow><mi is="true">n</mi></munderover><mi is="true">d</mi><mrow is="true"><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo></mrow><mo is="true">,</mo></mrow></math>where d(i, j) denotes the shortest path (i.e., the one consisting of the minimal number of edges) between i and j, while n is the number of nodes in the network. The local clustering coefficient (LCC) for a node i is: (3)Ci=2eiki(ki−1),<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">2</mn><msub is="true"><mi is="true">e</mi><mi is="true">i</mi></msub></mrow><mrow is="true"><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></mfrac><mo is="true">,</mo></mrow></math>where ei is the number of connections between direct neighbours of the ith node and ki is its degree. This measure defines the density of links between direct neighbours of a given node and it can reveal membership of this node in a specific subset of strongly interconnected nodes [12]. In order to calculate ℓi and Ci, one has to note that both quantities depend on n [16]. This is because, according to the Heaps law, there is a non-linear dependence between the text size s and the vocabulary size n: n ∼ sβ(s) with β(s) monotonically decreasing to zero for the infinitely long samples [11]. In result the network becomes saturated gradually with increasing the sample size and tends to form almost a dense graph with only those edges missing that are forbidden by grammar. Therefore, typically ℓi decreases with increasing s, while Ci increases with s [16]. This effect can thus be observed also in the present study if we calculate both quantities for different values of s (s ≪ sc in order to limit the calculation time).
Specifically, each monolingual corpus of length sc ≈ 106 was looped by connecting the last stop mark with the first word (this artificial link was removed from the networks, of course). Next, a substring of s words (103 ≤ s ≤ 105) was randomly chosen from the corpora and transformed into a word-adjacency network (by looping the corpora, it was always possible to create a substring of words of a given length if only s ≪ sc). This step was repeated m=100<math><mrow is="true"><mi is="true">m</mi><mo is="true">=</mo><mn is="true">100</mn></mrow></math> times giving a collection of m networks (we allowed for the substring overlapping since, for s ≪ sc, obtaining two identical substrings is unlikely). The network parameters ℓi and Ci were calculated for each network realization independently for the 10 most frequent items in each corpus and then their mean was also obtained: ℓi¯=m−1∑mℓi<math><mrow is="true"><mover accent="true" is="true"><msub is="true"><mi is="true">ℓ</mi><mi is="true">i</mi></msub><mo is="true">¯</mo></mover><mo is="true">=</mo><msup is="true"><mi is="true">m</mi><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mo is="true">∑</mo><mi is="true">m</mi></msub><msub is="true"><mi is="true">ℓ</mi><mi is="true">i</mi></msub></mrow></math> and Ci¯=m−1∑mCi,<math><mrow is="true"><mover accent="true" is="true"><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub><mo is="true">¯</mo></mover><mo is="true">=</mo><msup is="true"><mi is="true">m</mi><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msup><msub is="true"><mo is="true">∑</mo><mi is="true">m</mi></msub><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub><mo is="true">,</mo></mrow></math> respectively, together with its standard errors: σℓi<math><msub is="true"><mi is="true">σ</mi><msub is="true"><mi is="true">ℓ</mi><mi is="true">i</mi></msub></msub></math> and σCi<math><msub is="true"><mi is="true">σ</mi><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub></msub></math>.
The functional dependence ofℓ¯i(s)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">s</mi><mo is="true">)</mo></mrow></mrow></math>andC¯i(s)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">s</mi><mo is="true">)</mo></mrow></mrow></math>for the most common English words and punctuation marks is presented inFig. 6(left) andFig. 6(right), respectively. For the other languages considered here both plots look qualitatively similar except for that different words can be listed in each case. It is interesting to note thatℓ¯i(s)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">s</mi><mo is="true">)</mo></mrow></mrow></math>for #fs andC¯i(s)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">s</mi><mo is="true">)</mo></mrow></mrow></math>for both #fs and #com do not differ much from their counterparts representing the ordinary words,ℓ¯i(s)<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><mi is="true">s</mi><mo is="true">)</mo></mrow></mrow></math>for comma is distinguished by exceptionally small values while preserving the monotonically decreasing shape of ASPL for the other objects. The results obtained for all the 6 languages are summarized inFig. 7in a form of scatter plotsℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>vs.C¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>for the medium sample size ofs=104<math><mrow is="true"><mi is="true">s</mi><mo is="true">=</mo><msup is="true"><mn is="true">10</mn><mn is="true">4</mn></msup></mrow></math>. The standard errors determined forℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>andC¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>are typically so small that they do not differ much from the symbol size inFig. 7. The full stop and comma have rather low values of ASPL. Among the considered words, the most distinguished one is the German pronounichwith a significant variability of bothℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>andC¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>among the individual sample networks. Although not explicitly shown here, the same observation refers to this word’s counterparts in other languages (likeI, je, ja,
), whose variability is related to particular choices of the considered texts with different narration types.
Owing to the ASPL definition, in each case the value of ℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> is negatively correlated with the node degree ki. LCC is also strongly anticorrelated with ki and its empirical dependence on the node degree is roughly power-law, which agrees with the theoretical considerations for the hierarchical networks [7]. This double dependence on ki means that C¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> may also be considered a function of ℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math>. We expect thus that substantial contribution to the variability of C¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> and ℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> in Fig. 7 comes from this relation. In order to show this, we calculated both quantities for the random null model, in which no correlations between the words are allowed and their occurrences are governed only by their relative frequency given by the Zipf distribution. The corpora in each language was randomly shuffled, so the constituent texts lost their significance as they became just meaningless word sequences. Then we constructed the corresponding word-adjacency networks and calculated both ASPLs and LCCs for the nodes representing the same words as in Fig. 7. We repeated this procedure 100 times independently. Indeed, for each language we obtained an approximately power-law relation: C¯i(ℓ¯i)∼ℓ¯iγ<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">∼</mo><msubsup is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi><mi is="true">γ</mi></msubsup></mrow></math> with γ > 0 (denoted by a dashed line in Fig. 7).
The points that denote the (ℓ¯i,C¯i<math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></mrow></math>) coordinates for the particular items in Fig. 7 are distributed along this functional dependence. This means that the item positions on the scatter plots are strongly influenced by these items’ frequencies, while the actual grammar- and context-related contributions to C¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> and ℓ¯i<math><msub is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi></msub></math> are less evident. Therefore, we decided to remove the frequency-based contributions by dividing the empirical values by their average random-model counterparts: C¯iR<math><msubsup is="true"><mover accent="true" is="true"><mi is="true">C</mi><mo is="true">¯</mo></mover><mi is="true">i</mi><mi mathvariant="normal" is="true">R</mi></msubsup></math> and ℓ¯iR<math><msubsup is="true"><mover accent="true" is="true"><mi is="true">ℓ</mi><mo is="true">¯</mo></mover><mi is="true">i</mi><mi mathvariant="normal" is="true">R</mi></msubsup></math>. The resulting positions of the items are shown in Fig. 8. Now it is more evident than in Fig. 7 that both the high-frequency words and the punctuation marks occupy similar positions and no quantitative difference can be identified that is able to distinguish between both groups. In this figure, we also show these quantities calculated for three sample words chosen randomly from more distant parts of the Zipf plot: time (R=75<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">75</mn></mrow></math> in the English corpus), face (R=130<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">130</mn></mrow></math>), and home (R=264<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">264</mn></mrow></math>), as well as their semantical counterparts in the other languages (occupying different ranks, see Table 2). Obviously, each of these words may also have other, non-equivalent meanings in distinct languages and, while some languages use inflection, the other ones do not, which inevitably contribute to the rank differences. In contrast to the most frequent words discussed before, these words are significantly less frequent, which can itself lead to some differences in the statistical properties as compared to the top-ranked words. Therefore, they are not shown in Fig. 7, because their local clustering coefficient significantly exceeds the vertical axis upper limit. In Fig. 8, the sample lexical words are located in different places for different languages, but typically their average position is more or less shifted towards the upper left corner of the plots. This effect is the most pronounced for French, then for English, Russian, Italian, and Polish, while it is absent for German. This visible shift may originate from either the statistical fluctuations among the words, the statistical fluctuations among the texts selected for the corpora, or be a genuine effect for the less frequent words, the parts of speech, and/or a general property of the lexical words in specific languages. However, since our sample of the medium-ranked words is small, at present we prefer not to infer any decisive conclusions from this result as we plan to carry out a related, comprehensive study in near future. Nevertheless, we stress here that such displacements exhibited in Fig. 8 by the words of medium frequency by no means contradict our main statement that the punctuation marks show similar statistical properties as the most frequent words.
Now we consider another property of nodes, i.e., the indicators how important for the network structure their presence is. In other words, we study how the removing of particular nodes can impact the overall network structure expressed in terms of the global network measures. We look at three such measures: the average shortest path length: L=∑iℓi,<math><mrow is="true"><mi is="true">L</mi><mo is="true">=</mo><msub is="true"><mo is="true">∑</mo><mi is="true">i</mi></msub><msub is="true"><mi is="true">ℓ</mi><mi is="true">i</mi></msub><mo is="true">,</mo></mrow></math> the global clustering coefficient: C=∑iCi,<math><mrow is="true"><mi is="true">C</mi><mo is="true">=</mo><msub is="true"><mo is="true">∑</mo><mi is="true">i</mi></msub><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub><mo is="true">,</mo></mrow></math> and the global assortativity coefficient r: (4)r=∑ij(δij−kikj2e)∑ij(kiδij−kikj2e),<math><mrow is="true"><mi is="true">r</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">δ</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">−</mo><mfrac is="true"><mrow is="true"><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><msub is="true"><mi is="true">k</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">e</mi></mrow></mfrac><mo is="true">)</mo></mrow></mrow><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><msub is="true"><mi is="true">δ</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo is="true">−</mo><mfrac is="true"><mrow is="true"><msub is="true"><mi is="true">k</mi><mi is="true">i</mi></msub><msub is="true"><mi is="true">k</mi><mi is="true">j</mi></msub></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">e</mi></mrow></mfrac><mo is="true">)</mo></mrow></mrow></mfrac><mo is="true">,</mo></mrow></math>where e is the number of edges in the network and δij equals 1 if there is an edge between the nodes i and j or 0 otherwise. Due to the same reason as before, we first calculate the corresponding quantities LR, CR, and rR for the randomized text samples (100 independent realizations) and consider them the reference values determined solely by the frequencies of particular items and by neither grammar nor context. We thus consider the values of L(R)/LR(R) (Fig. 9), C(R)/CR(R) (Fig. 11), and r(R)/rR(R) (Fig. 13) and expect them to be related to grammar and context largely. For different text samples (novels), we compare the corresponding values calculated for a complete network with all the nodes present (denoted by the abscissa R=0<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>) and for 10 incomplete networks obtained by removing a given highly ranked node according to the Zipf distribution (1 ≤ R ≤ 10).
In each case, by removing one of the highly connected nodes, ASPL becomes longer than for the complete network and this is not surprising since the network loses one of its hubs. This increase of L(R) is different for different ranks and different novels - see Fig. 9, but a rule is that, statistically, the lower the rank (the larger R) is, the smaller is the change in L(R) (for a particular novel there might be some exceptions). This rule comes from the fact that in the word-adjacency networks removing a strong hub is more destructive for the network than removing some less connected node. This means that in a typical situation L(R) alters its value the most for comma and the full stop since they occupy the highest ranks, while the observed changes for the function words are smaller. This picture substantially changes if we look at the rescaled quantity: λ(R)=L(R)/LR(R)<math><mrow is="true"><mi is="true">λ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">L</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">/</mo><msup is="true"><mi is="true">L</mi><mi mathvariant="normal" is="true">R</mi></msup><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow></mrow></math> that is free of an item’s frequency contribution to ASPL. Fig. 10 shows that, except for #com, λ(R) does not exhibit any significant dependence on R and excluding a particular node does not alter its value much as compared to the complete network. Typically, the rescaled ASPL is restricted to a narrow range of 0.95 < λ(R) < 1.0 and this means that the correlations present in the text samples shorten effectively the paths between the nodes as compared to the random network, but this is a small effect. The case of comma is slightly different as, for some texts, the network without this node shows λ ≈ 1.0, i.e., the residual network has the same L(R) as the random one. Presence of this property of comma is text-dependent and it does not seem to be a property of written language. Moreover, it should be stressed that, even if one considers comma, the range of the λ(R) variability for different nodes is small.
The global clustering coefficient C(R) and the assortativity index r(R) present a more variable behaviour after removing a hub as these quantities can either increase, remain stable, or decrease. This behaviour obviously depends on a contribution of each particular node to C and r for R=0<math><mrow is="true"><mi is="true">R</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math>. For the clustering coefficient, a statistical rule is that without particular nodes C(R) does not differ much from its complete-network counterparts. Only for the node representing comma, C(R) can increase more significantly and the network becomes more clustered (Fig. 11). This can partially be explained by an observation that in all the considered languages commas can mediate words whose direct neighbourhood is unlikely due to rules of grammar. Since C(R) depends on an item’s frequency, in Fig. 12 we show the rescaled coefficient: κ(R)=C(R)/CR(R)<math><mrow is="true"><mi is="true">κ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">/</mo><msup is="true"><mi is="true">C</mi><mi mathvariant="normal" is="true">R</mi></msup><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow></mrow></math> whose values are related to the random model. Now the networks without #com and the ones without other nodes show comparable values of κ with only small difference (up to 10%) for some texts.
As regards the assortativity index r(R), the majority of hubs in the word-adjacency networks (like, e.g., #fs, articles, and the most frequent conjunctions) can be considered disassortative separators, so after their removal, the overall assortativity index increases (Fig. 13). Of course, since this is only a statistical observation, particular cases may show different behaviour like, e.g., comma, which sometimes acts like a disassortative separator and sometimes like an assortative one. In order to remove the approximately monotonous dependence of r on rank R, we look at the rescaled assortativity index: ρ(R)=r(R)/rR(R)<math><mrow is="true"><mi is="true">ρ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">=</mo><mi is="true">r</mi><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow><mo is="true">/</mo><msup is="true"><mi is="true">r</mi><mi mathvariant="normal" is="true">R</mi></msup><mrow is="true"><mo is="true">(</mo><mi is="true">R</mi><mo is="true">)</mo></mrow></mrow></math>. We see that now this dependence is absent and that both the punctuation marks and the function words exhibit comparable values of ρ (see the narrow range of the vertical axis in each panel of Fig. 14).
Punctuation marks are among the most common objects in written language. They do not play purely grammatical roles, but they also carry some semantic load, similar to such words like articles, conjunctions, and prepositions. This opens space for putting a question whether the punctuation marks may be included in any lexical analysis on par with the ordinary words. In this work we addressed this question by comparing the statistical properties of the most common punctuation marks and words using two approaches. We observed that the punctuation marks locate themselves exactly on or in a close vicinity of the power-law Zipfian regime as if they were ordinary words. Moreover, their inclusion acts towards restoring of the Zipf power-law from the more flat Zipf–Mandelbrot behaviour. We drew the same conclusion from an analysis of the word-adjacency networks, in which words, full stops (the aggregated sentence-ending punctuation marks), and commas were considered nodes. In such networks, the punctuation marks are more important than typical nodes: they play a role of the hubs (together with the most frequent words). Despite some minor, quantitative-only differences, topology of such networks and their growth is similar from the perspective of punctuation marks and from the perspective of words. Quantitatively, it is expressed by the node-specific average shortest path length, the local clustering coefficient, the local assortativity, and their global counterparts. These results are qualitatively invariant under language change even for the languages belonging to different Indo–European groups. Regarding the quantitative viewpoint, we do observe certain systematic differences of the network properties between different text samples (including different languages), but considering them here is beyond the scope of this work. A related study will be presented and discussed elsewhere.
By taking all these outcomes into consideration, the principal conclusion from this study is that punctuation marks are almost indistinguishable from other most and medium common words (both the function and the lexical ones) if one investigates their statistical properties. Since the punctuation marks have also non-neglectable meaning, we advocate their inclusion in any type of the word-occurrence and the word-adjacency analysis making it to be more complete. Incorporation of the punctuation marks into an analysis extends its dimensionality and, therefore, it opens more space for possible manifestation of some previously unobserved effects. That this can in fact be fruitful and bring important results, the best example is Ref. [8] where we showed that the sentence length variability can be multifractal for specific (written with the stream-of-consciousness narration) group of texts, while for other texts it remains monofractal. Multifractality is inherently accompanied by burstiness. In the present context this burstiness in the sentence length thus translates itself into analogous effects in the recurrence times (measured by a separation of two consecutive occurrences of the same item) of the full stops. At the same time, however, the recurrence times of the most frequent words appear to be much less bursty as it was also documented in the same Ref. [8]. This latter effect goes in parallel with an observation made earlier [1] for the most frequent words. Interestingly, according to the same paper [1], the burstiness is however often observed for the non-function words of high and medium ranks. The related intricacy in fact further supports our thesis that, from the statistical point of view, the punctuation marks are surprisingly similar to the regular words, even though at some angles they resemble more the most frequent function words, while at other angles they resemble more the non-function words. We expect more interesting results will be obtained in future from analyses, in which the punctuation marks are not neglected.
We thank the anonymous referees for very interesting and insightful suggestions that led to significant extensions and improvement of this paper.
The books used in our analysis (asterisks denote the corpora-forming books):
English: George Orwell 1984*, Mark Twain Adventures of Huckleberry Finn, Herman Melville Moby Dick*, Jane Austen Pride and Prejudice*, James Joyce Ulysses*, Jonathan Swift Gulliver’s Travels*, Margaret Mitchell Gone with the Wind.
German: Friedrich Nietzsche Also sprach Zarathustra*, Franz Kafka Der Process*, Heinrich Mann Der Untertan*, Thomas Mann Der Zauberberg*, Christiane Vera Felscherinow Wir Kinder vom Bahnhof Zoo*.
French: Alexandre Dumas Ange Pitou*, Albert Camus La Peste*, Émile Zola La Terre*, Le Docteur Pascal, Gustave Flaubert Madame Bovary*, Gaston Leroux Le Fantôme de L’Opéra*.
Italian: Umberto Eco Il pendolo di Foucault*, Gabriele d’Annunzio Trionfo della morte*, Giambattista Bazzoni Falco della Rupe o la guerra di Musso*, Luigi Capuana Giacinta*, Tullio Avoledo Le Radici del Cielo*.
Polish: Gustaw Herling-Grudziński Inny świat*, Karol Olgierd Borchardt Znaczy Kapitan*, Walery Łoziński Zaklȩty dwór*, Stefan Żeromski Przedwiośnie*, Władysław Reymont Ziemia obiecana*, Bolesław Prus Lalka, Jerzy Andrzejewski Bramy raju.
Russian: Lev Tolstoy
(Anna Karenina)*,
(Voyna i mir)*,
(Voskreseniye)*,Fyodor Dostoyevsky
(Besy)*,
(Brat’ya Karamazovy)*.
Czech: Bohumil Hrabal Taneční hodiny pro starší a pokročilé.