With the development of information technology, people at present enjoy the convenience of network. While the number and scale of security threats is growing rapidly, which has caused great damage to network resources and privacy leaks. Methods and features of network intrusion are constantly changing and developing. Thus intrusion detection is still an important research issue at present.
Intrusion detection technology has been continuously studied by researchers (Moustafa et al., 2019; Bhuyan et al., 2013; Jaiganesh et al., 2013; Aburomman and Reaz, 2017; Kabir et al., 2018). In general, intrusion detection can be taken as a classification problem, classifying the incoming network into normal and attack one. Existing intrusion detection models mainly combine various existing machine learning methods with intrusion detection data sets. The intrusion detection data set is a general big data set, which is directly input into the existing various machine learning models to train the intrusion detection classifier. And various current learning methods can be broadly classified into three types: traditional machine learning based method, deep learning based method, and hybrid method.
The traditional machine-learning methods include Support vector machine (SVM), k-Nearest Neighbor (kNN), Decision Trees, and so on. As collected data sets become larger and larger, deep learning-based approaches are gaining much attention since they can learn computational process in depth and may lead to better generalization capabilities. There are methods like Deep belief network (DBN), Convolutional neural network (CNN), Recurrent neural network (RNN), AutoEncoder, and so on. In order to further improve the accuracy of recognition, the method of combining various data classification methods to form a hybrid classifier has been studied. A large number of experiments shown that hybrid-based techniques display a better detection performance for specific data sets. Because of a specific classifier and merge method, they can achieve higher precision and detection rate than a single method.
Through continuous efforts, researchers now are able to design high accuracy detectors for fixed intrusion data sets. However, due to the continuous dynamic changes of network intrusion traffic, high accuracy for only fixed data sets cannot guarantee the excellent detection performance in the face of dynamic traffic. Our work conducts to analyze the detectability of dynamic intrusion traffic, and we then propose an effective intrusion detection algorithm based on semantic re-encoding and deep learning. Semantic re-encoding technology attempts to re-express the semantic space of intrusion traffic to achieve the purpose of increasing the distinguishability of abnormal traffic. On the basis of semantic re-encoding, deep learning technology is used to enhance the generalization ability of the intrusion detection model. The main contributions of this work are as follows:
We find that the semantics of network traffic are different. Normal network traffic and attack network traffic often have significant differences in narrative semantics. Based on this, a semantic re-encoding method for intrusion network flow is designed, which can effectively increase the distinguish ability of abnormal network traffic.
We design a deep learning-based detection model for intrusion traffic, which enhances the generalization capabilities of intrusion detection models.
Experimental results show that our approach get competitive performance.
The rest of this paper is organized as follows: Section 2 introduces the related works. Section 3 describes our proposed method in detail. Section 4 shows experimental performances. Finally, the conclusion is presented in Section 5.
Previously, many researchers use methods on pure traditional classifiers to the intrusion detection field. There are classifiers like Naïve Bayes, SVM, decision trees, kNN and so on (Dhanabal and Shantharajah, 2015; Deshmukh et al., 2015; Heba et al., 2010; Naoum and Al-Sultani, 2012).These methods have indeed achieved a lot of achievements, and laid a solid foundation for later research.
Many researches have been conducted since deep-learning. Researches make lots of work on the preprocess of dataset as well as search for good classifiers (Vinayakumar et al., 2017; Yin et al., 2017; Wu et al., 2018; Naseer et al., 2018; Hsu et al., 2018; Blanco et al., 2018). Various methods have been employed to improve the performance of intrusion detection based on the public dataset NSL-KDD. Ingre and Yadav (2015) propose a simple Artificial Neural Network method for intrusion detection. And they test the dataset on various layers of classifier and also do feature selection on the dataset. Zhang (Zhang et al., 2019) combines genetic algorithm with deep belief network, which effectively improves the detection rate and reduces the complexity of classifier. The genetic algorithm is used to select the appropriate network structure, and then use DBN to classify the samples of the dataset. Al-Qatf et al. (2018) develop a new method by using the sparse autoencoder for feature learning and dimensionality reduction. Besides, the learned features are fed into SVM algorithm to get final classification result. The paper uses an unsupervised method to reduce the dimension of feature. However, the effect of unsupervised learning lacks a reliable basis. Le et al. (2019) apply a feature selection model to test the performance of various RNN models on the dataset. The feature selection model is to generate the best feature subset from the original feature set. Combined with the LSTM, the model achieved good performance on the IDS test dataset. Chouhan et al. (Chouhan Khanet al., 2019) propose a novel Channel Boosted and Residual Learning architecture for deep convolutional neural network. The experiment shows that the method can improve the classification accuracy. The above methods did varies preprocess to the specific dataset of intrusion, but they did not highlight the difference between the normal traffic and the attack one on purpose.
Many researchers apply hybrid approach on the dataset to get good performance (Li et al., 2017; Gao et al., 2019; Shrivas and Dewangan, 2014). Li (Li et al., 2017) propose a two-step hybrid approach to solve the problem. Step 1 uses C4.5 algorithm to get most of the exact label of the samples. Step 2 employs kNN to divide the rest uncertain ones. The model showed high accuracy but still lack of generalization ability, and the performance maybe not so good on other intrusion datasets. Gao et al. (2019) apply several base classifiers, including decision tree, random forest, kNN, DNN, and design an ensemble adaptive voting algorithm. Zhang et al. (2018), propose an Xgboost based on stacked sparse autoencoder network (SSAE-XGB) method. The SSAE is used to learn the latent representation of original data, and the ensemble binary tree do the classification. This method has weak point on the interpretable of feature.
Yang et al. (2019) apply DCGAN to generating new samples and then use the modified LSTM classifier method to classify the datasets. This method showed the augment of dataset is important. However, GAN needs huge compute resources and the generation of new data is lack of interpretable.
Compared with previous works, we propose a combination of algorithm based on Semantic Recoding and Deep Learning. The semantic re-encoding is an effective feature extraction algorithm for intrusion detection with good interpretable on feature, and the Deep Learning is to gain good generalization ability.
As people's activities in cyberspace become more frequent, network intrusion traffic presents a trend of continuous dynamic changes, which makes the detection model for fixed dataset design often unsatisfactory. More importantly, dynamically changing network intrusion traffic has a large number of hidden and burst features showing discontinuity, while the current mainstream deep learning model behaves better at characterizing continuously changing data features. Then how to improve the determination of network intrusion detection model as well as the ability to adapt to the dynamic changes of the detection objects, becomes an important research issue. In this study, we attempt to incorporate some knowledge of network intrusion traffic into the intrusion classifier design.
There are two types of IDS data sets. One is the protocol representation string of the original data, and the other is the feature set extracted by the experts from the original data applied to the host. For the original data stream, by observing the attack and defense characteristics of the network application, it can be found that for the network attack acting on a specific application, the normal traffic and the attack traffic are significantly different in the narrative semantics. In other words, the attacker will inevitably change the original logic of a certain level of network application. As long as the logic can be clearly expressed, it can form a specific network traffic characteristic. This suggests that we can effectively differentiate network intrusion traffic with effective semantic transformation. For feature-extracted data streams, such as NSL-KDD datasets, the direct semantic differences between attack and normal traffic are not easily characterized, but semantic differences between attacks and normal traffic can still be expressed through some escaping combinations.
The details of the proposed method based on semantic re-encoding and deep learning (SRDLM) will be described in the next subsections.
In our method, both raw and converted network traffic can be semantically re-encoded, while the specific methods are different. As a result, we first judge whether the dataset is made up of raw data or not, and then re-code them separately. For the characterizable network traffic, such as web traffic, we resequence the network traffic characteristics to a new symbol space by re-encoding.
The existing network traffic raw data can easily be converted into a form of character stream. For example, the network application on the Internet uses the HTTP protocol to express its semantics in the form of the character stream. Furthermore, the network traffic of some non-http protocol can also be analyzed through its protocol format and then be transformed into a character stream.
Thus, the step 1 is “Transform data to character stream”.
Step 2, Word Segmentation. It means a word table that needs to be extracted from the raw character stream. We use delimiters such as breakpoint punctuation and special characters to split character stream. After that, we split the character stream into word string, and then pick up the words into a word table separately. For example, in the HTTP protocol, ‘&’, ‘|’, ‘\’, ‘?’, ‘‖’ are delimiters. As delimiters will vary with the network protocols, it needs to be updated continuously. According to the original word list, all records in the character stream are converted into word array, and the access string is rearranged into a record set composed of words in the word list. Abnormal and normal samples from the access traffic are processed, and a sample record set with words segmented is formed.
Step 3, Word Table Reordering. We want to get a new word table to map the origin word table. As shown in Fig. 1, the new word table is ordered by the difference between positive and negative sample. We then need to calculate the difference between positive and the negative sample, and define it as comprehensive word frequency (CWF). We calculate positive and negative Word Frequency (WF) in the original word table separately and sort word array according to the word frequency. If a word appears multiple times in a row, it is counted as one time. Set threshold T1, T2, we can easily see from Fig. 1 that T1 > 0 and T2 < 0. If CWF > T1 or CWF < T2, which means that the WF between positive and negative is enormous, thus we will do One-to-one re-encoding. While CWF < T1 and CWF > T2, do Many-to-one re-encoding. Besides, the multiple words that CWF < T1 and CWF > T2 are combined into word WordM. And unknown words are also encoded as WordM. Sorting the original word table by CWF, we get the new word table, named Word-table-after.
Step 4, Word Re-mapping. Positive and negative sample records are remapped to new isometric sample records based on the new word table (Word-table-after). As shown in Fig. 2, in the whole mapping process, multiple words will be mapped to one word, which is called Word bag mapping. When n words in the sample map to the same word, the value at the corresponding position of the word is added by n. The mapping procedure makes any unequal-length word sequence to the equal-length word sequence. When all the positive and negative samples re-map, we get a set of training samples of equal length.
Step 5, Word Sequence Re-projection. Word sequence re projection can be used as an optional step to make the new word sequence more distinguishable in semantic space. The re-projection function can be as follows:(1)S=xi,yi,1≤i≤m,y∈{0,1}<math><mi is="true">S</mi><mo is="true">=</mo><mfenced close="}" open="{" is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></mfenced></mrow></mfenced><mo is="true">,</mo><mn is="true">1</mn><mo linebreak="goodbreak" is="true">≤</mo><mi is="true">i</mi><mo linebreak="goodbreak" is="true">≤</mo><mi is="true">m</mi><mo is="true">,</mo><mi is="true">y</mi><mo linebreak="goodbreak" is="true">∈</mo><mrow is="true"><mo stretchy="false" is="true">{</mo><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn></mrow><mo stretchy="false" is="true">}</mo></mrow></math>
m is the number of training samples, and Xi means the record (word sequence) in the training set. The length of the word bag is n. y∈0,1<math><mi is="true">y</mi><mo is="true">∈</mo><mfenced close="}" open="{" is="true"><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn></mrow></mfenced></math> is the label of sample.
First, calculate the mean of positive and negative samples μj respectively, j means the class type, positive and negative.(2)μj=1mj∑(xi,yi)∈S&yi=jxi,j∈{0,1},1≤i≤m<math><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">m</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></mfrac><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mrow is="true"><mo stretchy="false" is="true">(</mo><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow><mo stretchy="false" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">∈</mo><mi is="true">S</mi><mi is="true">&#x26;</mi><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo linebreak="badbreak" is="true">=</mo><mi is="true">j</mi></mrow></munder><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><mspace width="1em" is="true"></mspace><mi is="true">j</mi><mo linebreak="goodbreak" is="true">∈</mo><mrow is="true"><mo stretchy="false" is="true">{</mo><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn></mrow><mo stretchy="false" is="true">}</mo></mrow><mo is="true">,</mo><mn is="true">1</mn><mo linebreak="goodbreak" is="true">≤</mo><mi is="true">i</mi><mo linebreak="goodbreak" is="true">≤</mo><mi is="true">m</mi></math>
Second, calculate the intra-class dispersion matrix of positive and negative samples Sw. X0 is the sample in the positive set, and X1 is the sample in the negative one.(3)Sw=∑x∈X0x−μ0x−μ0T+∑x∈X1x−μ1x−μ1T<math><msub is="true"><mrow is="true"><mi is="true">S</mi></mrow><mrow is="true"><mi is="true">w</mi></mrow></msub><mo is="true">=</mo><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">x</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></mrow></munder><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></mrow></mfenced><msup is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi><mo linebreak="badbreak" is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub></mrow></mfenced></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup><mo linebreak="goodbreak" is="true">+</mo><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">x</mi><mo linebreak="badbreak" is="true">∈</mo><msub is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></munder><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></mfenced><msup is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><mi is="true">x</mi><mo linebreak="badbreak" is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></mfenced></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup></math>
Third, calculate the inter-class dispersion matrix between positive and negative one.(4)Sd=μ0−μ1μ0−μ1T<math><msub is="true"><mrow is="true"><mi is="true">S</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo is="true">=</mo><mfenced close=")" open="(" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></mfenced><msup is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo linebreak="badbreak" is="true">−</mo><msub is="true"><mrow is="true"><mi is="true">μ</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></mfenced></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup></math>
Finally, calculate positive and negative sample space separation projection vectors W. W is the corresponding eigenvector of the largest eigenvalue of the matrix Sw−1Sd<math><msubsup is="true"><mrow is="true"><mi is="true">S</mi></mrow><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mo is="true">−</mo><mn is="true">1</mn></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">S</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub></math>. Then we do the dimensionality reduction operation, and the new feature space is wTxi.(5)S1=xi1,yi1,1≤i1≤m,y∈{0,1}<math><mi is="true">S</mi><mn is="true">1</mn><mo is="true">=</mo><mfenced close="}" open="{" is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">i</mi><mn is="true">1</mn></mrow></msub></mrow></mfenced></mrow></mfenced><mo is="true">,</mo><mn is="true">1</mn><mo linebreak="goodbreak" is="true">≤</mo><mi is="true">i</mi><mn is="true">1</mn><mo linebreak="goodbreak" is="true">≤</mo><mi is="true">m</mi><mo is="true">,</mo><mi is="true">y</mi><mo linebreak="goodbreak" is="true">∈</mo><mrow is="true"><mo stretchy="false" is="true">{</mo><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn></mrow><mo stretchy="false" is="true">}</mo></mrow></math>
xi1is the record in the training set. The dimension is n1. If the previous semantic re-encoding can better reflect the difference between normal and attack traffic, then this step can further open the spatial distance of the positive and negative samples. The algorithm is formalized in Algorithm 1.Algorithm 1. Network character stream semantic re-encoding algorithm.1: Transform data to the character stream.2: Word segmentation.3: Word Table Reordering.4: Word Re-mapping.5: Word Sequence Re-projection.(Optional)
For data streams extracted from raw data, a set of m records is used to express the network traffic,
X1,y1,X2,y2…Xm,ym<math><mfenced close="}" open="{" is="true"><mrow is="true"><mfenced close=")" open="(" is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msup><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msup></mrow></mfenced><mo is="true">,</mo><mfenced close=")" open="(" is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></mfenced><mo is="true">…</mo><mfenced close=")" open="(" is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">X</mi></mrow><mrow is="true"><mi is="true">m</mi></mrow></msup><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">m</mi></mrow></msup></mrow></mfenced></mrow></mfenced></math>. X is the feature vector, y is a label, such as NSL_KDD. Semantic re-encoding can still be used. However, because their original semantics have escaped, the effect of semantic re-encoding is not as good as the raw data stream.
We slightly modify Algorithm 1 and apply it to the NSL_KDD dataset. The NSL_KDD consists of original 41-dimensional features, and we then perform semantic re-encoding on each dimension feature separately. The specific process is as follows:
For symbolic feature, a one-hot encoding is used to transfer the feature space. For example, the protocol type feature has three types: TCP, UDP, ICMP. Then after a one-hot encoding, three types are converted into (0,0,1), (0,1,0),(1,0,0) separately.
For continuous numerical type feature i, we first normalize the feature to 0–1. And then we divide the feature equally to Nf intervals and count the feature frequency (FF) of normal and attack, or the positive and negative type. We think all Nf intervals as potential features. And we call the difference of the positive feature frequency and the negative feature frequency as comprehensive feature frequency (CFF). Set a threshold T = [T1,T2] as the standard of either do segment to the feature or not. T1 > 0,T2 < 0. According to the threshold, we can segment the feature to discrete type. Here are the specific operation: First, judge the CFF of one interval if CWF > T1 or CWF < T2, then add the upper limit value to intervali. In other words, the samples can be classified according to the intervals in intervali of feature i. Second, use a one-hot encoding. The transformation function is as follows: xi → vjvj+1 … vj+t. For example, Fig. 3 is the distribution of the original data feature 29. We set Nf = 20, so a single interval size is 0.05. Set T = [10000, −500]. We think normal is a positive type. We can see the intervals (0–0.05), (0.05–0.1), (0.1–0.15), the CFF < −500. And for intervals between 0.15 and 0.95, the CWF is betweenT1 and T2. Then we merge the intervals to a feature. About interval (0.95–1), CWF > 10000. So we can set T29 = [0.05, 0.1, 0.15.0.95]. The result of this step is shown in Table 1. This step is almost the same operation as Algorithm 1 step 3. Word Table Reordering.
For NSL-KDD, after doing the previous Reordering step, the original 41 dimension data changes to 169 dimension data. As shown in Fig. 4, the t-SNE (Maaten and Hinton, 2008) algorithm is used to represent multi-dimensional data in a 3-dimensional space. It can be seen that in contrast with 41 dimension, the 169 dimension data of the normal type and attack type can be separated more easily after Word Table Reordering. In other words, in the 169 dimension feature space, the inter-code space is broadened. However, there are some samples overlapped in 169 dimension. We add some PCA features to make the overlapped samples apart.
We list some of the symbols in Table 2 to describe the algorithm. After adding Npca features the original n dimension is converted to N dimension. N=n+∑i=1nlen_interi+Npca<math><mi is="true">N</mi><mo is="true">=</mo><mi is="true">n</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">+</mo><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></msubsup><mi is="true">l</mi><mi is="true">e</mi><mi is="true">n</mi><mtext is="true">_</mtext><mi is="true">i</mi><mi is="true">n</mi><mi is="true">t</mi><mi is="true">e</mi><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">p</mi><mi is="true">c</mi><mi is="true">a</mi></mrow></msub></math>. Further, the N-dimensional data is extended to 4∗N data, which consists of four parts (v, p, n, s). As shown in Fig. 5, the original n-dimensional vector X is semantically re-encoded to get the N-dimensional vector v. We calculate the mean feature map of positive training data (vp) and the negative training data (vn). Then (p, n, s) is calculated as follows:(6)p=v−vp<math><mi is="true">p</mi><mo is="true">=</mo><mi is="true">v</mi><mo linebreak="goodbreak" is="true">−</mo><mi is="true">v</mi><mi is="true">p</mi></math>(7)n=v−vn<math><mi is="true">n</mi><mo is="true">=</mo><mi is="true">v</mi><mo linebreak="goodbreak" is="true">−</mo><mi is="true">v</mi><mi is="true">n</mi></math>(8)s=v∗v<math><mi is="true">s</mi><mo is="true">=</mo><mi is="true">v</mi><mo is="true">∗</mo><mi is="true">v</mi></math>
The algorithm is formalized by Algorithm 2.Algorithm 2. Semantic dispre-encoding for data extracted from raw data.Input: input parameters codei, NpcaOutput: output final_code1: Initial: interval list Intervali and len_interi2: if f_type == 0 then3: len_interi = d_numi4: goto 155: else6: while j ≤ Nfdo7: if CFFj > T1 or CFFj < T2 then8: Intervali.add(j∗1/Nf)9: end if10: end while11: len_interi = len(Intervali)12: end if13: codei = norminization(codei)14: codei = threshold_segment(codei, Intervali)15: new_codei = one_hot(codei, len_interi)16: v = add_pca_feature(new_codei, Npca)17: final_code = calculate p, n, s, and combine v, p, n, s18: return final_code
We think of network traffic as a character stream of words. It is be assumed that all attack traffic and normal traffic can be distinguished by a sequence of finite-length words. We let q be a prime and n a positive integer. A q − ary(n, K, d) code is used to represent the codeword space formed by a sequence of words, where 1 ≤ d ≤ n, n represents the code length, K represents the number of code words, and d represents the distance between codes. According to the assumption, all attacks and normal traffic are finite-length character sequences, thus the total number of code words K can be a specific value. According to the Singleton Bound, then K ≤ qn−d+1. According to the discussion in Section 1 Introduction, 2 Related work, the semantic re-encoding and re-mapping can increase the code word length, that is, they can increase the value of n. Then, qd≤qn+1K<math><msup is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msup><mo is="true">≤</mo><mfrac is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">q</mi></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msup></mrow><mrow is="true"><mi is="true">K</mi></mrow></mfrac></math>, as the value of n increases, the value of d can increase accordingly. Meanwhile, the semantic re-encoding and re-mapping can increase the average inter-symbol distance between positive and negative samples of the training set. Therefore, if the semantic re-encoding and re-mapping are proper, the average inter-symbol distance between positive and negative samples can be improved, thereby increasing classification accuracy.
We choose the ResNet (He et al., 2016) as a deep learning framework for the method because it shows a good generalization ability for network unknown intrusion traffic.
ResNet is based on Convolutional Neural Networks (CNN). CNN are very similar to ordinary neural networks, and they all consist of neurons with learnable weights and biases. As shown in Fig. 6, ResNet is made up of building blocks with “curved curve”. From Fig. 7, we can see that the building blocks are made up of identity mapping and residual mapping. Those two maps can solve the problem of the accuracy decreasing as the network deepens. The identity mapping is the “curved curve” part. Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers fit another mapping of F(x)≔H(x) − x. The original mapping is recast into F(x) + x.
We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. The formulation of F(x) + x can be realized by feedforward neural networks with “shortcut connections”. Shortcut connections are those skipping one or more layers. Identity-shortcut connections add neither extra parameter nor computational complexity. The deep residual network can be regarded as the ensemble of shallow neural networks of different depths. Deepening the network will not cause the network to degradation, while the residual design can raise the generation ability.
There are many types of network intrusion traffic. For different types of traffic characteristics, a single spatial transformation is generally difficult to achieve proper feature extraction. In order to further separate the feature space formed by positive and negative samples, we proposed an algorithm called Algorithm 3 using multi-space projection.
Sometimes, even for the same type of attack, the network traffic it generates will have many different types of characteristics. For example, in the NSL-KDD attack dataset, the DoS attack type is not balanced. As shown in Fig. 8, the DoS and Normal data sets are processed by PCA (principal component analysis) algorithm for easy observation. The red is the DoS dataset, and the blue is the Normal dataset. It can be seen from Fig. 8 that the red dot overlaps the blue dot, and the DoS datasets are distributed in different three regions. It is difficult for the learning algorithm to learn three different distribution areas for the same type of attack traffic at the same time.
We describe the steps of Algorithm 3 in the following.
The first step in Algorithm 3 is to use K-means to get more negative set. In other words, we separate the negative sets into more groups. The positive sample set is still in a group. For example, we change the attack type of DoS into DoS1, DoS2, and DoS3. The purpose of this step is to make the classification of the sample set closer to the multi-dimensional normal distribution.
The second step: Multi-type binary classifiers (BCs) are used to divide the positive and negative samples. One of BCs consists of a pre-projection module + a specific training set + a ResNet network. Assuming that the pre-projection module runs first, and the network flow feature space obtained has a better specific feature distribution. The new distribution can be taken as a multidimensional normal distribution, and it can be effectively distinguished by BCs. Although ResNet can be stacked to very deep layers, we think that even network is not so deep enough to handle this problem. The structure of BCs is shown in Fig. 9.
There are several BCs and each of them classifies one type of sample. For an in-coming connection x, BCs make their decision, and if the BCi considers that it belongs to the normal, we define BCi(x) = 1, and when it belongs to the abnormal, we define BCi(x) = 0.
The third step is to collect the results of the BCs, and then to calculate the sum of the results.
The final step is to judge if the result is more than 1, then the x is a normal sample, or else the x is an attack sample. That is, if any of the four outcomes votes the connection to be an attack type, then we make the decision that the connection is an attack. The transformation function is as follows:(9)∑i=03BCi(x)>=1<math><munderover accent="false" accentunder="false" is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo linebreak="badbreak" is="true">=</mo><mn is="true">0</mn></mrow><mrow is="true"><mn is="true">3</mn></mrow></munderover><mi is="true">B</mi><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mrow is="true"><mo stretchy="false" is="true">(</mo><mrow is="true"><mi is="true">x</mi></mrow><mo stretchy="false" is="true">)</mo></mrow><mo is="true">&gt;</mo><mo linebreak="goodbreak" is="true">=</mo><mn is="true">1</mn></math>
We formalize this algorithm in Algorithm 3.Algorithm 3. Multi-space Projection algorithm.1: Pre-projection and get several training sets.2: Train BCs for classify using different training sets.3: Aggression the results of BCs.4: Final judge.
This experiment examines two data sets. One is the dataset collected especially for web attack in Hangdian Security Lab, which contains both the normal and abnormal http streams, named Hduxss_data1.0. The other is the NSL-KDD, which is considered to be the benchmark evaluation data set in the field of intrusion detection. The experiment is performed on Pytorch 1.0 using a computer with GPU 2080ti, the operating system is Ubuntu 18.04, and the memory is 32G.
The Hduxss_data1.0 data set consists of three types of data:
Attack samples generated by the SQLMAP(SQLMAP is an open source penetra-tion testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers). The total number of this type is 810,000.
Attack samples manually and automatically generated by the XSSLESS tool; the total number of this type is 11,000.
Normal request samples collected through Firefox browser when browses various web pages. We extract parameters from them and obtains 130,000 normal samples.
For normal samples and abnormal samples, we randomly select 50% of the data for training, 50% of the data for testing, and repeat 10 rounds of testing for the average. The results are shown in Table 3. In the test process, all data is processed by SRDLM Algorithm 1, and the outputs are reclassified by support vector machine, naive Bayes, and SRDLM Algorithm 3. It can be seen from Table 3 that all the machine learning methods involved in the test achieved an accuracy of more than 99% and F1 values, which is mainly because Algorithm 1 has a good feature separation effect on the above attack traffic. At the same time, the SRDLM algorithm achieved the best recognition performance compared with the classical machine learning algorithm. The SRDLM Algorithm 1 performs a round of semantic re-encoding on HTTP data streams, highlighting the special semantics of network data, and increasing the distinguishability of data streams. In the future, no matter which classifier like SVM, Bayes or SRDLM is used, good detection performance can be achieved.
The algorithm stability test results are shown in Fig. 10. It can be shown that the algorithm requires less data, even in the case of only one thousandth of the data is trained, and it still get high classification accuracy (>98%). With the increase of the training set, the accuracy rate is gradually close to 1. Tests show that the SRDLM algorithm has good stability.
Each record of NSL-KDD dataset is a vector that contains 41 features and a label which marks the types: normal or attack. The attack types include four categories: DoS, Probe, U2R, R2L. We use KDDTrain+ and KDDTest+ in our experiment. The data distribution is shown in Table 4.
ResNet shows a good generation ability through experiment. We train kNN, Shallow CNN and ResNet models on KDDTrain + to classify the normal and attack traffic in KDDTest+. For the convenience of transformation, we remove the 7th dimension feature rarely used in the 41 dimension features. The 7th dimension feature has zero values in almost all samples in the training set. The remaining 40 features are transformed into a 5 ∗ 8 matrix, which can be transformed into the input matrix of a CNN model. The experiment result is shown in Table 5. The table shows the correct predicted number of samples and the proportion of this number in the test set of the same type with samples. The error rate of the R2L type is the highest among the 5 types. This is mainly because the R2L types in KDDTrain + and KDDTest + are very different, and the classifier is difficult to generalize. As can be seen from Table 5, the ResNet network has better average generalization ability for various types of positive and negative samples. Then, the generation ability of the three models on R2L type is analyzed as follows.
We use Grad-CAM (Selvaraju et al., 2017) to locate important regions with class discrimination in the input sample. As shown in Fig. 11, the result of Grad-CAM is a heat map. The red area indicates that the pixels of this part contribute the most to the classification result. As can be seen from Fig. 11, features 34, 35, and 36 contribute the most to the determination of the sample type. Then, the performance on R2L of classifiers through feature 34, 35, 36 was studied. The result is shown in Fig. 12. In Fig. 12, the x-axis, y-axis, and z-axis are respectively features 34,35, and 36 named ‘dst_host_same_srv_rate’, ‘dst_host_diff_srv_rate’ and ‘dst_host_same_src_port_rate’. There are three types of dots in the figure. The green dots and the orange dots mean the test samples of R2L type wrongly predicted and correctly predicted separately by the models we trained. And the red dots mean the train samples of R2L type. The abbreviated name of the three types of dots in Fig. 12 are ‘R2L_test_F’, ‘R2L_test_T’ and ‘R2L_train’. Obviously, the orange dots in Fig. 12(c) are much larger than those in Fig. 12(a) (b). From Fig. 12(c), it can be seen that the trained ResNet model learns some of the attack traffic when the feature 34 is between (0–0.6), the feature 35 is between (0–1) and the feature 36 is between (0–0.7). While in the kNN and Shallow CNN, from Fig. 12(a) and (b), it can hardly be seen that the correct predicted sample. We then created new 5000 samples that the feature 34, 35, 36 were in the interval of (0–0.6), (0–1), (0–0.7) separately, while the other features were the same as one of R2L samples in the training set. From Fig. 13, 67% of the samples were predicted right through the ResNet model. The same test works on KNN and shallow CNN networks, 7% of the samples were predicted right for kNN model, while all the created 5000 samples were predicted as the opposite type on the Shallow CNN model. Based on the present experiments, we infer that the kNN model and the Shallow CNN model does not learn the mapping of feature 34, 35, 36 to the classification of the result. While the ResNet model is successfully generated from the training sample to the near region. But there are still regions that could not be generated well, which means the ResNet model still has some limitation on generation. Comparing with the other two models, ResNet model has better generalization ability.
Based on the theoretical analysis of the CNN, it shows that the depth of the network matters much on the classification accuracy and the training time. Thus, we design an experiment to choose an appropriate depth.
For the ResNet, we change the numbers of filters to 2, 4, 8 respectively. The network started with a 3∗3 convolution layer, and ended with a global average pooling, a 2-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. Table 6 summarizes the architecture: The mix of KDDTrain+ and KDDTest + are used as the training set and the KDDTest+ is the testing set. We compare 8, 20, 56-layer networks. From Table 7, it can be seen that with the deepening of network depth, there is hardly an improvement in accuracy. It can be speculated that the depth of the 8-layer ResNet network is enough, and the remaining data that could not be accurately identified is the data that the training set does not know at all. Deeper network means more time. As a result, we chose 8-layer ResNet in our following experiment.
Three kinds of dimensions are employed to test the performance of the feature re-encoding algorithm. They are 40∗4, 172∗4,194∗4. For the 40∗4 dimension, we remove the unusable feature 7 from the training and test set, and then copy 3 times to form a 40∗4 dimension. For the 172∗4 and 194∗4 dimension, Algorithm 2 is used to cut the two dimensions to 676 dimensions, which could be transformed into 26∗26.
In this section, we use 40∗4,172∗4 and 194∗4 dimension datasets to evaluate the re-encoding impact on the binary classification of NSL-KDD.
First, KDDTrain + is used as the training set, and KDDTest + is used as the test set. We set the batchsize of the training set to 16384. And the learning rate is 0.01, the optimization method is Adam, the weight decay is 0.1, and the number of epoch is 10000. It can be seen from Fig. 14 that the accuracy of 172∗4 and 194∗4 is better than 40∗4 dimension. This indicates the effectiveness of semantic re-encoding.
There seems still some false alarm and missing alarm. In order to promote the ability of generalization, we create some predicted data according to the feature of intrusion traffic, and put them into the training set. And we use this new training dataset to train. The hyper-parameters are the same as the previous one. As is shown in Fig. 14, the semantically recoded data set performed better than the original data set.
We want to find why there are samples that cannot be classified right. So we mixed the KDDTrain+ with KDDTest+, and split it in to 10 folds with the same distribution of different types of network traffic. Each time, 9 of them are used as the training set and 1 is used as the test set. After 10 times training and testing, we get the average accuracy. From Fig. 14, it can be seen that the cross-validated detection rate raises obviously than the other two, and the re-encoded dataset is a little better than the 40∗4 dimension. The reason can be that the samples in KDDTrain + are not very typical. Some sample types of KDDTest + do not appear at all in KDDTrain+, and the attack samples themselves are significantly different, which makes it difficult to predict KDDTest + based on KDDTrain+.
We also compare with big convolution in the first layer, through trying to set the first convolution layer size to 7∗7, and contrast with the 3∗3 network. The Algorithm 3 is used. Table 8 shows the result of BCs, the 40∗4_3 means the 40∗4 dimension with the first convolution of 3∗3, and the 40∗4_7 means the 40∗4 dimension with the first convolution of 7∗7. Referring to Fig. 8, the DoS dataset is separated into 3 parts. In the process of experiment, it is found that although a single DoS is easy to be classified, the effect of the combination of the BCs of DoS and other Attacks is not good. For example, when using accuracy as the classifier design criteria, the results of DoS classification have little impact on the final classification results after merging the results of all BCs. Through the experiment, it is found that using the precision standard to design DoS classifier is the most helpful to the result of multi BCs fusion. Therefore, in the experimental process, the precision standard is used for the design of DoS classifier, and the rest of the classifier design use the accuracy standard. The precisions of the DoS classifiers are shown in Table 8. As shown in Fig. 15, the network whose first convolution is 7∗7 convolution outperform the 3∗3 size, and the best accuracy of this experiment is 94.03%.
The performance comparison between different classification algorithms is shown in Table 9 and Table 10. From Table 9, the experimental results show that SRDLM improves the performance by more than 8% compared with other classifiers. While comparing the ANN with the SRDLM, the classification result is unsatisfactory. It can be inferred that the semantic re-encoding and ResNet improved the gap between the attack and the normal samples, and the deep learning algorithm has the ability to fit nonlinear operations. Comparing shallow CNN with SRDLM, the advantage of semantic re-encoding and the generation ability of ResNet are shown obviously. In addition, as the ResNet has the strength of CNN, it runs less time than RNN-IDS. Compared with several current algorithms, the SDRLM algorithm can effectively learn the latent feature of attack sample of each types attack and make the re-encoded semantic space boundaries clearing, thus leading a good accuracy. Table 10 shows the computational complexity of related typical algorithms. As can be seen in Table 10, the complexity of kNN is O(m∗n). When classifying test samples, the algorithm complexity of kNN is positively related to the size of existing samples, while CNN and SRDLM algorithms are not limited by the size of existing sample sets. The complexity of the rest algorithms in Table 9 has nothing to do with the number of training set, but they are related to the size of the learning model. The complexity of CNN is O(n∗r∗c∗l), and the SRDLM is just k times more than the CNN model because of the k BCs computing. The r, c, k, and l values of the SRDLM algorithm are all <10. By introducing GPU parallel computing, the algorithm's running time can be reduced to an acceptable level.
This paper proposes an SRDLM intrusion detection method based on semantic re-encoding and deep learning. The SRDLM algorithm has advantages in dealing with anomaly detection of network traffic with huge semantic coding space and negligible word order. However, for the network traffic that has been extracted features, semantic re-encoding technology has limited performance improvement in traffic detection. Semantic re-encoding technology can be combined with deep learning technology to achieve better network traffic detection results. This paper studies the ResNet network architecture and combines ResNet with semantic re-encoding to effectively improve the generalization ability of the network anomaly detection model. The follow-up work will study the prediction of network abnormal traffic to enhance the robustness of the network detection model.
Zhendong Wu: Methodology, Writing - original draft, Writing - review & editing, Software, Resources. Jingjing Wang: Software, Writing - original draft. Liqin Hu: Investigation, Formal analysis. Zhang Zhang: Validation. Han Wu: Validation.
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
This research is supported by National Natural Science Foundation of China (No.61772162), Key Projects of NSFC Joint Fund of China (No.U1866209), National Natural Science Foundation of China (No.61602144), National Key R&D Program of China (No.2018YFB0804102).