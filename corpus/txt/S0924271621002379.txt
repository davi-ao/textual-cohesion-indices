Driven by the rapid development of Earth observation technology, massive numbers of remotely sensed images at fine spatial resolution are commercially available for a variety of applications, such as image classification (Lyons et al., 2018, Maggiori et al., 2016), object detection (Li et al., 2017; Xia et al., 2018) and semantic segmentation (Kemker et al., 2018, Zhang et al., 2019a). The re-visit capabilities of orbital sensors facilitate continuous monitoring of the land surface, ocean, and atmosphere (Duan and Li, 2020). Fine-resolution remotely sensed images are rich in information and contain substantial spatial detail for land cover and land use classification and segmentation. Different automatic and semi-automatic methods have been developed to identify land cover and land use categories by exploiting spectral and spectral-spatial features within remote sensing images (Gong et al., 1992, Ma et al., 2017, Tucker, 1979, Zhong et al., 2014, Zhu et al., 2017). However, these traditional approaches rely on handcrafting features and information transformation, which commonly fail to adequately capture the contextual information contained abundantly within images, and are often limited in their flexibility and general adaptability (Li et al., 2020, Tong et al., 2020). This is especially true given the detailed structural and contextual information provided at a very fine spatial resolution. Meanwhile, recent developments in deep learning, and deep convolutional neural network (CNN), in particular, have replaced feature engineering with high-level non-linear feature representations created end-to-end, hierarchically, and in an automatic fashion. This has had a transformative impact on information understanding and semantic characterization from fine-resolution remotely sensed imagery (Li et al., 2021b, Zheng et al., 2020).
Semantic segmentation, which assigns each pixel in an image to a particular category, has become one of the most important approaches for ground feature interpretation, playing a pivotal role in different application scenarios (Wang et al., 2021), such as precision agriculture (Griffiths et al., 2019, Picoli et al., 2018), environmental protection (Samie et al., 2020, Yin et al., 2018) and economic assessment (Zhang et al., 2020, Zhang et al., 2019a). The fully convolutional network (FCN) was demonstrated to be the first effective end-to-end CNN structure for semantic segmentation (Long et al., 2015). Restricted by the oversimplified design of the decoder, the results of FCN, although encouraging in principle, are presented at a coarse resolution. Subsequently, more elaborate encoder-decoder structures, such as U-Net, have been proposed, with two symmetric paths: a contracting path for extracting features and an expanding path for achieving accurate results through precise positioning (Badrinarayanan et al., 2017, Li et al., 2021a, Ronneberger et al., 2015). The per-pixel classification is often ambiguous in the presence of only local information for semantic segmentation, while the task becomes much simpler if global contextual information, from the whole image, is available (as shown in Fig. 1). Therefore, to guarantee the accuracy of segmentation, global contextual information and multiscale semantic features were utilized comprehensively to differentiate semantic categories at different spatial scales. Through the spatial pyramid pooling module, the pyramid scene parsing network (PSPNet) aggregated contextual information across different regions (Zhao et al., 2017). The dual attention network (DANet) applied the dot-product attention mechanism to extract abundant contextual relationships (Fu et al., 2019). Subject to an enormous memory and computational demand, DANet simply attached the dot-product attention mechanism at the lowest layer without capturing the long-range dependencies from the larger feature maps in the higher layers. DeeplabV3 adopted atrous convolution to mine the multiscale features (Chen et al., 2017a) and a simple, yet useful, decoder module was added in DeepLabV3 + to further refine the segmentation results (Chen et al., 2018a).
The extraction of global contextual information and the exploitation of large-scale feature maps are computationally expensive (Chen et al., 2017b, Diakogiannis et al., 2020b, Li et al., 2021b). Therefore, a series of lightweight networks have been developed to accelerate the computation while maintaining the trade-off between accuracy and efficiency (Hu et al., 2020; Oršić and Šegvić, 2021; Romera et al., 2017; Yu et al., 2018, Zhuang et al., 2019). For example, the asymmetric convolution used in ERFNet factorized the standard 3 × 3 convolutions into a 1 × 3 convolution and a 3 × 1 convolution, saving approximately 33% of the computational cost (Romera et al., 2017). By exploiting spatial correlations and cross-channel correlations, respectively, BiseNet achieved depth-wise separable convolution (Yu et al., 2018), which further reduced the consumption of standard convolution (Chollet, 2017). Multi-scale encoder-decoder branch pairs with skip connections were studied in ShelfNet (Zhuang et al., 2019), where a shared-weight strategy was harnessed in the residual block to reduce the number of parameters without sacrificing accuracy. For non-local context aggregation, FANet employed the fast attention module in efficient semantic segmentation (Hu et al., 2020). SwiftNet explored the effectiveness of pyramidal fusion in compact architectures (Oršić and Šegvić, 2021). However, the CNN is designed to extract local patterns and lacks the ability to model global context in its nature. More severely, as lightweight networks normally adopted relatively shallow backbones, the capacity of those networks to extract global contextual information is further limited.
Due to the limited capacity of lightweight networks to extract global contextual information, there is a huge gap in accuracy between lightweight networks and state-of-the-art deep models, which limits their applicability to fine-resolution remotely sensed images. The dot-product attention mechanism, as a powerful approach that can capture long-range dependencies, is potentially an ideal solution to address this issue (Vaswani et al., 2017). However, the memory and computational costs of the dot-product attention mechanism increase quadratically with an increase in the spatio-temporal size of the input, which runs counter to the aim of lightweight networks. Encouragingly, previous researches on linear attention (Katharopoulos et al., 2020, Li et al., 2021b) reduce the complexity of the dot-product attention mechanism from O(N2)<math><mrow is="true"><mi is="true">O</mi><mo stretchy="true" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mn is="true">2</mn></msup><mo stretchy="true" is="true">)</mo></mrow></math> to O(N)<math><mrow is="true"><mi is="true">O</mi><mo stretchy="true" is="true">(</mo><mi is="true">N</mi><mo stretchy="true" is="true">)</mo></mrow></math>, with a significant increase in computational speed, while maintaining high accuracy.
In this paper, we aim to further increase segmentation accuracy while ensuring the efficiency of semantic segmentation simultaneously. We address this challenge by modeling the global contextual information using the linear attention mechanism. Specifically, we propose an Attentive Bilateral Contextual Network (ABCNet) to realize efficient semantic segmentation of fine-resolution remote sensing images. Following the design philosophy of BiSeNet (Yu et al., 2018), we design the ABCNet based on a bilateral architecture: a spatial path to retain the abundant spatial detail and a contextual path to capture the global contextual information. As the features generated by the two paths are quite disparate semantically, we further design a feature aggregation module (FAM) to fuse those features. The comparison between the conventional encoder-decoder structure and the bilateral architecture used in the proposed ABCNet can be seen in Fig. 2. The main contributions are two-fold. On the one hand, we propose a novel approach for efficient semantic segmentation of fine-resolution remotely sensed imagery, i.e., ABCNet with spatial and contextual paths. On the other hand, we design two specific modules: an attention enhancement module (AEM) for exploring long-range contextual information, and a feature aggregation module (FAM) for fusing the features obtained by the two paths. A thorough benchmark comparison was undertaken against the state-of-the-art to demonstrate the effectiveness of the proposed ABCNet.
Context is critically important for semantic segmentation and, thus, tremendous effort has been made to extract such information in an intelligent manner. The dilated or atrous convolution (Chen et al., 2014, Yu and Koltun, 2015) has been demonstrated to be an effective approach for enlarging receptive fields without shrinking spatial resolution. Besides, the encoder-decoder architecture (Ronneberger et al., 2015), which merges high-level and low-level features via skip connections, is an alternative for extracting spatial context. Based on the encoder-decoder framework or dilation backbone, some research has focused on exploring the use of spatial pyramid pooling (SPP) (He et al., 2015). For example, the pyramid pooling module (PPM) in PSPNet is composed of convolutions with kernels of four different sizes (Zhao et al., 2017), while DeepLab v2 (Chen et al., 2018a), equipped with the atrous spatial pyramid pooling (ASPP) module, groups parallel atrous convolution layers with varying dilation rates. However, certain limitations persist in SPP. Particularly, the SPP with the standard convolution faces a dilemma when expanding the receptive field with a large kernel size. The above operations are normally accompanied by a very large number of parameters. The SPP with small kernels (e.g. ASPP), on the other hand, lacks sufficient connection between adjacent features, and the gridding problem (Wang et al., 2018a) occurs when the field is enlarged by a dilated convolutional layer. In contrast, the dot-product attention mechanism has the powerful ability to model long-range dependencies, which enables contextual information extraction at a global scale.
Let H, W, and C<math><mi is="true">C</mi></math> denote the height, weight, and channels of the input, respectively. The input feature is defined as X=[x1,⋯,xN]∈RN×C<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">x</mi></mrow><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mo is="true">⋯</mo><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">x</mi></mrow></mrow><mi is="true">N</mi></msub><mo stretchy="true" is="true">]</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">C</mi></mrow></msup></mrow></math>, where N=H×W<math><mrow is="true"><mi is="true">N</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">H</mi><mo is="true">×</mo><mi is="true">W</mi></mrow></math>. Initially, the dot-product attention mechanism utilizes three projected matrices Wq∈RDx×Dk<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">W</mi></mrow><mi is="true">q</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mi is="true">D</mi><mi is="true">x</mi></msub><mo is="true">×</mo><msub is="true"><mi is="true">D</mi><mi is="true">k</mi></msub></mrow></msup></mrow></math>, Wk∈RDx×Dk<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">W</mi></mrow><mi is="true">k</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mi is="true">D</mi><mi is="true">x</mi></msub><msub is="true"><mrow is="true"><mo is="true">×</mo><mi is="true">D</mi></mrow><mi is="true">k</mi></msub></mrow></msup></mrow></math>, and Wv∈RDx×Dv<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">W</mi></mrow><mi is="true">v</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mi is="true">D</mi><mi is="true">x</mi></msub><msub is="true"><mrow is="true"><mo is="true">×</mo><mi is="true">D</mi></mrow><mi is="true">v</mi></msub></mrow></msup></mrow></math> to generate the corresponding query matrix Q, the key matrix K, and the value matrix V:(1)Q=XWq∈RN×Dk;K=XWk∈RN×Dk;V=XWv∈RN×Dv.<math><mfenced open="{" close="" is="true"><mrow is="true"><mrow is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mrow is="true"><mi is="true">Q</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">W</mi></mrow></mrow><mi is="true">q</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">N</mi><msub is="true"><mrow is="true"><mo is="true">×</mo><mi is="true">D</mi></mrow><mi is="true">k</mi></msub></mrow></msup><mo is="true">;</mo></mrow></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mi is="true">K</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">W</mi></mrow></mrow><mi is="true">k</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><msub is="true"><mi is="true">D</mi><mi is="true">k</mi></msub></mrow></msup><mo is="true">;</mo></mrow></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mi is="true">V</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">X</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">W</mi></mrow></mrow><mi is="true">v</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><msub is="true"><mi is="true">D</mi><mi is="true">v</mi></msub></mrow></msup><mo is="true">.</mo></mrow></mtd></mtr></mtable></mrow></mrow></mfenced></math>The graphical representation of the dot-product attention mechanism can be seen in Fig. 3. The dimensions of Q and K are identical, and all vectors in this section are column vectors by default. Accordingly, a normalization function ρ is employed to measure the similarity between the i-th query feature qiT∈RDk<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mi is="true">T</mi></msubsup><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><msub is="true"><mi is="true">D</mi><mi is="true">k</mi></msub></msup></mrow></math> and the j-th key feature kj∈RDk<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><msub is="true"><mi is="true">D</mi><mi is="true">k</mi></msub></msup></mrow></math> as ρ(qiT∙kj)∈R1<math><mrow is="true"><mi is="true">ρ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msubsup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mi is="true">T</mi></msubsup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mn is="true">1</mn></msup></mrow></math>. As the query feature and key feature are generated via different layers, the similarities between ρ(qiT∙kj)<math><mrow is="true"><mi is="true">ρ</mi><mo stretchy="true" is="true">(</mo><msubsup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mi is="true">T</mi></msubsup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></math> and ρ(qjT∙ki)<math><mrow is="true"><mi is="true">ρ</mi><mo stretchy="true" is="true">(</mo><msubsup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mi is="true">T</mi></msubsup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow></math> are not identical. Therefore, the N × N QKT<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></math> matrix model the long-range dependency between each pixel pair in the input feature maps, where the pixel at j-th row and i-th column measures the i-th position’s impact on j-th position. In other words, the long-range global contextual information between every pixel of the input can be fully modeled by the N × N matrix QKT<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></math>. By calculating similarities between all pairs of pixels in the input feature maps and taking the similarities as weights, the dot-product attention mechanism generates the value at position i by aggregating the value features from all positions using weighted summation:(2)DQ,K,V=ρQKTV.<math><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced><mo linebreak="goodbreak" is="true">=</mo><mi is="true">ρ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></mfenced><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow><mo is="true">.</mo></mrow></math>
Softmax is frequently used as the normalization function:(3)ρQKT=softmaxrowQKT,<math><mrow is="true"><mi is="true">ρ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></mfenced><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">softmax</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">row</mi></mrow></msub><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></mfenced><mo is="true">,</mo></mrow></math>
where softmaxrow<math><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">softmax</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">row</mi></mrow></msub></math> indicates that the softmax along each row of the matrix QKT<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></math>.
By modeling the similarities between each pair of positions of the input, the global dependencies in the features can be extracted thoroughly by ρQKT<math><mrow is="true"><mi is="true">ρ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></mrow></mfenced></mrow></math>. The dot-product attention mechanism was initially designed for machine translation (Vaswani et al., 2017), while the non-local module (Wang et al., 2018b) was introduced and modified for computer vision (Fig. 4). Based on the dot-product attention mechanism, as well as its variants, different attention-based networks have been proposed to address the semantic segmentation task. Inspired by the non-local module (Wang et al., 2018b), the double attention networks (A2<math><msup is="true"><mrow is="true"><mi is="true">A</mi></mrow><mn is="true">2</mn></msup></math>-Net) (Chen et al., 2018b), dual attention network (DANet) (Fu et al., 2019), and object context network (OCNet) (Yuan and Wang, 2018) were proposed successively for scene segmentation by exploring the long-range dependencies. Furthermore, Bello et al. (2019) augmented convolutional operators with attention mechanisms, while Zhang et al. (2019c) incorporated the attention mechanism into the generative adversarial network. Lu et al. (2019) extended the attention mechanism to CO-attention Siamese Network (COSNet) for unsupervised video object segmentation. Recently, Diakogiannis et al. (2020a) improved the attention mechanism and proposed the fractal Tanimoto attention layer for semantic change detection.
Although the introduction of attention boosts segmentation accuracy significantly, the huge resource-demand of the dot-product hinders its application to large inputs. Specifically, for Q∈RN×Dk<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><msub is="true"><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">D</mi></mrow><mi is="true">k</mi></msub></msup></mrow></math> and KT∈RDk×N<math><mrow is="true"><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><msub is="true"><mi is="true">D</mi><mi is="true">k</mi></msub><mo is="true">×</mo><mi is="true">N</mi></mrow></msup></mrow></math>, the product between Q<math><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow></math> and KT<math><msup is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow><mi is="true">T</mi></msup></math> belongs to RN×N<math><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">N</mi></mrow></msup></math>, leading to O(N2)<math><mrow is="true"><mi is="true">O</mi><mo stretchy="true" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mn is="true">2</mn></msup><mo stretchy="true" is="true">)</mo></mrow></math> memory and computational complexity. Consequently, it is necessary to reduce the demand for computational resources of the dot-product attention mechanism. Substantial endeavors have been poured in aiming to alleviate the bottleneck to efficiency and push the boundaries of attention, including accelerating the generation process of the attention matrix (Huang et al., 2019a, Huang et al., 2019b, Yuan et al., 2019, Zhang et al., 2019b), pruning the structure of the attention block (Cao et al., 2019), and optimizing attention based on low-rank reconstruction (Li et al., 2019c).
If the normalization function is set as softmax, the i-th row of the result matrix generated by the dot-product attention mechanism can be written as:(4)DQ,K,Vi=∑j=1NeqiT∙kjvj∑j=1NeqiT∙kj,<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></mrow><mi is="true">T</mi></msup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></msup><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mi is="true">j</mi></msub></mrow><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></mrow><mi is="true">T</mi></msup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></msup></mrow></mfrac><mo is="true">,</mo></mrow></math>
where vj<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mi is="true">j</mi></msub></math> is j-th value feature.
Eq. (4) can be rewritten and generalized to any normalization function as:(5)DQ,K,Vi=∑j=1Nsimqi,kjvj∑j=1Nsimqi,kj,simqi,kj≥0.<math><mrow is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mi is="true">i</mi></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mi mathvariant="normal" is="true">s</mi><mi mathvariant="normal" is="true">i</mi><mi mathvariant="normal" is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mi is="true">j</mi></msub></mrow><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mi mathvariant="normal" is="true">s</mi><mi mathvariant="normal" is="true">i</mi><mi mathvariant="normal" is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced></mrow></mfrac><mo is="true">,</mo></mrow></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mi mathvariant="normal" is="true">s</mi><mi mathvariant="normal" is="true">i</mi><mi mathvariant="normal" is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced><mo is="true">≥</mo><mn is="true">0</mn><mo is="true">.</mo></mrow></mtd></mtr></mtable></mrow></math>
simqi,kj<math><mrow is="true"><mi mathvariant="normal" is="true">s</mi><mi mathvariant="normal" is="true">i</mi><mi mathvariant="normal" is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced></mrow></math> can be expanded as ϕ(qi)Tφ(kj)<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">ϕ</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup><mi is="true">φ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> which measures the similarity between qi<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></math> and kj<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></math>, and Eq. (4) can be rewritten as Eq. (6) and simplified as Eq. (7):(6)DQ,K,Vi=∑j=1Nϕ(qi)Tφ(kj)vj∑j=1Nϕ(qi)Tφ(kj),<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msup is="true"><mrow is="true"><mi is="true">ϕ</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup><mi is="true">φ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mi is="true">j</mi></msub></mrow><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msup is="true"><mrow is="true"><mi is="true">ϕ</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup><mi is="true">φ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac><mo is="true">,</mo></mrow></math>(7)DQ,K,Vi=ϕ(qi)T∑j=1Nφ(kj)vjTϕ(qi)T∑j=1Nφ(kj).<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">ϕ</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mi is="true">φ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mi is="true">j</mi></msub></mrow><mi is="true">T</mi></msup></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">ϕ</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mi is="true">φ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></mfrac><mo is="true">.</mo></mrow></math>
Ifsimqi,kj=eqiT∙kj<math><mrow is="true"><mi mathvariant="normal" is="true">f</mi><mi mathvariant="normal" is="true">s</mi><mi mathvariant="normal" is="true">i</mi><mi mathvariant="normal" is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></mrow><mi is="true">T</mi></msup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></msup></mrow></math>, Eq. (5) is equivalent to Eq. (4). The vectorized form of Eq. (7) is:(8)DQ,K,V=ϕQφKTVϕQ∑jφKi,jT<math><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">ϕ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow></mrow></mfenced><msup is="true"><mrow is="true"><mi is="true">φ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow></mfenced></mrow><mi is="true">T</mi></msup><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow><mrow is="true"><mi is="true">ϕ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow></mrow></mfenced><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><msubsup is="true"><mrow is="true"><mi is="true">φ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow></mfenced></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mi is="true">T</mi></msubsup></mrow></mfrac></mrow></math>
As the softmax function is substituted for simqi,kj=ϕ(qi)Tφ(kj)<math><mrow is="true"><mi mathvariant="normal" is="true">s</mi><mi mathvariant="normal" is="true">i</mi><mi mathvariant="normal" is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msup is="true"><mrow is="true"><mi is="true">ϕ</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup><mi is="true">φ</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>, the order of the commutative operation can be altered, thereby avoiding multiplication between the reshaped key matrix K and query matrix Q. In concrete terms, we can first compute the multiplication between φKT<math><msup is="true"><mrow is="true"><mi is="true">φ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow></mrow></mfenced></mrow><mi is="true">T</mi></msup></math> and V, and then multiply the result with Q, leading to only O(dN)<math><mrow is="true"><mi is="true">O</mi><mo stretchy="true" is="true">(</mo><mi is="true">d</mi><mi is="true">N</mi><mo stretchy="true" is="true">)</mo></mrow></math> time complexity and O(dN)<math><mrow is="true"><mi is="true">O</mi><mo stretchy="true" is="true">(</mo><mi is="true">d</mi><mi is="true">N</mi><mo stretchy="true" is="true">)</mo></mrow></math> space complexity. The suitable ϕ∙<math><mrow is="true"><mi is="true">ϕ</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mo is="true">∙</mo></mrow></mfenced></mrow></math> and φ(∙)<math><mrow is="true"><mi is="true">φ</mi><mo stretchy="true" is="true">(</mo><mo is="true">∙</mo><mo stretchy="true" is="true">)</mo></mrow></math> enable the above scheme to achieve competitive performance with finite computational complexity (Katharopoulos et al., 2020, Li et al., 2021c).
In our previous research (Li et al., 2021b), we proposed a linear attention mechanism to replace the softmax function with the first-order approximation of the Taylor expansion, as in Eq. (9):(9)eqiT∙kj≈1+qiT∙kj<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></mrow><mi is="true">T</mi></msup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></msup><mo is="true">≈</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">+</mo><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></mrow><mi is="true">T</mi></msup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></math>
To guarantee the above approximation to be nonnegative, qi<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></math> and kj<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></math> are normalized by the l2<math><msub is="true"><mi is="true">l</mi><mn is="true">2</mn></msub></math> norm, thereby ensuring qiT∙kj≥-1<math><mrow is="true"><msup is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub></mrow><mi is="true">T</mi></msup><mo is="true">∙</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mo is="true">≥</mo><mo linebreak="badbreak" linebreakstyle="after" is="true">-</mo><mn is="true">1</mn></mrow></math>:(10)simqi,kj=1+qi‖qi‖2Tkj‖kj‖2<math><mrow is="true"><mi is="true">s</mi><mi is="true">i</mi><mi is="true">m</mi><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub></mrow></mfenced><mo linebreak="goodbreak" is="true">=</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">+</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow><mi is="true">T</mi></msup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow></math>
Thus, Eq. (5) can be rewritten as Eq. (11) and simplified as Eq. (12):(11)DQ,K,Vi=∑j=1N1+qi‖qi‖2Tkj‖kj‖2vj∑j=1N1+qi‖qi‖2Tkj‖kj‖2,<math><mrow is="true"><msub is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">D</mi></mrow><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">i</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">N</mi></mrow></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">+</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">i</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">T</mi></mrow></msup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow></mfenced><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow></msub></mrow><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">N</mi></mrow></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">+</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">i</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">T</mi></mrow></msup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mrow is="true"><mi mathvariant="bold-italic" is="true">j</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow></mfenced></mrow></mfrac><mo is="true">,</mo></mrow></math>(12)DQ,K,Vi=∑j=1Nvj+qi‖qi‖2T∑j=1Nkj‖kj‖2vjTN+qi‖qi‖2T∑j=1Nkj‖kj‖2.<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mi is="true">j</mi></msub><mo is="true">+</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow><mi is="true">T</mi></msup><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced><msubsup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mi is="true">T</mi></msubsup></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">+</mo><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">q</mi></mrow><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow><mi is="true">T</mi></msup><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow></mfrac><mo is="true">.</mo></mrow></math>
Eq. (12) can be turned into a vectorized form:(13)DQ,K,V=∑jVi,j+Q‖Q‖2K‖K‖2TVN+Q‖Q‖2∑jK‖K‖2i,jT<math><mrow is="true"><mi is="true">D</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo is="true">,</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msub is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo stretchy="true" is="true">‖</mo></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced><mfenced open="(" close=")" is="true"><mrow is="true"><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><msub is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo stretchy="true" is="true">‖</mo></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow><mi is="true">T</mi></msup><mrow is="true"><mi mathvariant="bold-italic" is="true">V</mi></mrow></mrow></mfenced></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">+</mo><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><msub is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">Q</mi></mrow><mo stretchy="true" is="true">‖</mo></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><msubsup is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><msub is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">K</mi></mrow><mo stretchy="true" is="true">‖</mo></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi></mrow><mi is="true">T</mi></msubsup></mrow></mfrac></mrow></math>
Since ∑j=1Nkj‖kj‖2vjT<math><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced><msubsup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">v</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mi is="true">T</mi></msubsup></mrow></math> and ∑j=1Nkj‖kj‖2<math><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><mfenced open="(" close=")" is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><msub is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi></mrow><mi is="true">j</mi></msub><mrow is="true"><mo stretchy="true" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msub></mfrac></mrow></mfenced></mrow></math> can be calculated and reused for each query, the time and memory complexity of the attention based on Eq. (13) is OdN<math><mrow is="true"><mi is="true">O</mi><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">d</mi><mi is="true">N</mi></mrow></mfenced></mrow></math>. For more detailed information on the proposed attention mechanism, as well as its validity and efficiency, the reader is referred to (Li et al., 2021b).
Besides dot-product attention, there exists another genre of techniques referred to as attention mechanisms in the literature. To distinguish it from the dot-product attention mechanism, we call them scaling attention. Unlike dot-product attention which models global dependencies from feature maps, scaling attention reinforces informative features and whittles information-lacking features. For example, Wang et al. (2017) proposed a residual attention network (RAN) which introduces the scaling attention mechanism inserted into deep residual networks. As a high-capacity structure, the residual attention is mainly built on max-pooling layers, convolutional layers, and residual units. In contrast, Hu et al. (2018) presented the squeeze-and-excitation (SE) module, a lightweight gating mechanism constructed on the global average pooling layer and linear layers, to calculate a scaling factor for each channel, thereby weighting the channels accordingly. The convolutional block attention module (CBAM) (Woo et al., 2018), selective kernel unit (SK unit) (Li et al., 2019b) and efficient channel attention module (ECA) (Wang et al., 2020) further boost the SE block’s performance. Despite both names containing attention, the principles and purposes of dot-product attention and scaling attention are entirely divergent.
For many practical applications, efficiency is critical, and this is especially pertinent for real-time (≥30FPS) scenarios such as autonomous driving. Therefore, huge efforts have been made to accelerate models for efficient semantic segmentation, by employing lightweight operations or down-sampling the input size. The utilization of lightweight convolutions (e.g., asymmetric convolution and depth-wise separable convolution) is a common strategy for designing lightweight networks (Romera et al., 2017; Yu et al., 2018). The down-sampling of the input size is a trivial solution to speed up semantic segmentation by reducing the resolution of the input images, which inevitably results in the loss of information. To extract spatial details at the original resolution, some of the latest methods include a further shallow branch, forming a two-path architecture (Yu et al., 2020, Yu et al., 2018).
The proposed Attentive Bilateral Contextual Network (ABCNet), as well as the components, are demonstrated in Fig. 5.
It is very challenging to reconcile the requirement for spatial detail with a large receptive field simultaneously. However, both of them are crucial to achieving high segmentation accuracy. Especially, for efficient semantic segmentation, mainstream solutions focus on down-sampling of the input image or speeding up the network by channel pruning. The former loses the majority of the spatial detail, whereas the latter can change its character deleteriously. By contrast, in the proposed ABCNet, we adopt a bilateral architecture (Yu et al., 2018), which is equipped with a spatial path to capture spatial details and generate low-level feature maps. Therefore, a rich channel capacity is essential for this path to encode sufficient spatial detailed information. Meanwhile, since the spatial path focuses merely on low-level details, a shallow structure with a small stride is sufficient for this branch. Specifically, the spatial path is comprised of three layers as shown in Fig. 5(a). The kernel size, channel number, stride and padding for each layer is [7, 64, 2, 3], [3, 64, 2, 1], and [3, 64, 2, 1], respectively. Each layer is followed by batch normalization (Ioffe and Szegedy, 2015) and ReLU (Glorot et al., 2011). Therefore, the output feature maps of this path are 1/8 of the original image, which encodes abundant spatial details resulting from the large spatial size.
In parallel to the spatial path, the contextual path is designed to provide a sufficient receptive field, thereby extracting global high-level contextual information. For segmentation, as the receptive field determines the richness of context, several recent approaches attempt to address the issue by taking advantage of the spatial pyramid pooling. However, huge computational demand and memory consumption will be brought when expanding the receptive field by a large kernel size. Instead, we develop the contextual path with the linear attention mechanism (Li et al., 2021b), which considers the long-range contextual information and efficient computation simultaneously.
In the contextual path as shown in Fig. 5(a), we harness the lightweight backbone (i.e., ResNet-18) (He et al., 2016) to down-sample the feature map and encode the high-level semantic information. We deploy two attention enhancement modules (AEM) on the last two layers of the backbone to fully extract the global contextual information. Besides, a global average pooling operation is attached to the tail of the contextual path to extract the contextual information, while the obtained features are added with the enhanced features generated by AEM2. Thereafter, the acquired features are upsampled by scale = 2 to restore the shape. Finally, the features obtained by the AEM1 and AEM2 are added and then fed into the feature aggregation module (FAM).
The feature representations of the spatial path and the contextual path are complementary, but provided in different domains (i.e., the spatial path generates the low-level and detailed features, while the contextual path provides the high-level and semantic features). Specifically, the output feature captured by the spatial path encodes mainly rich detail information, while the information generated by the contextual path mostly encodes contextual information. Thus, even though summation and concatenation can merge those features (Poudel et al., 2019), these simple fusion schemes are less effective to fuse information in diverse domains (Yang et al., 2021). Here, we design a feature aggregation module (FAM) to merge both types of feature representation in consideration of the need for high accuracy and efficiency.
As shown in Fig. 5(b), with two domains of features, we first concatenate the output of the spatial and contextual paths. Thereafter, a convolutional layer with batch normalization (Ioffe and Szegedy, 2015) and ReLU (Glorot et al., 2011) is attached to balance the scales of the features. Then, we capture the long-range dependencies of the generated features using the linear attention mechanism, thereby weighing the features selectively. Finally, the weighted features are multiplied and added with the balanced features. As both the scales and contributions of features are readjusted adaptively, the outputs of spatial and contextual paths can be fused effectively.
As shown in Fig. 5(a), besides the principal loss function used to supervise the output of the entire network, we utilize two auxiliary loss functions along the contextual path to accelerate the convergence velocity. We select the cross-entropy loss as the principal loss:(14)LCE=-1N∑n=1N∑k=1Kyk(n)logy^k(n)<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">L</mi><mrow is="true"><mi mathvariant="italic" is="true">CE</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mo linebreak="badbreak" is="true">-</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">N</mi></mfrac><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">K</mi></msubsup><msubsup is="true"><mi is="true">y</mi><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msubsup><mi mathvariant="normal" is="true">log</mi><msubsup is="true"><mover accent="true" is="true"><mi is="true">y</mi><mo stretchy="true" is="true">^</mo></mover><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msubsup></mrow></math>
where N and K are the number of samples and number of classes, respectively. y(n)<math><msup is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msup></math> and y^(n)<math><msup is="true"><mrow is="true"><mover accent="true" is="true"><mi is="true">y</mi><mo stretchy="true" is="true">^</mo></mover></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msup></math> with n∈[1,⋯,N]<math><mrow is="true"><mi mathvariant="normal" is="true">n</mi><mo is="true">∈</mo><mo stretchy="true" is="true">[</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><mi is="true">N</mi><mo stretchy="true" is="true">]</mo></mrow></math> are one-hot vectors of the true labels and the corresponding softmax output from the network. Essentially, y^k(n)<math><msubsup is="true"><mover accent="true" is="true"><mi is="true">y</mi><mo stretchy="true" is="true">^</mo></mover><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msubsup></math> depicts the network’s confidence of sample n being classified as k. The auxiliary loss functions are chosen as the focal loss:(15)LFocal=-1N∑n=1N∑k=1K1-y^k(n)γyk(n)logy^k(n)<math><mrow is="true"><msub is="true"><mi mathvariant="normal" is="true">L</mi><mrow is="true"><mi mathvariant="italic" is="true">Focal</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mo linebreak="badbreak" is="true">-</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">N</mi></mfrac><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">n</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></msubsup><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">K</mi></msubsup><msup is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><msubsup is="true"><mover accent="true" is="true"><mi is="true">y</mi><mo stretchy="true" is="true">^</mo></mover><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msubsup></mrow></mfenced></mrow><mi is="true">γ</mi></msup><msubsup is="true"><mi is="true">y</mi><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msubsup><mi mathvariant="normal" is="true">log</mi><msubsup is="true"><mover accent="true" is="true"><mi is="true">y</mi><mo stretchy="true" is="true">^</mo></mover><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></msubsup></mrow></math>
where γ is the focusing parameter, which controls the down-weighting of the easily classified examples, parameterized as 2 in the experiments. Hence, the overall loss of the network is:(16)L=LCE+LFocalaux1+LFocalaux2<math><mrow is="true"><mi mathvariant="normal" is="true">L</mi><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mi mathvariant="normal" is="true">L</mi><mrow is="true"><mi mathvariant="italic" is="true">CE</mi></mrow></msub><mo linebreak="badbreak" is="true">+</mo><msubsup is="true"><mi mathvariant="normal" is="true">L</mi><mrow is="true"><mi mathvariant="italic" is="true">Focal</mi></mrow><mrow is="true"><mi is="true">a</mi><mi is="true">u</mi><mi is="true">x</mi><mn is="true">1</mn></mrow></msubsup><mo linebreak="badbreak" is="true">+</mo><msubsup is="true"><mi mathvariant="normal" is="true">L</mi><mrow is="true"><mi mathvariant="italic" is="true">Focal</mi></mrow><mrow is="true"><mi is="true">a</mi><mi is="true">u</mi><mi is="true">x</mi><mn is="true">2</mn></mrow></msubsup></mrow></math>
There are four main parts in our proposed ABCNet, i.e., the contextual path, the spatial path, the attention enhancement module (AEM), and the feature aggregation module (FAM). Hence, there are mainly five variants of our ABCNet.
Baseline: The baseline (denoted as Cp) can be constructed based on the contextual path without AEM and FAM, while the backbone is set as ResNet-18. The baseline can be utilized as the benchmark to evaluate the effectiveness of components in the network.
Cp + AEM: In the contextual path, the attention enhancement module is designed to capture global contextual information. Hence, a simple variant is a contextual path with attention enhancement modules. The performance of Cp + AEM compared with the baseline will illustrate the effectiveness of the attention enhancement module.
Cp + Sp + AEM (Sum) and Cp + Sp + AEM (Cat): As abundant spatial information is crucial for semantic segmentation, the spatial path is designed to provide a relatively large spatial size and extract spatial information. Two simple fusion schemes including summation (Sum) and concatenation (Cat) can be utilized to merge features. The effectiveness of the spatial path can be validated by merging the spatial information into the network.
Cp + Sp + AEM + FAM: Given that the features obtained by the spatial and contextual paths are in different domains, neither summation nor concatenation provides the optimal fusion scheme. The full version of the proposed ABCNet is fusing the contextual information and spatial information by the feature aggregation module. By comparing the accuracy with Cp + Sp + AEM (Sum) and Cp + Sp + AEM (Cat), the superiority of the feature aggregation module will be demonstrated.

Datasets
The effectiveness of the proposed ABCNet was tested using the ISPRS Vaihingen dataset and the ISPRS Potsdam dataset (http://www2.isprs.org/commissions/comm3/wg4/semantic-label-ing.html). There are two types of ground truth provided in the ISPRS datasets: with and without eroded boundaries. We conducted all experiments on the ground truth with eroded boundaries.
Vaihingen: The Vaihingen dataset contains 33 images with an average size of 2494 × 2064 pixels and a ground sampling distance (GSD) of 9 cm. The near-infrared, red, and green channels together with corresponding digital surface models (DSMs) and normalized DSMs (NDSMs) are provided in the dataset. We utilized ID: 2, 4, 6, 8, 10, 12, 14, 16, 20, 22, 24, 27, 29, 31, 33, 35, 38 for testing, ID: 30 for validation, and the remaining 15 images for training. The DSMs were not used in the experiments. The reference data are labeled according to six land-cover types: impervious surfaces, building, low vegetation, tree, car, and clutter/background.
Potsdam: There exist 38 fine-resolution images of size 6000 × 6000 pixels with a GSD of 5 cm in the Potsdam dataset. The dataset provides the near-infrared, red, green, and blue channels as well as DSMs and NDSMs. We utilized ID: 2_13, 2_14, 3_13, 3_14, 4_13, 4_14, 4_15, 5_13, 5_14, 5_15, 6_13, 6_14, 6_15, 7_13 for testing, ID: 2_10 for validation, and the remaining 22 images, except for image named 7_10 with error annotations, for training. We employed only the red, green, and blue channels in the experiments. The reference data are divided into the same six categories as the Vaihingen data set.
Training and testing setting
All the training processes were implemented with PyTorch on a single Tesla V100 with 32 batch size, and the optimizer was set as AdamW with a learning rate of 0.0003 and a weight decay value of 0.0025. For the learning rate scheduler, we adopted available ReduceLROnPlateau in PyTorch with the patience of 5 and the learning rate decrease factor as 0.5. If OA on the validation set does not increase for more than 10 epochs, the training procedure will be stopped, while the maximum iteration period is 1000 epochs. For training, we cropped the raw images as well as corresponding labels into 512 × 512 patches and augmented them via rotating on a random angle (90°, 180°, or 270°), resizing by a random scale (from 0.5 to 2.0), flipping by the horizontal axis, flipping by the vertical axis, and adding stochastic Gaussian noise. The probabilities to conduct those augmentation strategies for a patch were set as 0.15, 0.15, 0.25, 0.25, and 0.1, respectively. The comparative benchmark methods selected included the contextual information aggregation methods designed initially for natural images, such as pyramid scene parsing network (PSPNet) (Zhao et al., 2017) and dual attention network (DANet) (Fu et al., 2019), the multi-scale feature aggregation models proposed for remote sensing images, including multi-stage attention ResU-Net (MAResU-Net) (Li et al., 2021b) and edge-aware neural network (EaNet) (Zheng et al., 2020), as well as lightweight networks developed for efficient semantic segmentation, including depth-wise asymmetric bottleneck network (DABNet) (Li et al., 2019a), efficient residual factorized convNet (ERFNet) (Romera et al., 2017), bilateral segmentation network V1 (BiSeNetV1) (Yu et al., 2018) and V2 (BiSeNetV2) (Yu et al., 2020), fast attention network (FANet) (Hu et al., 2020), ShelfNet (Zhuang et al., 2019) and SwiftNet (Oršić and Šegvić, 2021). In the inference stage, we also utilized the data augmentation operation including random rotation and horizontal as well as vertical flipping which is also known as test-time augmentation (TTA).
Evaluation metrics
The performance of ABCNet was evaluated using the overall accuracy (OA), mean Intersection over Union (mIoU), and F1 score (F1). Based on the accumulated confusion matrix, the OA, mIoU, and F1 are computed as:(17)OA=∑k=1KTPk∑k=1KTPk+FPk+TNk+FNk,<math><mrow is="true"><mi mathvariant="italic" is="true">OA</mi><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">K</mi></msubsup><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub></mrow><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">K</mi></msubsup><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FP</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TN</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FN</mi></mrow><mi is="true">k</mi></msub></mrow></mfrac><mo is="true">,</mo></mrow></math>(18)mIoU=1K∑k=1KTPkTPk+FPk+FNk,<math><mrow is="true"><mi mathvariant="italic" is="true">mIoU</mi><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">K</mi></mfrac><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">K</mi></msubsup><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FP</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FN</mi></mrow><mi is="true">k</mi></msub></mrow></mfrac><mo is="true">,</mo></mrow></math>(19)precisionk=TPkTPk+FPk,<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">precision</mi></mrow><mi is="true">k</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FP</mi></mrow><mi is="true">k</mi></msub></mrow></mfrac><mo is="true">,</mo></mrow></math>(20)recallk=TPkTPk+FNk,<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">recall</mi></mrow><mi is="true">k</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FN</mi></mrow><mi is="true">k</mi></msub></mrow></mfrac><mo is="true">,</mo></mrow></math>(21)F1k=2×precisionk×recallkprecisionk+recallk<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">F</mi><mn is="true">1</mn></mrow><mi is="true">k</mi></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">2</mn><mo is="true">×</mo><mfrac is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">precision</mi></mrow><mi is="true">k</mi></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">recall</mi></mrow><mi is="true">k</mi></msub></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">precision</mi></mrow><mi is="true">k</mi></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">recall</mi></mrow><mi is="true">k</mi></msub></mrow></mfrac></mrow></math>
where TPk<math><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TP</mi></mrow><mi is="true">k</mi></msub></math>, FPk<math><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FP</mi></mrow><mi is="true">k</mi></msub></math>, TNk<math><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">TN</mi></mrow><mi is="true">k</mi></msub></math>, and FNk<math><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">FN</mi></mrow><mi is="true">k</mi></msub></math> represent the true positive, false positive, true negative, and false negatives, respectively, for a particular object indexed as class k. The OA was computed for all categories including the background class.

Ablation study
To evaluate the effectiveness of the components in the proposed ABCNet, we conducted extensive ablation experiments; the setting details and quantitative results are listed in Table 1.
Baseline: The baseline was constructed based on the contextual path, while the generated feature maps were up-sampled directly to the same shape as the original input image.
Ablation for attention enhancement module: To capture the global contextual information, we designed an attention enhancement module (AEM) in the contextual path. As presented in Table 1, for two datasets, the utilization of AEM (indicated as Cp + AEM) produced an increase of greater than 1.4% in the mIoU.
Ablation for the spatial path: Table 1 demonstrates that even simple fusion schemes for merging spatial information such as summation (represented as Cp + Sp + AEM(Sum)) and concatenation (represented as Cp + Sp + AEM(Cat)) boosted the performance of the mIoU by about 1.8% on Vaihingen dataset, and 0.4% on Potsdam dataset.
Ablation for feature aggregation module: As shown in Table 1, the significant gap in performance (more than 2.4% in the mIoU) demonstrates the validity of the feature aggregation module (signified as Cp + Sp + AEM + FAM).
The complexity and speed of the network
Complexity and speed are important criteria for measuring the merit of an algorithm, and this is especially true for practical applications. We first compared the computation and memory requirements between the linear attention mechanism and dot-product attention mechanism which can be found in Fig. 6.
For a comprehensive comparison, we further implemented the experiments under different settings. A comparison between the parameters and computational complexity of the different networks is reported in Table 2. The proposed ABCNet maintained both high speed and high accuracy simultaneously. As listed in the last column of Table 2, the mIoU on the Potsdam dataset achieved by the ABCNet is at least 2.0% higher than the benchmark methods. Meanwhile, the ABCNet was able to achieve a 72.13 FPS speed for a 512 × 512 input. The remarkable performance of the speed and occupation of memory not only derives from the linear attention mechanism but also results from that we only utilized the AEM in deeper layers with small spatial dimensionality. Besides, the elaborate design enabled the ABCNet to handle the massive input (4096 × 4096), while more than half of the benchmark methods ran out of memory for a such large input.
Results on the ISPRS Vaihingen and Potsdam datasets
The ISPRS Vaihingen is a relatively small dataset. All images represent the same city, such that the statistical characters of the training and test datasets are similar (Ghassemi et al., 2019). Therefore, high accuracy can be achieved relatively easily by specifically designed networks, especially for those that fuse orthophoto (TOP) images with auxiliary DSMs or NDSMs. In this section, we demonstrate that the proposed ABCNet model using only TOP images with an efficient architecture can not only transcend lightweight networks (Table 3) but also achieve highly competitive accuracy compared to specially designed models (Table 4).
As shown in Table 3, the numeric scores for the ISPRS Vaihingen test dataset demonstrated that ABCNet delivers high accuracy, exceeding other lightweight networks in the mean F1, OA, and mIoU by a significant margin. Particularly, the ‘‘car’’ class in the Vaihingen dataset is difficult to handle as it is a relatively small object. Nonetheless, ABCNet produced an 85.3% F1 score for this class, which is at least 4.1% higher than for the benchmark methods. In addition, we visualize area 27 in Fig. 7 to qualitatively demonstrate the effectiveness of ABCNet, while the enlarged results are shown in Fig. 9 (top).
For a comprehensive evaluation, ABCNet was also compared with other state-of-the-art methods. As can be seen in Table 4, as a lightweight network, the proposed ABCNet achieved a competitive performance even compared with those models designed with complex structures. It is worth noting that the speed of ABCNet is two-to-seven times faster than those methods.
Furthermore, we undertook experiments on the ISPRS Potsdam dataset to further evaluate the performance of ABCNet. Compared with the encoder-decoder structure, the bilateral architecture can retain more spatial information without reducing the speed of the model (Yu et al., 2018). The spatial path stacks only three convolution layers to generate 1/8 feature maps, while the contextual path includes two attention enhancement modules (AEM) to refine the features and capture contextual information. Numerical comparisons with other lightweight methods are shown in Table 5. Remarkably, ABCNet achieved 91.3% overall accuracy and 86.5% in mIoU. Visualization of area 3_13 is displayed in Fig. 8, and the enlarged results are exhibited in Fig. 9 (bottom). As there are sufficient images in the Potsdam dataset to train the network, the performance of ABCNet can be equivalent to the state-of-the-art methods with a much faster speed. The comparison results are listed in Table 6.
The comprehensive experiments undertaken demonstrate the superiority of ABCNet, not only for segmentation accuracy but also efficiency. There are three important factors that guarantee accuracy without drastically increasing computational consumption. First, the bilateral architecture resolves the contradiction between sufficient contextual information and fine-grained spatial detail. The channel pruning or input cropping operations are commonly used in the encoder-decoder structure to boost inference speed, leading to the loss of low-level and spatial details which cannot be recovered easily. In contrast, the proposed ABCNet adopts a bilateral architecture, where a spatial path extracts low-level features and a contextual path exploits high-level features. To demonstrate the difference between the contextual path (Cp) and spatial path (Sp) visually, we visualize the feature maps generated by the Cp and Sp in Fig. 10. Please note that the features maps of are upsampled to restore the shape. As can be seen in the figure, the information provided by the contextual path and spatial has indeed differences. Specifically, in feature maps of the contextual path, objects have a more consistent character with those pixels in the same class. By contrast, more detailed information is preserved in the spatial path. Meanwhile, the relatively efficient design of the spatial path (three stacked identical layers) and contextual path (the ResNet-18 backbone) avoids large computational requirements. Second, the attention enhancement module balances the trade-off between global contextual information and huge calculation complexity. Conventionally, the dot-product attention mechanism employed to capture long-range dependencies is accompanied by quadratic increases in time and memory consumption with input size. Instead, we harness the linear attention mechanism, developed in our previous research, to provide a calculation-friendly scheme for global contextual information extraction. Third, the feature aggregation module merges the spatial features and contextual features in an appropriate fashion. The spatial features generated by the spatial path are low-level and detailed, while the contextual features generated by the contextual path are high-level and semantically rich. In other words, the features have entirely different semantic meanings. Hence, although a degree of improvement in accuracy can be brought, the simple summation or concatenation operations are not the optimal feature fusion scheme. The elaborate feature aggregation module developed here ensures reasonable fusion and full utilization of both sets of features.
In this paper, we propose a novel lightweight framework for efficient semantic segmentation in the field of remote sensing, namely the Attentive Bilateral Contextual Network (ABCNet). As both sufficient contextual information and fine-grained spatial detail are crucial for the accuracy of segmentation, we design the ABCNet based on the bilateral architecture which captures simultaneously and adaptively the abundant spatial details in fine-resolution remotely sensed imagery via a spatial path and the global contextual information via a contextual path. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness and efficiency of the proposed ABCNet, with huge potential for practical real-time applications. Although achieving a relatively fine balance between effectiveness and efficiency, the speed of the proposed ABCNet has a certain room for improvement, especially when compared with those single-branch lightweight networks. As the contextual path occupies the majority of parameters and complexities, our future work will focus on further optimizing the contextual path of the ABCNet, especially to design an efficient Transformer backbone using our linear attention mechanism, thereby replacing the original ResNet backbone with this novel structure.
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
This research was funded by the National Natural Science Foundation of China, Grant No. 41671452.