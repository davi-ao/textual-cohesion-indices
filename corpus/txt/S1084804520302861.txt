Micro-blog is an essential platform for sharing information and disseminating information. Users can freely participate in the discussion of a hot topic purposefully by inputting "#topic#" every day. However, one new subtopic or more is generated by changing the relevant attributes (time, place, people, and others) from the initial center of the topic, since users with different concerns gradually join the discussion of hot topics. The initial center of a micro-blog hot topic develops into multi-subtopic with the user's further analysis, including more branches, more levels, and more structures. The development of the current topic center is difficult to be timely and comprehensively understood from the massive flow of information, and unconducive for government sectors to monitor the public opinion on the Internet.
Regarding to the research on a micro-blog hot topic, a previous work proposed a microblogging word extraction algorithm and realize the emotional tendency analysis of microblog texts (Zhang and Wei, 2018), and an algorithm of computing user authority to construct topic feature graphic (TFG) and user micro-blog feature graph (UMFG) that realizes a micro-blog topic recommendation system in (Zhang and Zhang, 2017). In (Zhang and Lu, 2017), a rapid mining model for extracting sparse distribution k-ASL from large-scale web resources that solves the problem of extraction omission caused by sparseness in keyword-level association semantic link extraction is proposed, also approaching the sub-topic division of micro-blog hot topics. Different from the previous work, this micro-blog hot topic research is a fine-grained sub-topic segmentation and based on feature co-occurrence and semantic community division to build a multi-subtopic bi-level network of the micro-blog hot topic model.
An effective multi-subtopic bi-level network for the micro-blog hot topic must consider the key scientific problems, and they are twofold. The former one involves how to extract the feature words that are used to represent the current topic center from a hot topic of the micro-blog text and to use specific rules to connect the feature words, so the complex network graph is preliminarily constructed according to the relationship between words. As latter, the topic complex network is divided into different semantic communities according to the core semantics of hot topics, each of which represents a subtopic. Meanwhile, the construction of the bi-level network has two basic requirements. Like the first one, the central content of the current topic must be expressed as wholly as possible by using feature words, while the two keywords with edge connection are strongly correlated in the model as far as possible is the second.
Considering the above two key scientific problems, it is proposed in this paper a complex network multi-subtopic bi-level network based on feature co-occurrence and semantic community division for a hot topic on micro-blog. The contributions are the following two steps, as shown in Fig. 1.
(1)The construction of the micro-blog hot topic complex network. First, the hot micro-blog text is pre-processed to remove irrelevant data such as stop words and emoticons, and then, “Jieba” Chinese text segmentation1 is used to segment words in the text. Second, the highlighted words are extracted in consideration of the importance of the micro-blog (incorporating the number of comments and the number of praise points) and the time decay. The global co-occurrence rate and the local co-occurrence rate of words are combined to represent the correlation strength between feature words. Finally, a complex network of micro-blog topic is constructed in the form of co-occurrence words.
(2) The construction of a multi-subtopic bi-level network for a micro-blog hot topic. First, an improved weighted modularity function is used to measure the quality of a division of complex networks. Next, the genetic algorithm is used to solve the maximum value of the weighted modularity, and the semantic community division of the micro-blog hot topic complex network is obtained. The multi-subtopic of the micro-blog topic is represented in the form of multiple semantic communities. At last, the terminal location information of each micro-blog in different semantic communities is analyzed to draw a regional location map, so finally, a multi-subtopic bi-level network of the micro-blog hot topic complex network can be built.
In this article, the model construction of micro-blog hot topics is completed in two steps. When extracting micro-blog hot topic feature words, a combination of the time factor of micro-blog, user comments, and praise points is used to introduce co-occurrence to improve the accuracy of unique testimony extraction. The advantage of ‘more’ accurate micro-blog hot topic feature words extraction is to make the constructed micro-blog hot topic complex network more representative. This paper presents a new weighting module formula, which is more suitable for the division of complex network semantic community, and is conducive to the construction of micro-blog multi-topic bi-level network. For the first time, a genetic algorithm is applied to the division of semantic community to construct the first level MSBN, on this basis, the location information and time are analyzed to build the second level of MSBN. The advantage of using the method proposed in this paper to construct the MSBN is not only to help users fully understand the multi-subtopic of microblog topics but also to help users understand the time and place of the topic. The disadvantage of the method proposed in this paper is that the algorithm is a bit complicated and not simple enough.
The remaining of this paper is structured as follows. In Section 2, we review the status of existing work, while in Section 3 the construction of a complex network for the micro-blog topic is presented. Section 4 introduces the multi-subtopic division of micro-blog hot topic complex networks, and experimental analysis of the model are depicted in Section 5. Finally, concluding remarks and future work are presented in Section 6.
In this section, we briefly review three aspects of the current theoretical work including feature word extraction, complex network construction, and community division.
The methods of feature word extraction can be broadly divided into two categories. The first one is dependent on the external knowledge base. For example, TF-IDF values are used to be the basis for statistical sorting to assess the importance of words in the text set (Salton and Buckley, 1988; Subaihin et al., 2019). Based on TF-IDF, words are assigned to different weights according to the position of their appearance to extract feature words (Paik, 2013). RAKE algorithm is based on the concept of word frequency and the degree to extract key phrases (Vidyadhari et al., 2019), highly dependent on the external knowledge base and the accuracy of feature words extraction is dependable on the knowledge base. The other is independent of the external knowledge base. For example, words are considered to be a page with the extension of Pagerank. Textrank is proposed to extract phrases and abstracts (Onan et al., 2016; Chowdhury et al., 2019; Deng and Wan, 2017), an algorithm based on left and right entropy (Ruan and Xia, 2017; Tabor and Spurek, 2014), an algorithm based on mutual information (Sugiyama et al., 2014), and others. Although this method is independent of the external knowledge base, it ignores the connection between feature words and knowledge base, and also affects the accuracy of feature words extraction. In recent years, the co-occurrence relationship of words has been studied deeply (Qu et al., 2018; Yu et al., 2015; Hai and Luo, 2006; Li et al., 2019a). This method takes into account the above two methods and has significant advantages.
Complex Network is the network with some or all properties of self-organization, self-similarity, attractor, small-world and scale-free (Strogatz, 2001; Akilal et al., 2019; Chen and Zhu, 2020). In 1998, Watts and Strogatz constructed a network between the regular networks and random networks (Watts and Strogatz, 1998). Subsequently, Newman and Watts gave a new construction method of network. In their NW network, the original edge was destroyed and the average distance was shortened by adding a new edge to the original regular network with a very small probability (Newman and Watts, 1999). With the further study of the real network, it was found that almost all the real networks have small-world characteristics (Yu et al., 2017; Shi et al., 2015; Li et al., 2019b) and the node degree submit to the power-law distribution (Chattopadhyay and Murthy, 2017; Qiao et al., 2019; Zhang et al., 2013). Modularity is one of the most important and common characteristics in complex networks. It refers to the community structure existed in complex networks. The nodes in the same community are densely connected, and the nodes in different communities are sparsely connected (Lancichinetti et al., 2009; Li et al., 2019c; Chung et al., 2019). In recent years, many achievements have made in the study of community division. Lv et al. (2017) propose a community division scheme based on a maximal tree to help retrieve content quickly and effectively. Wang et al. (2018) propose a cross-domain sentiment classification algorithm for short texts which can be used to divide a hot topic on social media into several sub-topic from sentiment view.
Based on the analysis of the above research results, a high-frequency feature word extraction method that combines characteristics of micro-blog and time decay coefficient is proposed. The concept of complex co-occurrence rate of words is introduced to fully consider the correlation degree of frequency and semantics between co-occurrence words. Then, the micro-blog hot topic complex network of co-occurrence feature words is constructed. Following next, an improved weighted modularity function is used to evaluate the community division of complex network. So finally, the genetic algorithm is used to solve the maximum module degree and obtain the optimal multi-subtopic division result to realize the multi-subtopic identification of micro-blog hot topic.
This section gives some basic definitions about the extraction of Weibo feature words and extracts feature words on micro-blog hot topics by combining micro-blog importance and time decay coefficient. Next, a complex network is constructed by connecting edges between any two feature words that are more closely related in co-occurrence relationships.
Definition 1

(Time decay coefficient (Ctd<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub></mrow></math>))

The time decay coefficient is used to improve the accuracy of feature words selection by weakening the importance of early micro-blog. Due to the fast update of micro-blog, the earlier micro-blog of a topic is published, the less essential its content will be. The Ebbinghaus Forgetting Curve is a description of the forgetting of the human brain on new things. This paper makes use of the MATLAB tool to fit the forgetting curve and retrieve a solid line, as presented in Fig. 2.

Download : Download high-res image (180KB)Download : Download full-size imageFig. 2. Curves fitting to Ebbinghaus Forgetting Curve.

The time decay coefficient with a fitting function can be described as follows:(1)Ctd=1-0.56×(T1-T0)0.06<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub><mo linebreak="badbreak" is="true">=</mo><mn is="true">1</mn><mo is="true">-</mo><mn is="true">0.56</mn><mo linebreak="goodbreak" is="true">×</mo><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">T</mi><mn is="true">1</mn></msub><mo is="true">-</mo><msub is="true"><mi is="true">T</mi><mn is="true">0</mn></msub><mo stretchy="true" is="true">)</mo></mrow><mn is="true">0.06</mn></msup></mrow></math>where T0<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mn mathvariant="italic" is="true">0</mn></msub></mrow></math> represents the time when this micro-blog is released and T1<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mn mathvariant="italic" is="true">1</mn></msub></mrow></math> indicates the time when this micro-blog was captured.

Definition 2 (Micro-blog importance (Imb<math><mrow is="true"><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub></mrow></math>))

The micro-blog importance is a defined coefficient used to determine the feature word of micro-blog. It mainly depends on the number of praise points and comments. For example, a micro-blog with thousands of comments is generally higher in importance than another micro-blog, which a few people comment on. Let a<math><mi is="true">a</mi></math> and b<math><mi is="true">b</mi></math> denote the adjustment factors, the formula of micro-blog importance can be represented as:(2)Imb=aNr+bNc<math><mrow is="true"><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub><mo linebreak="badbreak" is="true">=</mo><mi is="true">a</mi><msub is="true"><mi is="true">N</mi><mi is="true">r</mi></msub><mo linebreak="goodbreak" is="true">+</mo><mi is="true">b</mi><msub is="true"><mi is="true">N</mi><mi is="true">c</mi></msub></mrow></math>where Nr<math><mrow is="true"><mi is="true">N</mi><mi is="true">r</mi></mrow></math>, Nc<math><mrow is="true"><mi is="true">N</mi><mi is="true">c</mi></mrow></math> denote the number of praise points and comments respectively. To balance the importance of praise points and comments a<math><mi is="true">a</mi></math> is assigned to 0.3 and b<math><mi is="true">b</mi></math> is valued as 0.7. Next, the value formula of Nr<math><mrow is="true"><mi is="true">N</mi><mi is="true">r</mi></mrow></math> and Nc<math><mrow is="true"><mi is="true">N</mi><mi is="true">c</mi></mrow></math> can be formalized as:(3)Nr‵c={1N<10210≤N<1024102≥N<math><msub is="true"><mi is="true">N</mi><mrow is="true"><msub is="true"><mtext is="true">r</mtext><mo is="true">‵</mo></msub><mspace width="0.25em" is="true"></mspace><mspace width="0.25em" is="true"></mspace><mspace width="0.25em" is="true"></mspace><mtext is="true">c</mtext></mrow></msub><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable columnalign="left" is="true"><mtr is="true"><mtd is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mn is="true">1</mn></mtd><mtd is="true"><mi is="true">N</mi><mo is="true">&lt;</mo><mn is="true">10</mn></mtd></mtr></mtable></mtd></mtr><mtr is="true"><mtd is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mn is="true">2</mn></mtd><mtd is="true"><mn is="true">10</mn><mo is="true">≤</mo><mi is="true">N</mi><mo is="true">&lt;</mo><msup is="true"><mn is="true">10</mn><mn is="true">2</mn></msup></mtd></mtr></mtable></mtd></mtr><mtr is="true"><mtd is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mn is="true">4</mn></mtd><mtd is="true"><msup is="true"><mn is="true">10</mn><mn is="true">2</mn></msup><mo is="true">≥</mo><mi is="true">N</mi></mtd></mtr></mtable></mtd></mtr></mtable></mrow></math>

Definition 3 (Word support (Sup(w)<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>))

Micro-blog feature word support is an important measurement to judge the importance of words, which are given based on the time decay coefficient and micro-blog importance. In the word set W<math><mi is="true">W</mi></math>, the support formula w<math><mi is="true">w</mi></math> can be described as:(4)Sup(w)=‖w‖‖W‖×Ctd×Imb<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">w</mi><mo is="true">‖</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">W</mi><mo is="true">‖</mo></mrow></mrow></mfrac><mo linebreak="badbreak" is="true">×</mo><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub><mo linebreak="badbreak" is="true">×</mo><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub></mrow></math>where ‖w‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">w</mi><mo is="true">‖</mo></mrow></mrow></math> denotes the number of w<math><mi is="true">w</mi></math> in the word set W<math><mi is="true">W</mi></math>, ‖W‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">W</mi><mo is="true">‖</mo></mrow></mrow></math> represents the total number of participle, Ctd<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub></mrow></math> is used to describe the time decay coefficient and Imb<math><mrow is="true"><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub></mrow></math> is treated as the micro-blog importance.

Definition 4 (Micro-blog topic feature word (v<math><mi is="true">v</mi></math>))

Micro-blog topic feature words (Abbr. Feature word) are common words with a higher importance in a certain period of time. It can be remarked as:(5)v1,v2,...,vk<math><mrow is="true"><msub is="true"><mi is="true">v</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mn is="true">...</mn><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub></mrow></math>where v<math><mi is="true">v</mi></math> denotes the feature words and k<math><mi is="true">k</mi></math> is used to represent the number of v<math><mi is="true">v</mi></math>.

Definition 5 (Global co-occurrence rate Pg(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">g</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>).

The global co-occurrence rate represents the probability of two words appearing together in all data sets. In large-scale data sets, if two words often appear together in the text, the two words are considered to be strongly related in semantic. In the word concentration U<math><mi is="true">U</mi></math>, the global co-occurrence rate of ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math> can be expressed as:(6)P1(ui,uj)=‖D(ui,uj)‖l1<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">1</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mtext is="true">u</mtext><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow><mo is="true">‖</mo></mrow></mrow><mrow is="true"><msub is="true"><mi is="true">l</mi><mn is="true">1</mn></msub></mrow></mfrac></mrow></math>where D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mtext is="true">u</mtext><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> is a micro-blog collection (i,j<k<math><mrow is="true"><mi is="true">i</mi><mi is="true">,</mi><mi is="true">j</mi><mo linebreak="badbreak" is="true">&lt;</mo><mi is="true">k</mi></mrow></math> ) that concurrently contains both the feature words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>. ‖·‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mo linebreak="badbreak" is="true">·</mo><mo is="true">‖</mo></mrow></mrow></math> Denotes the number of collection D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mtext is="true">u</mtext><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>, and l1<math><mrow is="true"><msub is="true"><mi is="true">l</mi><mn is="true">1</mn></msub></mrow></math> expresses the total number of micro-blog texts.

Definition 6 (Local co-occurrence rate Pl(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">l</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>).

The local co-occurrence rate represents the probability of two words appearing together in a text containing only two feature words. It is used to measure the relevance of any two feature words in a local context. In the word set U<math><mi is="true">U</mi></math>, the local co-occurrence rate of ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math> can be described as:(7)P2(ui,uj)=‖D(ui,uj)‖l2<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">2</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow><mo is="true">‖</mo></mrow></mrow><mrow is="true"><msub is="true"><mi is="true">l</mi><mn is="true">2</mn></msub></mrow></mfrac></mrow></math>where D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> denotes the micro-blog set containing both the feature words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>, ‖·‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mo linebreak="badbreak" is="true">·</mo><mo is="true">‖</mo></mrow></mrow></math> indicates the number of D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> and micro-blog texts containing characteristic words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>.

Definition 7 (Complex co-occurrence rate Pc(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">c</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>).

The complex co-occurrence rate is the probability that the global co-occurrence rate and local co-occurrence rate of co-occurrence words are taken into account by adding adjusting factors.(8)P(ui,uj)=γP1(ui,uj)+(1−γ)P2(ui,uj)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">=</mo><mi is="true">γ</mi><msub is="true"><mi is="true">P</mi><mn is="true">1</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">+</mo><mrow is="true"><mo stretchy="true" is="true">(</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">−</mo><mi is="true">γ</mi><mo stretchy="true" is="true">)</mo></mrow><msub is="true"><mi is="true">P</mi><mn is="true">2</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>where γ<math><mi is="true">γ</mi></math> denotes regulator, P1(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">1</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> presents the global co-occurrence rate of words and P2(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">2</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> indicates the local co-occurrence rate of words.
(Time decay coefficient (Ctd<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub></mrow></math>))
The time decay coefficient is used to improve the accuracy of feature words selection by weakening the importance of early micro-blog. Due to the fast update of micro-blog, the earlier micro-blog of a topic is published, the less essential its content will be. The Ebbinghaus Forgetting Curve is a description of the forgetting of the human brain on new things. This paper makes use of the MATLAB tool to fit the forgetting curve and retrieve a solid line, as presented in Fig. 2.
The time decay coefficient with a fitting function can be described as follows:(1)Ctd=1-0.56×(T1-T0)0.06<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub><mo linebreak="badbreak" is="true">=</mo><mn is="true">1</mn><mo is="true">-</mo><mn is="true">0.56</mn><mo linebreak="goodbreak" is="true">×</mo><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">T</mi><mn is="true">1</mn></msub><mo is="true">-</mo><msub is="true"><mi is="true">T</mi><mn is="true">0</mn></msub><mo stretchy="true" is="true">)</mo></mrow><mn is="true">0.06</mn></msup></mrow></math>where T0<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mn mathvariant="italic" is="true">0</mn></msub></mrow></math> represents the time when this micro-blog is released and T1<math><mrow is="true"><msub is="true"><mi is="true">T</mi><mn mathvariant="italic" is="true">1</mn></msub></mrow></math> indicates the time when this micro-blog was captured.
Definition 2 (Micro-blog importance (Imb<math><mrow is="true"><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub></mrow></math>))
The micro-blog importance is a defined coefficient used to determine the feature word of micro-blog. It mainly depends on the number of praise points and comments. For example, a micro-blog with thousands of comments is generally higher in importance than another micro-blog, which a few people comment on. Let a<math><mi is="true">a</mi></math> and b<math><mi is="true">b</mi></math> denote the adjustment factors, the formula of micro-blog importance can be represented as:(2)Imb=aNr+bNc<math><mrow is="true"><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub><mo linebreak="badbreak" is="true">=</mo><mi is="true">a</mi><msub is="true"><mi is="true">N</mi><mi is="true">r</mi></msub><mo linebreak="goodbreak" is="true">+</mo><mi is="true">b</mi><msub is="true"><mi is="true">N</mi><mi is="true">c</mi></msub></mrow></math>where Nr<math><mrow is="true"><mi is="true">N</mi><mi is="true">r</mi></mrow></math>, Nc<math><mrow is="true"><mi is="true">N</mi><mi is="true">c</mi></mrow></math> denote the number of praise points and comments respectively. To balance the importance of praise points and comments a<math><mi is="true">a</mi></math> is assigned to 0.3 and b<math><mi is="true">b</mi></math> is valued as 0.7. Next, the value formula of Nr<math><mrow is="true"><mi is="true">N</mi><mi is="true">r</mi></mrow></math> and Nc<math><mrow is="true"><mi is="true">N</mi><mi is="true">c</mi></mrow></math> can be formalized as:(3)Nr‵c={1N<10210≤N<1024102≥N<math><msub is="true"><mi is="true">N</mi><mrow is="true"><msub is="true"><mtext is="true">r</mtext><mo is="true">‵</mo></msub><mspace width="0.25em" is="true"></mspace><mspace width="0.25em" is="true"></mspace><mspace width="0.25em" is="true"></mspace><mtext is="true">c</mtext></mrow></msub><mo is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><mtable columnalign="left" is="true"><mtr is="true"><mtd is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mn is="true">1</mn></mtd><mtd is="true"><mi is="true">N</mi><mo is="true">&lt;</mo><mn is="true">10</mn></mtd></mtr></mtable></mtd></mtr><mtr is="true"><mtd is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mn is="true">2</mn></mtd><mtd is="true"><mn is="true">10</mn><mo is="true">≤</mo><mi is="true">N</mi><mo is="true">&lt;</mo><msup is="true"><mn is="true">10</mn><mn is="true">2</mn></msup></mtd></mtr></mtable></mtd></mtr><mtr is="true"><mtd is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mn is="true">4</mn></mtd><mtd is="true"><msup is="true"><mn is="true">10</mn><mn is="true">2</mn></msup><mo is="true">≥</mo><mi is="true">N</mi></mtd></mtr></mtable></mtd></mtr></mtable></mrow></math>
Definition 3 (Word support (Sup(w)<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>))
Micro-blog feature word support is an important measurement to judge the importance of words, which are given based on the time decay coefficient and micro-blog importance. In the word set W<math><mi is="true">W</mi></math>, the support formula w<math><mi is="true">w</mi></math> can be described as:(4)Sup(w)=‖w‖‖W‖×Ctd×Imb<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">w</mi><mo is="true">‖</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">W</mi><mo is="true">‖</mo></mrow></mrow></mfrac><mo linebreak="badbreak" is="true">×</mo><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub><mo linebreak="badbreak" is="true">×</mo><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub></mrow></math>where ‖w‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">w</mi><mo is="true">‖</mo></mrow></mrow></math> denotes the number of w<math><mi is="true">w</mi></math> in the word set W<math><mi is="true">W</mi></math>, ‖W‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mi is="true">W</mi><mo is="true">‖</mo></mrow></mrow></math> represents the total number of participle, Ctd<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mrow is="true"><mi is="true">t</mi><mi is="true">d</mi></mrow></msub></mrow></math> is used to describe the time decay coefficient and Imb<math><mrow is="true"><msub is="true"><mi is="true">I</mi><mrow is="true"><mi is="true">m</mi><mi is="true">b</mi></mrow></msub></mrow></math> is treated as the micro-blog importance.
Definition 4 (Micro-blog topic feature word (v<math><mi is="true">v</mi></math>))
Micro-blog topic feature words (Abbr. Feature word) are common words with a higher importance in a certain period of time. It can be remarked as:(5)v1,v2,...,vk<math><mrow is="true"><msub is="true"><mi is="true">v</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mn is="true">...</mn><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub></mrow></math>where v<math><mi is="true">v</mi></math> denotes the feature words and k<math><mi is="true">k</mi></math> is used to represent the number of v<math><mi is="true">v</mi></math>.
Definition 5 (Global co-occurrence rate Pg(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">g</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>).
The global co-occurrence rate represents the probability of two words appearing together in all data sets. In large-scale data sets, if two words often appear together in the text, the two words are considered to be strongly related in semantic. In the word concentration U<math><mi is="true">U</mi></math>, the global co-occurrence rate of ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math> can be expressed as:(6)P1(ui,uj)=‖D(ui,uj)‖l1<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">1</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mtext is="true">u</mtext><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow><mo is="true">‖</mo></mrow></mrow><mrow is="true"><msub is="true"><mi is="true">l</mi><mn is="true">1</mn></msub></mrow></mfrac></mrow></math>where D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mtext is="true">u</mtext><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> is a micro-blog collection (i,j<k<math><mrow is="true"><mi is="true">i</mi><mi is="true">,</mi><mi is="true">j</mi><mo linebreak="badbreak" is="true">&lt;</mo><mi is="true">k</mi></mrow></math> ) that concurrently contains both the feature words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>. ‖·‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mo linebreak="badbreak" is="true">·</mo><mo is="true">‖</mo></mrow></mrow></math> Denotes the number of collection D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mtext is="true">u</mtext><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>, and l1<math><mrow is="true"><msub is="true"><mi is="true">l</mi><mn is="true">1</mn></msub></mrow></math> expresses the total number of micro-blog texts.
Definition 6 (Local co-occurrence rate Pl(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">l</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>).
The local co-occurrence rate represents the probability of two words appearing together in a text containing only two feature words. It is used to measure the relevance of any two feature words in a local context. In the word set U<math><mi is="true">U</mi></math>, the local co-occurrence rate of ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math> can be described as:(7)P2(ui,uj)=‖D(ui,uj)‖l2<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">2</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow><mo is="true">‖</mo></mrow></mrow><mrow is="true"><msub is="true"><mi is="true">l</mi><mn is="true">2</mn></msub></mrow></mfrac></mrow></math>where D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> denotes the micro-blog set containing both the feature words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>, ‖·‖<math><mrow is="true"><mrow is="true"><mo is="true">‖</mo><mo linebreak="badbreak" is="true">·</mo><mo is="true">‖</mo></mrow></mrow></math> indicates the number of D(ui,uj)<math><mrow is="true"><mi is="true">D</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> and micro-blog texts containing characteristic words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>.
Definition 7 (Complex co-occurrence rate Pc(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">c</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>).
The complex co-occurrence rate is the probability that the global co-occurrence rate and local co-occurrence rate of co-occurrence words are taken into account by adding adjusting factors.(8)P(ui,uj)=γP1(ui,uj)+(1−γ)P2(ui,uj)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">=</mo><mi is="true">γ</mi><msub is="true"><mi is="true">P</mi><mn is="true">1</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">+</mo><mrow is="true"><mo stretchy="true" is="true">(</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">−</mo><mi is="true">γ</mi><mo stretchy="true" is="true">)</mo></mrow><msub is="true"><mi is="true">P</mi><mn is="true">2</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math>where γ<math><mi is="true">γ</mi></math> denotes regulator, P1(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">1</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> presents the global co-occurrence rate of words and P2(ui,uj)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mn is="true">2</mn></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> indicates the local co-occurrence rate of words.
For the selection of feature words, D={d1,d2,d3,⋯,dn}<math><mrow is="true"><mi is="true">D</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mi is="true">d</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mn is="true">2</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mn is="true">3</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mi is="true">n</mi></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math> is assumed to be a collection of micro-blog texts related to a certain "#topic#", and the preprocessed micro-blog word set wi={wi1,wi2,⋯,wij,⋯,wnm}<math><mrow is="true"><msub is="true"><mtext is="true">w</mtext><mtext is="true">i</mtext></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mi is="true">w</mi><mrow is="true"><mtext is="true">i</mtext><mn is="true">1</mn></mrow></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mrow is="true"><msub is="true"><mi is="true">i</mi><mn is="true">2</mn></msub></mrow></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mtext is="true">j</mtext></mrow></msub><mi is="true">,</mi><mo is="true">⋯</mo><mi is="true">,</mi><msub is="true"><mi is="true">w</mi><mtext is="true">nm</mtext></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math> is used to corresponds to the micro-blog text di(i=1,2,⋯,n)<math><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">i</mi><mo linebreak="badbreak" is="true">=</mo><mn is="true">1,2</mn><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>. The wij<math><mrow is="true"><msub is="true"><mtext is="true">w</mtext><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub></mrow></math> represents the j<math><mi is="true">j</mi></math> th participle of the micro-blog text di<math><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub></mrow></math>. W={w1,w2,⋯,wn}<math><mrow is="true"><mi is="true">W</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mtext is="true">w</mtext><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">w</mi><mtext is="true">n</mtext></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math> represents all text preprocessed word sets in the micro-blog text collection D<math><mi is="true">D</mi></math>, and wi<math><mrow is="true"><msub is="true"><mi is="true">w</mi><mi is="true">i</mi></msub></mrow></math> represents the micro-blog word set corresponding to the micro-blog di<math><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub></mrow></math>.
Sup(w)<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math> Stands for the support degree of a word w<math><mi is="true">w</mi></math> in the word set W<math><mi is="true">W</mi></math>. WhenSup(w)≥α<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">≥</mo><mi is="true">α</mi></mrow></math> (representing the support threshold of the minimum feature words), thew<math><mi is="true">w</mi></math> is added to the feature word setU={u1,u2,⋯,uk}<math><mrow is="true"><mi is="true">U</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mi is="true">u</mi><mn is="true">1</mn></msub><mi is="true">,</mi><msub is="true"><mi is="true">u</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">k</mi></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math>。k<math><mi is="true">k</mi></math> is the length of the feature words setU<math><mi is="true">U</mi></math>.
In large-scale data sets, if two words often appear in the same document, the two words are considered to be related in meaning. The higher the probability that the two words co-occur, the closer the internal association is.
According to the above definition of the word global co-occurrence rate, co-occurrence words with higher word frequency can be found from large-scale data sets. However, some co-occurrence words with low word frequency but high correlation intensity will be ignored. For example, in the topic discussion of the World Cup, there is a data set containing 1000 pieces of micro-blog text. In this data set, 90 micro-blogs contain “opening wars”; 90 micro-blogs contain “Saudi Arabia”; 80 micro-blogs contain “opening war” and “Saudi Arabia”. The global co-occurrence rate is only 0.08, but the probability that the two words appear together in the text containing “opening wars” or “Saudi Arabia” is 0.8. From the actual situation, the 2018 World Cup Saudi Arabia participated in the opener competition. Therefore, the relevance of these two words should be high. From the actual situation, Saudi Arabia did participate in the 2018 World Cup opening war, and the significance of these two words should be very high. Therefore, the concept of the local co-occurrence rate needs to be introduced to help us more accurately measure the strength of contextual association between words.
In this paper, the complex co-occurrence rate is used to measure the correlation strength between two feature words, not only considering the common frequency of feature words but also considering the contextual relevance of two feature words from a more detailed perspective. If the feature words ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math> are related to a subtopic of the topic, the P(ui,uj)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> must be greater than a certain constant, that is, P(ui,uj)≥β<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">u</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">≥</mo><mi is="true">β</mi></mrow></math> (β<math><mi is="true">β</mi></math> represents the minimum complex co-occurrence threshold). At this time, ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math> are called a pair of co-occurrence feature words, and the higher the complex co-occurrence rate of the feature word ui<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">i</mi></msub></mrow></math> and uj<math><mrow is="true"><msub is="true"><mi is="true">u</mi><mi is="true">j</mi></msub></mrow></math>, the closer the internal correlation is.
The micro-blog hot topic complex network is constructed to visualize the relationship between feature words. It helps users to understand the main content of micro-blog hot topics more comprehensively and accurately. First, it is extracted in this paper feature words, so then the connection relationship between feature words is abstracted into a formal representation of the network G=(V,E)<math><mrow is="true"><mi is="true">G</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>. Where, V={v1,v2,⋯vk}<math><mrow is="true"><mi is="true">V</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mtext is="true">v</mtext><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><msub is="true"><mi is="true">v</mi><mtext is="true">k</mtext></msub><mo stretchy="true" is="true">}</mo></mrow></mrow></math> is the node-set of the graph (ie, the word set of feature words); k<math><mi is="true">k</mi></math> is the number of nodes (i.e., the number of feature words); E={e12,⋯,est,⋯}<math><mrow is="true"><mi is="true">E</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">{</mo><msub is="true"><mi is="true">e</mi><mn is="true">12</mn></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo is="true">,</mo><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">s</mi><mi is="true">t</mi></mrow></msub><mo is="true">,</mo><mo is="true">⋯</mo><mo stretchy="true" is="true">}</mo></mrow></mrow></math> （1≤s≤k,1≤t≤k,s≠t<math><mrow is="true"><mn is="true">1</mn><mo linebreak="goodbreak" linebreakstyle="after" is="true">≤</mo><mi is="true">s</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">≤</mo><mi is="true">k</mi><mo is="true">,</mo><mn is="true">1</mn><mo linebreak="goodbreak" linebreakstyle="after" is="true">≤</mo><mi is="true">t</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">≤</mo><mi is="true">k</mi><mo is="true">,</mo><mi is="true">s</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">≠</mo><mi is="true">t</mi></mrow></math>） represents edge set in the network; s<math><mi is="true">s</mi></math>、 t<math><mi is="true">t</mi></math> denote variables that no larger than k<math><mi is="true">k</mi></math>. The specific steps are as follows:
Micro-blog data acquisition. Regular expressions is used to crawl data sets that conform to the "#topic#" format. Data sets should include the release time, crawl time, number of comments, and number of praises;
Micro-blog data preprocessing. “Jieba” Chinese text segmentation is used to segment the data set. The stop word list is customized. These measures can save storage space and improve search efficiency. Own defined dictionary should be chosen to include words that are not in the “Jieba” lexicon;
Node set generation. It extracts feature words on micro-blog hot topics by integrating micro-blog importance and time decay coefficient. The support degree of each word is calculated based on the word support formula. The word support threshold α<math><mi is="true">α</mi></math> is set. If the word support is higher than the threshold α<math><mi is="true">α</mi></math>, the word will be selected as the feature word and added to the node set V<math><mi is="true">V</mi></math>. Otherwise, the word will be deleted;
Edge set generation. The degree of association between any two feature words is measured from the two aspects of the global co-occurrence rate and local co-occurrence rate. The complex co-occurrence rate between any two feature words is calculated according to the word complex co-occurrence rate formula. The threshold β<math><mi is="true">β</mi></math> is set for complex co-occurrence rates. If the complex co-occurrence rate of any two feature words is higher than the threshold β<math><mi is="true">β</mi></math>, the edge which is generated with the two words will be added to the eage set E<math><mi is="true">E</mi></math>. Otherwise, there's no correlation between them.
The specific algorithm process can be shown by the following algorithm 1.
Algorithm 1. Construction of a micro-blog hot topic complex network. G<math><mi is="true">G</mi></math>Input:A micro-blog hot topic text setOutput:hot topic complex network (G<math><mi is="true">G</mi></math>)1.V=E=∅<math><mrow is="true"><mi is="true">V</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">E</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo linebreak="goodbreak" linebreakstyle="after" is="true">∅</mo></mrow></math>;2. The micro-blog text is preprocessed to form a word set W<math><mi is="true">W</mi></math>;3. For (Each word in the word set W<math><mi is="true">W</mi></math>)4. Calculate the support degree Sup(w)<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math> of word.5. If (Sup(w),α<math><mrow is="true"><mi is="true">S</mi><mi is="true">u</mi><mi is="true">p</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">w</mi><mo stretchy="true" is="true">)</mo></mrow><mi is="true">,</mi><mi is="true">α</mi></mrow></math>) Delete the word;6. Else Join the node set V<math><mi is="true">V</mi></math>.7. End If8. End For9. Calculate the complex co-occurrence rate P(vi,vj)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">v</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow></mrow></math> between feature words10. If (P(vi,vj),β<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">v</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo stretchy="true" is="true">)</mo></mrow><mi is="true">,</mi><mi is="true">β</mi></mrow></math>) vi,vj<math><mrow is="true"><msub is="true"><mi is="true">v</mi><mtext is="true">i</mtext></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub></mrow></math> are not connected;11. Else Add eij<math><mrow is="true"><msub is="true"><mi is="true">e</mi><mtext is="true">ij</mtext></msub></mrow></math> to the edge set E<math><mi is="true">E</mi></math>;12. End If13. End
Algorithm 1 design description.
Steps 1–13 are used to construct micro-blog hot topic complex network G<math><mi is="true">G</mi></math>;
Steps 3–8 are used to extract the feature words of a micro-blog hot topic and calculate the support of the words. When the support is less than the set threshold, the word is considered not to be a feature word, so the word is deleted. Otherwise, the word is added to the node set;
Steps 9–12 are used to construct the edges of a hot topic complex network. First, in step 9, the weight of the edge (the complex co-occurrence rate between the feature words) is calculated. Then, if the weight is less than the set threshold, the two feature words are not associated. Otherwise, The two feature words can be connected with one edge. The edge can be added to the edge set E<math><mi is="true">E</mi></math>.
Calculating the support of n<math><mi is="true">n</mi></math> words and comparing it with the threshold α<math><mi is="true">α</mi></math> requires roughly O(2n)<math><mrow is="true"><mi is="true">O</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mn is="true">2</mn><mi is="true">n</mi></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math> time. Calculating the co-occurrence rate between k<math><mi is="true">k</mi></math> feature words and comparing it with the threshold β<math><mi is="true">β</mi></math> requires approximately O(n2−n)<math><mrow is="true"><mi is="true">O</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msup is="true"><mi is="true">n</mi><mn is="true">2</mn></msup><mo linebreak="badbreak" is="true">−</mo><mi is="true">n</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math> time. Hence, the time complexity of the algorithm is O(n2)<math><mrow is="true"><mi is="true">O</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msup is="true"><mi is="true">n</mi><mn is="true">2</mn></msup><mo stretchy="true" is="true">)</mo></mrow></mrow></math>.
In this section, we mainly consider two aspects: One aspect is to assess the performance of complex networks multi-subtopic division by appropriate measurement standard, while another is to find the optimal evaluation index and get the multi-subtopic division result by the effective algorithm.
The modularity function is a measurement standard proposed by Newman to evaluate the strengths and weaknesses of complex network community structures. The modularity function can measure whether a semantic community division is a good result under the premise that it is difficult to determine the number of semantic communities first.
The co-occurrence rate between feature words is used to build a complex network. The difference in the complex co-occurrence rate caused by the different tightness of the connection between different feature words is considered. An improved weighted modularity function is proposed based on the complex co-occurrence rate between feature words so that the words with close internal relations can be more effectively assigned to the same semantic community. The formula for improving the modularity function Q<math><mi is="true">Q</mi></math> is as follows:(9)Q=∑c∈C(LcM−(Dc2M)2)<math><mrow is="true"><mi is="true">Q</mi><mo linebreak="badbreak" is="true">=</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">c</mi><mo linebreak="badbreak" is="true">∈</mo><mi is="true">C</mi></mrow></munder><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mrow is="true"><msub is="true"><mi is="true">L</mi><mi is="true">c</mi></msub></mrow><mi is="true">M</mi></mfrac><mo linebreak="badbreak" is="true">−</mo><msup is="true"><mrow is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mfrac is="true"><mrow is="true"><msub is="true"><mi is="true">D</mi><mi is="true">c</mi></msub></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">M</mi></mrow></mfrac></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow><mn is="true">2</mn></msup></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></mrow></math>where M<math><mi is="true">M</mi></math> represents the sum of complex co-occurrence rates for all edges in the complex network; Lc<math><mrow is="true"><msub is="true"><mi is="true">L</mi><mtext is="true">c</mtext></msub></mrow></math> indicates the sum of complex co-occurrence rates for all edges in the semantic community c<math><mi is="true">c</mi></math>; Dc<math><mrow is="true"><msub is="true"><mi is="true">D</mi><mtext is="true">c</mtext></msub></mrow></math> represents the sum of all node degrees in the semantic community c<math><mi is="true">c</mi></math>.
The larger the improved weighted modularity value Q<math><mi is="true">Q</mi></math>, the more obvious the semantic community structure of the network, and the better the division effect. So when Q<math><mi is="true">Q</mi></math> reaches its maximum, the semantic community division achieves the best results.
The genetic algorithm is a random adaptive global search algorithm to calculate the maximum of weighted modularity, while its operator can provide an optimal solution or satisfactory solution for complex optimization problems. Micro-blog hot topic complex network division algorithm takes the weighted modularity Q<math><mi is="true">Q</mi></math> as the fitness function, and the weighted modularity maximum Q<math><mi is="true">Q</mi></math> can be computed by the operations (selection, crossover, or others) to obtain the division results of the corresponding semantic community.
The detailed steps are shown as follows.
Initialization. Set the evolutionary algebraic counter t=0<math><mrow is="true"><mi is="true">t</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn></mrow></math>, set the maximum evolution algebra T<math><mi is="true">T</mi></math> and randomly generate the first-generation group P(0)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mn is="true">0</mn><mo stretchy="true" is="true">)</mo></mrow></mrow></math>.
Individual evaluation. Weighted modularity Q<math><mi is="true">Q</mi></math> is used as a fitness function to measure the fitness of individuals in the population and calculate the fitness of every individual Q<math><mi is="true">Q</mi></math> in the population P(t)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>.
Selection operation. The selection operator is applied to the population to inherit the optimized individuals directly to the next generation or to produce new individuals through pairing crossover and then to the next generation. The selection operation is based on the fitness evaluation of individuals in a population.
Crossover operation. Crossover operator is applied to population and it plays a central role in genetic algorithm.
Mutation operation. Applying a mutation operator to a population changes the gene values at individual loci in the population. On the one hand, this operation can make the genetic algorithm have the capabilities of local random search, on the other hand it can maintain the diversity of the population.
Get the next generation P(t+1)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo linebreak="badbreak" is="true">+</mo><mn is="true">1</mn><mo stretchy="true" is="true">)</mo></mrow></mrow></math>.The group P(t)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math> can get the next generation P(t+1)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo linebreak="badbreak" is="true">+</mo><mn is="true">1</mn><mo stretchy="true" is="true">)</mo></mrow></mrow></math> after selection, crossover and mutation operations.
The judgment of termination condition. If t>T<math><mrow is="true"><mi is="true">t</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">&gt;</mo><mi is="true">T</mi></mrow></math>, the individual with the maximum fitness will be obtained as the output of the optimal solution in the evolutionary process and then the calculation is terminated.
The main steps of the algorithm are shown in Algorithm 2.
Step 1–2: parameters such as k<math><mi is="true">k</mi></math>K<math><mi is="true">K</mi></math>, Pc<math><mi is="true">P</mi><mi is="true">c</mi></math>Pm<math><mrow is="true"><mi is="true">P</mi><mi is="true">m</mi></mrow></math>M<math><mi is="true">M</mi></math>T<math><mi is="true">T</mi></math>, etc are initialized and the first generation population P(t)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math> was randomly generated. Where, k<math><mi is="true">k</mi></math> denotes the number of semantic communities and is assigned to 2, K<math><mi is="true">K</mi></math> represents the maximum number of semantic communities, Pc<math><mrow is="true"><mi is="true">P</mi><mi is="true">c</mi></mrow></math> indicates the probability of crossover occurrence, M<math><mi is="true">M</mi></math> means the population size and T<math><mi is="true">T</mi></math> is valued as the algebra of terminates evolution.
Step 3–20: when the number of semantic communities is the most, the algorithm terminates. And then, the maximum value of module degree and the result of the multi-subtopic division of topic complex network are obtained. Otherwise, continue the operations in steps 4–19.
Step 4–19: when the number of iterations is T<math><mi is="true">T</mi></math>, the evolution ends. Otherwise, continue the operations in steps 7–16 to continuously update the population Pnew(t)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">w</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;
Step 7–16: two individuals are selected based on the fitness selection ratio Ps=Qs/∑Q<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mi is="true">s</mi></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msub is="true"><mi is="true">Q</mi><mi is="true">s</mi></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">/</mo><mo is="true">∑</mo><mi is="true">Q</mi></mrow></math>first, so then a uniform random number between 0 and 1 is generated. If the random number is less than the crossover probability Pc<math><mrow is="true"><mi is="true">P</mi><mi is="true">c</mi></mrow></math>, two individuals will have the operation of crossover. Next, another random number between 0 and 1 is generated. If the random number is less than the mutation probability Pm<math><mrow is="true"><mi is="true">P</mi><mi is="true">m</mi></mrow></math>, the mutation will happen anywhere. At last, two new individuals are added to Pnew(t)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">w</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>.
The algorithm needs to calculate a limited number of K<math><mi is="true">K</mi></math>semantic communities and nest internal genetic algorithms, which is a double iterative algorithm. So, the overall time complexity is roughly O(n2)<math><mrow is="true"><mi is="true">O</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msup is="true"><mi is="true">n</mi><mn is="true">2</mn></msup><mo stretchy="true" is="true">)</mo></mrow></mrow></math>.Algorithm 2. Multi-subtopic division algorithm for micro-blog topic complex networkInput: Network G=(V,E)<math><mrow is="true"><mi is="true">G</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>corresponding adjacency matrixOutput: Weighted modularity maximum Qmax<math><mrow is="true"><msub is="true"><mi is="true">Q</mi><mi is="true">max</mi></msub></mrow></math>and its corresponding semantic community division1. Parameters such as k<math><mi is="true">k</mi></math>K<math><mi is="true">K</mi></math>, Pc<math><mrow is="true"><mi is="true">P</mi><mi is="true">c</mi></mrow></math>Pm<math><mrow is="true"><mi is="true">P</mi><mi is="true">m</mi></mrow></math>M<math><mi is="true">M</mi></math>,T<math><mi is="true">T</mi></math>, etc should be initialized;2. Randomly generate the first generation population P(t)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;3. For (k=2;k++;k,K<math><mrow is="true"><mi is="true">k</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn mathvariant="italic" is="true">2</mn><mo is="true">;</mo><mi is="true">k</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">+</mo><mo linebreak="goodbreak" linebreakstyle="after" is="true">+</mo><mo is="true">;</mo><mi is="true">k</mi><mi is="true">,</mi><mi is="true">K</mi></mrow></math>)4. For (t=1;t++;t,T<math><mrow is="true"><mi is="true">t</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn mathvariant="italic" is="true">1</mn><mo is="true">;</mo><mi is="true">t</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">+</mo><mo linebreak="goodbreak" linebreakstyle="after" is="true">+</mo><mo is="true">;</mo><mi is="true">t</mi><mi is="true">,</mi><mi is="true">T</mi></mrow></math>)5. Calculate each individual fitness Q<math><mi is="true">Q</mi></math>in the population P(t)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;6. Initialize the empty population Pnew(t)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">w</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;7. For (population size M<math><mi is="true">M</mi></math>progeny)8. Select two individuals from the population P(t)<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;9. If(random(0,1),Pc<math><mrow is="true"><mi is="true">r</mi><mi is="true">a</mi><mi is="true">n</mi><mi is="true">d</mi><mi is="true">o</mi><mi is="true">m</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mn mathvariant="italic" is="true">0</mn><mo is="true">,</mo><mn mathvariant="italic" is="true">1</mn><mo stretchy="true" is="true">)</mo></mrow><mi is="true">,</mi><mi is="true">P</mi><mi is="true">c</mi></mrow></math>)10. Perform cross-operation on the two individuals according to the crossover probability Pc<math><mrow is="true"><mi is="true">P</mi><mi is="true">c</mi></mrow></math>;11. End If12. If(random(0,1),Pm<math><mrow is="true"><mi is="true">r</mi><mi is="true">a</mi><mi is="true">n</mi><mi is="true">d</mi><mi is="true">o</mi><mi is="true">m</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mn mathvariant="italic" is="true">0</mn><mo is="true">,</mo><mn mathvariant="italic" is="true">1</mn><mo stretchy="true" is="true">)</mo></mrow><mi is="true">,</mi><mi is="true">P</mi><mi is="true">m</mi></mrow></math>)13. Perform mutation operations on the two individuals according to the mutation probability Pm<math><mrow is="true"><mi is="true">P</mi><mi is="true">m</mi></mrow></math>;14. End If15. Add two new individuals to the Pnew(t)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">w</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;16. End For17.Pnew(t)=Pnew(t)∪P(t)<math><mrow is="true"><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">w</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msub is="true"><mi is="true">P</mi><mrow is="true"><mi is="true">n</mi><mi is="true">e</mi><mi is="true">w</mi></mrow></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">∪</mo><mi is="true">P</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">t</mi><mo stretchy="true" is="true">)</mo></mrow></mrow></math>;18. Output Qmax<math><mrow is="true"><msub is="true"><mi is="true">Q</mi><mi is="true">max</mi></msub></mrow></math>and its corresponding multi-subtopic division;19. End For20. End For
To verify the effectiveness of the method proposed, two experiments are performed to verify the effectiveness of the feature word extraction method proposed and the performance of the constructed MSBN.
The API interface based on the Sina Weibo open platform can be used to get all the microblog texts for this topic. We crawled the relevant blogs of the two hot topics of “World Cup” and " Total lunar eclipse " through a crawler program, and obtained dataset 1 and dataset 2.
(1) Dataset 1:The hot topic is “World Cup”. We selected two-time intervals. One is from 22:00 on June 14, 2018 to 22:00 on June 15, 2018. The acquisition dates are 14:00 on June 15, 2018 and 1986 micro-blogs are returned. The other is from 22:00 on June 14, 2018 to 24:00 on June 15, 2018. The acquisition dates are 24:00 on June 15, 2018, and 8328 micro-blogs are returned. Feature words are extracted by manual methods. Examples of some feature words in dataset 1 are shown in Table 1.
(2) Dataset 2. The hot topic is “Lunch Whole Food”. We selected two time intervals. One is from 10:00 on July 27, 2018 to 2:00 on July 28, 2018. The acquisition dates are 22:00 on July 28, 2018 and 1342 micro-blogs are returned. The other is from 10:00 on July 27, 2018 to 8:00 on July 28, 2018. The acquisition dates are 8:00 on July 28, 2018 and 3754 micro-blogs are returned. Feature words are manually extracted. Examples of some feature words in dataset 2 are shown in Table 2.
We compare our methods of extracting feature words with the following state-of-the-art methods:
(1)A rapid mining model based on association semantic link (ASL), It extracts the keywords-level association semantic link first and then extracts feature words (Zhang and Lu, 2017).
(2)Feature word extraction model based on Markov conditional random field. First, use the random field to find the semantic core of the sentence, and then learn the association to remove redundant words to extract feature words (Liu et al., 2016; Nehe and Holambe, 2012).
The accuracy rate P, recall rate R, and F value are selected as the evaluation indexes for feature word extraction. As shown in Table 3, use P to represent the proportion of positive feature words among feature words predicted to be a specific topic. Calculated as follows:(10)P=rr+t<math><mrow is="true"><mi is="true">P</mi><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mi is="true">r</mi><mrow is="true"><mi is="true">r</mi><mo linebreak="badbreak" is="true">+</mo><mi is="true">t</mi></mrow></mfrac></mrow></math>
As in Table 3, R represents the proportion of feature words predicted as a topic to all true feature words. Calculated as follows:(11)R=rr+s<math><mrow is="true"><mi is="true">R</mi><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mi is="true">r</mi><mrow is="true"><mi is="true">r</mi><mo linebreak="badbreak" is="true">+</mo><mi is="true">s</mi></mrow></mfrac></mrow></math>
In order to comprehensively consider the accuracy rate P and the recall rate R, this article uses the weighted harmonic mean F of the two to measure the final extraction effect. Calculated as follows:(12)F=2∗P∗RP+R<math><mrow is="true"><mi is="true">F</mi><mo linebreak="badbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">2</mn><mo linebreak="badbreak" is="true">∗</mo><mi is="true">P</mi><mo linebreak="badbreak" is="true">∗</mo><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">P</mi><mo linebreak="badbreak" is="true">+</mo><mi is="true">R</mi></mrow></mfrac></mrow></math>
To verify the effectiveness of our proposed method, we performed experiments on dataset 1 and dataset 2 respectively. To detect the validity of the feature word extraction method and construct the MSBN at the same time, the specific experimental steps are as follows.
(1) Obtaining the data set:API interface based on the Sina Weibo open platform is used to get all the micro-blog texts of the topic. Among the micro-blog texts, a regular expression is used to retrieve text that conforms to the "#topic#" rule further.
(2) Data preprocessing:the micro-blog text is preprocessed to remove irrelevant data such as stop words and emoticons, and “Jieba” Chinese text segmentation is used to segment the words。
(3) Construct the MSBN:Use three different feature word extraction algorithms to extract topic feature words, compare the extraction results of different methods, and further use the method proposed in this paper to construct MSBN.
According to the experimental method of 5.1, we do the following two experiments.
The first experiment: The hot topic is “World Cup”. First, we use the precise pattern of the “Jieba” Chinese text segmentation to complete the segmentation of data sets. Second, equation (4) is used to calculate the support of each word in the micro-blog data set. And then, the support degree threshold of the selected word is assigned to 0.01 according to the analysis for the rank of word support degree. Here, the words whose support degree exceeds the threshold are defined as the hot topic feature words of the time interval. We compare our method with two baseline methods. The results are shown in Fig. 3.
From Fig. 3, we note that the accuracy and recall of the feature words extracted by the method proposed are high, better expressing the topic content of the current time period. The topic contents comply well with the theme of the hot search words, given by micro-blog platform “micro-sentiment”. Besides, they can also expand the topic further. At this point, we make use of the experimentally obtained feature words as nodes and construct the micro-blog hot topic complex network based on the method in subsection 3.3. To highlight the close connection among the feature words from a global perspective, the adjustment factor is presupposed as 0.6, 0.7, 0.8 respectively. Then, when the complex co-occurrence rate threshold takes a different value, the weighted modularity maximum can be calculated by Algorithm 2 and the results are shown in Fig. 4.
According to Fig. 4, when the complex co-occurrence rate threshold increases from 0, the weighted modularity increases and the division effect of the complex network semantic community are better. Though, when the complex co-occurrence rate threshold reaches a certain level, the weighted modularity begins to decrease and the division effect of the complex network semantic community becomes worse. Besides, we can find that the weighted modularity of the complex network constructed by the feature words in time interval 1 up to the maximum value 0.421 when β1=0.12<math><mrow is="true"><msub is="true"><mi is="true">β</mi><mn is="true">1</mn></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0.12</mn></mrow></math>and the weighted modularity of the complex network constructed by the feature words in time interval 2 up to the maximum value 0.416 whenβ2=0.14<math><mrow is="true"><msub is="true"><mi is="true">β</mi><mn is="true">2</mn></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0.14</mn></mrow></math>. Meanwhile, when the weighted modularity takes the maximum value, the semantic community division result can be output correspondingly by algorithm 2 and the first level of MSBN be drawn as Fig. 4. In Fig. 4, each subtopic is represented by a different color. The larger the node, the larger the degree of the node, and the thicker the edge, the closer the relationship between the two points of the edge connection.
From Fig. 5, each micro-blog hot subtopic of this model can represent the topic that users focus on. For example, “2018 World Cup opening battle between Russia and Saudi Arabia ", “The performance of Ronaldo and Williams at Luzhniki Stadium” and so on what the users focus on are summarized through the artificial overview in time interval 1. Then, the hot topic that people pay attention to is shown by the MSBN model intuitively. Compared the two graphs in Fig. 5, we can see that the importance of many feature words in time interval 1 has been weakened and many new feature words are mined with the increasing of time. Meanwhile, what the users concerned about is also shifting. For example, users are more concerned about “The content and results of the competition in Uruguay and Egypt”, “Buy lottery tickets during the World Cup to gamble” and other subtopics in time interval 2.
Further, semantic communities that include keywords, such as “2018, Russia, World Cup”, are selected, and the location information when the micro-blog is published in the community is analyzed. The second level of MSBN is shown in Fig. 6, the darker the color the higher the attention.
The second experiment: The hot topic is “Lunch Whole Food”. Take the same steps as the first experiment. Due to the higher novelty of this topic about Mars's total lunar eclipse, the custom dictionary is added to the “Jieba” Chinese text segmentation to ensure higher feature recognition efficiency first. In the Custom dictionary, the development time like July 28, July 27 and the new words like Blood Moon, Mars Dachong can be obtained. Then the data set is segmented based on the precise pattern of the “Jieba” Chinese text segmentation. And the support degree threshold of the selected word is assigned to 0.015 according to the analysis for the rank of word support degree. Here, the words whose support degree exceeds the threshold are defined as the hot topic feature words of the time interval. We compare our method with two baseline methods. The results are shown in Fig. 7.
From Fig. 7, we can find that the accuracy and recall of the feature words extracted by the method proposed are high. Similar to experiment 1, the adjustment factor is preset as 0.6, 0.7, 0.8 respectively. Next, the weighted modularity maximum is calculated by Algorithm 2 and the results depicted in Fig. 8.
According to Fig. 8, when the complex co-occurrence rate threshold increases from 0, the weighted modularity increases and the division effect of the complex network semantic community are better. Though, as the complicated co-occurrence rate threshold reaches a certain level, the weighted modularity starts to decrease and the division effect of the complex network semantic community becomes worse. Besides, we can find that the weighted modularity of the complex network constructed by the feature words in time interval 3 up to the maximum value 0.542 when β1=0.14<math><mrow is="true"><msub is="true"><mi is="true">β</mi><mn is="true">1</mn></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0.14</mn></mrow></math>and the weighted modularity of the complex network constructed by the feature words in time interval 4 up to the maximum value 0.566 whenβ2=0.16<math><mrow is="true"><msub is="true"><mi is="true">β</mi><mn is="true">2</mn></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0.16</mn></mrow></math>. Still, when the weighted modularity takes the maximum value, the semantic community division result can be output correspondingly by Algorithm 2, and the first level of MSBN is drawn as Fig. 9, where a different color represents each subtopic. The larger the node, the larger the degree of the node, and the thicker the edge, the closer the relationship between the two points of the edge connection.
Each micro-blog hot subtopic of this model can represent the topic that users are focused in this figure. For instance, in the following clauses“The longest total lunar eclipse in this century”, “Everyday in the morning of July 28th”, and“Whether the weather conditions during the total lunar eclipse are suitable for taking pictures”, the info users are focused on are summarized through the artificial overview in time interval 3. Moreover, the hot topic that people pay attention to is shown by the MSBN model intuitively. Comparing the two charts in Fig. 9, we can see that the importance of many feature words in time interval 3 is weakened. Moreover, several new feature words are mined with the increasing of time. Meanwhile, what the users concerned about is also shifting. For example, users are more concerned about subtopics like “the progress of total lunar eclipse at various time points” in time interval 4. Further, semantic communities that include keywords, such as “2018, Russia, World Cup”, are selected, and the location information when the micro-blog is published in the community is analyzed. As shown in Fig. 10, the darker the color the more significant the attention in the second level of MSBN.
From the two experiments shown above, the first level of MSBN proposed can help users to understand the discussion direction of hot topics intuitively and the drift phenomenon of the initial center with the increasing of time. The second level of MSBN can help users or relevant departments to understand which areas are most intensely discussed for the hot topic. This plays an important role in understanding the development process of the topic and controlling the development of online public opinion.
To support users to timely and comprehensively understand the multi-subtopic overview of the hot topics in a certain period of time and particular area, a new MSBN based on the feature words co-occurrence and the complex network semantic community division is proposed, where natural language processing and data mining techniques are applied to the micro-blog platform. The paper has completed the following aspects:
The micro-blog hot topic feature words are effectively extracted and micro-blog hot topic complex network effectively constructed. First, combined with the time factor of micro-blog release with the number of micro-blog users' comments and praise points, the feature words are extracted. Second, the complex co-occurrence rate is introduced to define the co-occurrence of feature words, so then a complex network of the micro-blog hot topic is constructed,
MSBN is successfully constructed. A new weighted modularity formula that is more suitable for the investigation of complex network semantic community division is proposed to measure the degree of semantic community division, so then the maximum value of the weighted modularity and the result of semantic community division are obtained by genetic algorithm to construct the first level of MSBN. The location information when the micro-blog is published in different communities is analyzed to build the second level of MSBN.
Experimental results show that the MSBN can represent the subject of a hot topic in each period of time or location to help users understand the multi-subtopic of micro-blog hot topic discussion timely and comprehensively. In the future, the MSBN based on feature co-occurrence and semantic community division can also serve as the foundation for other related researches (such as topic representation, topic tracking, social Internet of Things, and others).
Guangli Zhu: Conceptualization, Methodology, Investigation, Supervision. Zhuangzhuang Pan: Data curation, Visualization, Writing - original draft. Qiaoyun Wang: Software, Validation. Shunxiang Zhang: Supervision, Project administration, Funding acquisition. Kuan-Ching Li: Writing - review & editing.
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
This research work was supported in part by the Anhui Provincial Natural Science Foundation Project (No.: 19808085 MF189) and the Anhui University Top Talent Cultivation Project (No. GxbjZD15). Natural Science Research Project of Anhui College (No. KJ 2018A0285, KJ2018A0084) by the China Coal Science and Industry Group - Tiandi Science and Technology Youth Fund project (No: 2018TDQN019).