Automatic text summarization is a well-established subfield of natural language processing, which is relevant for a number of scenarios [39], [66]. A summary can be seen as a condensed representation of a source text that maintains the important information of its original counterpart [65]. When a summary is constructed by selecting and juxtaposing source pieces, such as sentences, it is called an extract. The summary produced with changes in the surface form of the source text, resulting from generalizations or paraphrases, is referred to as an abstract. Therefore, humans usually produce abstracts. Systems for generating abstracts require sophisticated resources such as discourse and semantic analyzers to infer the meaning of the source text, as well as language generators to compose the summary [42]. Extractive summarizers, on the other hand, do not require linguistic knowledge to select the most relevant pieces of the source text for an extract. Various approaches exist for extractive summarization, including the use of word frequency [19], [38], cue words or phrases [19], [52], machine learning [30], lexical chains [7] and sentence compression through syntactical or statistical restrictions [67], [70]. Less familiar approaches include the application of psychology models to extractive summarization [24] and the use of a hierarchical fractal-based model for representing and condensing texts [69]. Although extractive summarization can produce texts that have cohesion and coherence problems, many systems have been proven to yield summaries whose informative level is satisfactory. This is particularly true when the extract is used as a component of another system (e.g. in information retrieval [61]) and is not directly used by humans. Nevertheless, despite the efforts to develop high-quality automatic summarizers, improvement is required in a number of issues [66].
A graph, or network, is a representation that may capture text structure in various ways, being therefore suitable for extractive summarization. Network nodes (vertices) might represent words, sentences or paragraphs. Network edges (links) connect nodes according to some task-dependent criteria. For instance, if the sentences are linked when sharing complementary content about the same topic, one could argue that a relevant node for summarization is one that shares various edges with other nodes. Therefore, a network algorithm may be used to assign numerical values (relevance scores or ranks) to nodes and to select a subset of them (i.e. pieces of text) to compose an extract. In this paper, we investigate a graph-based, language-independent approach to extractive text summarization inspired by recent developments in the area of complex networks.
Complex networks have attracted a lot of attention [1], [9], [48] since the small-world [68] and scale-free [6], [22] properties were identified in many real-world networks about 10 years ago. The origins of this field can be traced back to the beginning of graph theory [17], passing through the development of random graph theory [20] and analysis of sociological experiments using graphs [44]. The recent discoveries contributed significantly to elucidate the structure and dynamics of diverse real-world entities such as the Internet, the brain and the natural languages. When represented as graphs, some of these entities contain a few highly connected nodes, called hubs, that coexist with a large number of much less connected nodes. These networks are said to be scale-free. Other networks reveal a relatively small distance between each pair of nodes, i.e. on average a small number of nodes separates them. This type of complex network, known as small-world, also shows high clustering, i.e. if two nodes are connected to the same node, the probability that they are connected to each other is high. These and other properties can be identified in a network through the computation of some graph measurements, such as the average node degree, the average clustering coefficient and the average length of shortest paths [16]. Another interesting feature of some complex networks is the existence of a community (or modular) structure, which allows a network to be partitioned into well-defined groups of highly interconnected nodes. With an increasing number of systems being treated as networks, the concepts of complex networks have been widely used in many disciplines (e.g. in computer science [26]). For linguistics, for example, word co-occurrence networks have been shown to be small-world and scale-free [23], a theory of language evolution was developed [18], and language development on an individual basis was assessed through network measurements [27]. Natural language processing has also benefited from complex networks, including authorship analysis [5] and quality assessment of high-school essays [4], summaries [53] and translations [3].
Here we address the design of extractive summarizers based on complex networks concepts. Our method uses a simple network of sentences that requires only surface text pre-processing, thus allowing us to assess extracts obtained with no sophisticated linguistic knowledge. Given a network representation of a source text, where each node corresponds to a source sentence and an edge links sentences that share common lemmatized nouns, the proposed method selects a subset of sentences (nodes) to compose an extract by ranking them according to some network measurement. Generic summaries, i.e. neither user-specific nor topic-oriented, were produced for newspaper articles in Brazilian Portuguese using network parameters such as shortest paths, k-cores and communities. Experiments to evaluate the informativeness level of the extracts were carried out, and the resulting scores for Rouge-1 [37] and Precision/Recall [62] were compared with the scores of other summarizers previously evaluated within the same experimental setup.
The remainder of this paper is organized as follows. Section 2 contains a review of related work on network-based extractive summarization. Section 3 introduces the network representation of a source text, as well as the graph measurements chosen to generate extracts. The evaluation setup and comparative results regarding informativeness scores are presented and discussed in Section 4. Section 5 complements the evaluation with a correlation analysis of the proposed systems. Finally, Section 6 brings concluding remarks and prospects for future work.
Several developments of summarization techniques based on graphs are reported in the literature. Salton et al. [63], for instance, denote paragraphs as nodes, which are interconnected according to a similarity measure based on the number of words they share. Routing algorithms were proposed to select the most prominent paragraphs. In an evaluation with a corpus of 50 texts in English, the best algorithm have chosen 45.6% of the paragraphs selected by human summarizers. Although simple, the approach based on paragraphs is limited by the compression rate, since paragraphs cannot be broken to fit into the extract.
In another approach, Mani and Bloedorn [41] represent instances of terms as nodes, which are connected by cohesion relations such as proximity, repetition, synonymy and co-reference. The extract generated must satisfy a topic given as input. After selecting nodes corresponding to terms of the topic, a spreading activation algorithm gives a weight to each node, and the topmost nodes indicate which text sentences should be selected to compose the extract. In an experiment with five texts in English, this algorithm has outperformed the tf.idf measurement [62] and the node degree, which is defined in Section 3.1.
In Mihalcea’s work [43], an extract is generated by selecting the sentences with the highest ranks given by recommendation algorithms developed to classify Web pages: Google’s PageRank [51] and HITS [28]. In a network of sentences, there is an edge between two nodes whenever they share common terms, and the number of shared terms corresponds to the edge weight. Differently from PageRank, HITS distinguishes the nodes with high indegree values (authorities – HITSA) from the nodes with high outdegree values (hubs – HITSH). Moreover, Mihalcea defines three types of networks: (i) Undirected, (ii) Forward, whose directed edges between sentences follow the text reading course, and (iii) Backward, with directed edges in the opposite course. The author has evaluated her proposal with the English corpus of DUC’2002 (Document Understanding Conference) [50] and with the Brazilian Portuguese corpus TeMário [54] according to the metric Rouge-1 [37]. For the networks with forward and backward edges, HITS algorithms were superior to the high-scoring system of DUC’2002, while PageRank for backward edges performed a little worse than these systems. The best three strategies of Mihalcea for Brazilian Portuguese texts were also considered in the evaluation of the method proposed in this paper. Detailed results are reported in Section 4.2.
A similar technique was employed by Erkan and Radev [21] in multi-document summarization. Two variations of the PageRank algorithm and a measure of degree centrality were grouped in a linear combination, following Edmundson’s approach [19], to compute node relevance in a sentence-based network linked by similarity. The sentence position and sentence length attributes were used in an evaluation with English texts of DUC’2003 and DUC’2004. Experimental results show that Erkan and Radev’s approach is among the best competing systems of these DUC editions.
The methods outlined in this section use different linguistic knowledge to establish node adjacency in a network. Although sophisticated resources have become available for languages such as English, this is not true for other languages, including Portuguese. Therefore, the method proposed in this paper aims at producing extracts using only shallow linguistic knowledge: we have used a lemmatizer and a part-of-speech tagger to determine the connectivity between sentences represented as nodes.
The technique presented here for automatically generating extracts is based on a set of network measurements typically applied to characterize complex networks [16]. The underlying assumption is that each measurement selected reflects some features of the source text that might be interesting for summarization. Complex networks concepts were considered potentially useful for the summarization task because they offer different, often complementary, views of a network, and thus can be used to highlight a subset of its nodes. For the sake of clarity and to focus the analysis mainly on the extractive algorithms, we employed a simple network that encodes one type of lexical cohesion: nodes represent sentences and there is an edge between two nodes if the corresponding sentences have at least one word in common (i.e. lexical repetition). Furthermore, only lemmatized nouns are considered, thus restricting the number of edges in the network, highly increased if other words are considered, which would make it more difficult to select a group of sentences. In subsidiary experiments, we observed that including verbs in our analysis did not improve the performance of the summarizers.
We also argue that if two sentences are connected in this network they probably convey complementary information about related topics, possibly about the same topic. We illustrate this idea with an example (see Fig. 1) taken from the corpus employed in our experiments (details about this corpus are given in Section 4). Note that the noun ‘USP’ (acronym for University of São Paulo) in Fig. 1 has been highlighted, which allowed us to verify that there must be at least one edge between every pair of nodes in the corresponding network of sentences (network not shown). Upon inspection one notes that each sentence conveys a unique piece of information regarding the topic ‘facilities offered by the university’, thus complementing all other sentences. Nevertheless, two sentences may contain redundant information. But we argue that sentences complement one another in most cases; otherwise, texts would be extremely repetitive. We believe that this reflects our everyday experience on reading and writing: we rarely write two sentences expressing nearly the same content. As our goal is to construct informative extracts, the concept of complementary sentences is crucial for the development of our summarization techniques.
The proposed method, called CN-Summ (Complex Networks-based Summarization), consists of four steps. In Step 1, the source text is pre-processed in a straightforward manner: sentence boundaries are recognized and nouns are lemmatized. Nouns are identified by the part-of-speech tagger MXPost [2], [58] and lemmatization is done by accessing the Brazilian Portuguese lexicon KLS [29], [49]. In Step 2, the resulting text is mapped into a network representation according to the adjacency and weight matrices of order N×N<math><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">N</mi></mrow></math> (where N is the number of nodes/sentences) explained in Appendix A. These matrices, respectively called A and W, are used in Step 3, which computes one of a set of network measurements (defined in the following sections). Thus, the result of Step 3 is the association of a numerical value, or rank, to each network node. In the last stage (Step 4) the first n nodes (i.e. sentences) of the ranking are selected to compose the extract. The number of sentences in the extract, n, depends on the compression rate, and the way the sentences are ranked yields one summarization strategy. In what follows we introduce 7 network measurements leading to 14 different sentence rankings, i.e. 14 different summarization strategies. One further version is also proposed, which combines the previous strategies in what we call a voting system. Roughly speaking, this combination puts into an extract the sentences best ranked by most of the 14 strategies.
In the first summarization strategy we try to identify informative nodes by using the number of sentences a node is connected to, i.e. its degree. The degree ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> of a node i is the number of other nodes connected to it. More specifically, ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> can be obtained from the adjacency matrix A as follows,(1)ki=∑j=1Naij.<math><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></munderover><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mtext is="true">.</mtext></math>If the elements of the matrix W are considered instead in Eq. (1), a slightly different type of degree, referred to as strength, is obtained,(2)si=∑j=1Nwij.<math><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></munderover><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mtext is="true">.</mtext></math>Therefore, the strength of a node i is the sum of the weights of the edges associated to that node (matrices A and W are defined in Appendix A). These two measurements, although simple and already known from graph theory, are frequently used to characterize complex networks (e.g. degree distribution and degree–degree correlations) [16]. Nodes with high ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> or si<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> share a considerable number of connections with other nodes, and are called hubs. They play an important role in complex network theory, being the hallmark of scale-free networks [6].
Since the network encodes some kind of lexical cohesion between sentences, a sentence that shares a large number of links with other sentences probably conveys relevant information that complements many other sentences. The n sentences with the highest ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> are selected to build an extract in the first summarization strategy – called CN-Degree. Similarly, si<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> can also be used to identify highly connected nodes, and the corresponding strategy – called CN-Strength – selects the sentences with the highest si<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">s</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> to compose the extract.
Measurements associated with distance usually take into account the global structure of a network [16]. One example of such measurements is the length of the shortest path between two nodes. A path is a sequence of non-repeating edges that leads one node to another, and the length of a path is the number of edges in the sequence. In contrast to the degree, this measurement considers not only the immediate neighbors of a node i, but also the nodes indirectly connected to it. These nodes are at a distance higher than 1 from node i, and this information may be important to identify indirect cohesive relations. Thus, the shorter these distances are, the stronger the corresponding sentence inter-relationships might be.
The length dij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> of the shortest path that connects nodes i and j can be calculated from the matrix A through various algorithms [11]. We take the mean length of the shortest paths that associate a node i to every other node in the network as a measurement of the overall accessibility of i. More formally, it is given by(3)spi=1N-1∑i≠jdij,<math><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">sp</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></mfrac><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">≠</mo><mi is="true">j</mi></mrow></munder><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mtext is="true">,</mtext></math>where dij=N<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><mi is="true">N</mi></mrow></math> if a path between i and j does not exist (recall that N is the number of nodes in the network). The absence of paths linking some pair of nodes can be observed in disconnected networks. For weighted networks, the distance between two nodes is usually taken as the sum of the edge weights contained in the shortest path linking them. In our case, it is necessary to change matrix W to put the most important edges, those with high weights, inside shortest paths. Thus, we have created weights inversely proportional to the original weights wij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math>. This step is necessary because an edge with a high weight significantly increases the length of a path, and thus may be frequently avoided by a shortest path algorithm, even if it represents a strong relation between two sentences. We then define two matrices, Wwc<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math> and Wwi<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math>, that are both modified versions of W. The first one has elements wijwc=0<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msubsup><mo is="true">=</mo><mn is="true">0</mn></mrow></math> if wij=0<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, or wijwc=wmax-wij+1<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msubsup><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">max</mi></mrow></msub><mo is="true">-</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">+</mo><mn is="true">1</mn></mrow></math> if wij>0<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">&gt;</mo><mn is="true">0</mn></mrow></math>, i.e. its weights complement the values of W taking the highest element of W(wmax)<math><mrow is="true"><mi is="true">W</mi><mspace width="0.35em" is="true"></mspace><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">max</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math> as reference. When considering Wwc<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math> in the shortest path calculations, the measurement given by Eq. (3) is calculated taking dij=N〈wwc〉<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><mi is="true">N</mi><mo stretchy="false" is="true">〈</mo><msup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup><mo stretchy="false" is="true">〉</mo></mrow></math> when there is no path between i and j (〈wwc〉<math><mrow is="true"><mo stretchy="false" is="true">〈</mo><msup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup><mo stretchy="false" is="true">〉</mo></mrow></math> is the average weight of Wwc<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math>). The other modified version of W, denoted by Wwi<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math>, has elements wijwi=0<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msubsup><mo is="true">=</mo><mn is="true">0</mn></mrow></math> if wij=0<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, or wijwi=1/wij<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msubsup><mo is="true">=</mo><mn is="true">1</mn><mo is="true">/</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> if wij>0<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">&gt;</mo><mn is="true">0</mn></mrow></math>, i.e. it takes the inverse of the weights of W. Eq. (3) can also be applied using matrix Wwi<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math>, this time defining dij=N〈wwi〉<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><mi is="true">N</mi><mo stretchy="false" is="true">〈</mo><msup is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup><mo stretchy="false" is="true">〉</mo></mrow></math> when there is no path between two nodes i and j. Because a node with low spi<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">sp</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> is close to the other nodes on average, we now define a summarization strategy that selects for the extract the n nodes with the lowest values of spi<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">sp</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>. With the matrices A,Wwc<math><mrow is="true"><mi is="true">A</mi><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math> and Wwi<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math> in the computation of spi<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">sp</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>, we obtain three different summarizers, identified, respectively, by CN-SP, CN-SPwc<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math> and CN-SPwi<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math>.
The locality index is a measurement that takes into account the pattern of connectivity in the neighborhood of a node [15]. If the neighbors of a node i share few connections with the remaining nodes of the network, they have a localized connectivity pattern and therefore are more strongly related with i than with other nodes. The locality index is useful for pointing out these central nodes of relatively isolated groups, which can represent important sentences that summarize the meaning of their neighbors.
The locality index compares the number of internal connections of the neighbors of a node i (denoted by Niint<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup></mrow></math>) with the number of their external connections (denoted by Niext<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup></mrow></math>). More specifically, Niint<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup></mrow></math> is the number of edges between the ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> neighbors of i, plus the ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> edges that connect node i to its neighbors. Niext<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup></mrow></math> sums up the connections that the ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> neighbors of i maintain with the remaining nodes of the network. Hence, the locality index associates Niint<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup></mrow></math> and Niext<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup></mrow></math> in the following manner,(4)li=NiintNiint+Niext,<math><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup></mrow></mfrac><mtext is="true">,</mtext></math>where 0⩽li⩽1<math><mrow is="true"><mn is="true">0</mn><mo is="true">⩽</mo><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">⩽</mo><mn is="true">1</mn></mrow></math> (if Niint=Niext=0<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup><mo is="true">=</mo><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, then li=0<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>). If Niext=0<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup><mo is="true">=</mo><mn is="true">0</mn></mrow></math> and Niint>0,li<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup><mo is="true">&gt;</mo><mn is="true">0</mn><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> takes its maximum value, i.e. the pattern of connections surrounding node i is local. Conversely, li<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> approaches zero when Niext<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ext</mi></mrow></msubsup></mrow></math> is much higher than Niint<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">int</mi></mrow></msubsup></mrow></math>, which indicates that the neighbors of i are more connected with other nodes than with each other.
Therefore, it is possible to think of a node i with high li<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> as the center of a group of nodes that share few connections with the remaining nodes of the network. Since each of these groups is almost isolated from the rest, they may contain sentences of topics that are not considered by other sentences. More informative extracts might then be built if one sentence of each of those groups is selected, thus covering the topic structure of a text and also avoiding topic redundancy. Thus, another strategy for summarization, called CN-LI, gives priority to the central nodes of the groups mentioned, by selecting the top n sentences sorted in decreasing order of li<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">l</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>.
A d-ring is a subgraph generated by the morphological operation dilation [14], [25]. Dilation δ(g)<math><mrow is="true"><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></math> of a subgraph g of a graph G is another subgraph that includes the nodes of g plus the nodes connected to g. When this operation is performed d times on g, we obtain its d-dilation,(5)δd(g)=δ(δ(…(g)…))︸d,<math><msub is="true"><mrow is="true"><mi is="true">δ</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><munder is="true"><mrow is="true"><munder accentunder="true" is="true"><mrow is="true"><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><mo is="true">…</mo><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo><mo is="true">…</mo><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mo stretchy="true" is="true">︸</mo></mrow></munder></mrow><mrow is="true"><mi is="true">d</mi></mrow></munder><mtext is="true">,</mtext></math>where δ0=g<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">δ</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">=</mo><mi is="true">g</mi></mrow></math>. Finally, the d-ring of g is a subgraph Rd(g)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></math> whose nodes are(6)⧹N(δd(g))⧹N(δd-1(g)),<math><mi mathvariant="script" is="true">N</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">δ</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo><mo is="true">⧹</mo><mi mathvariant="script" is="true">N</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">δ</mi></mrow><mrow is="true"><mi is="true">d</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext></math>where the symbol ⧹ denotes the set difference operation, N()<math><mrow is="true"><mi mathvariant="script" is="true">N</mi><mo stretchy="false" is="true">(</mo><mo stretchy="false" is="true">)</mo></mrow></math> is a function that gives the set of vertices of a graph and R0=g<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mn is="true">0</mn></mrow></msub><mo is="true">=</mo><mi is="true">g</mi></mrow></math>. The d-ring has the nodes of δd(g)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">δ</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></math> that are not included in δd-1(g)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">δ</mi></mrow><mrow is="true"><mi is="true">d</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></math>, and it is also called hierarchical level d of subgraph g. For our purposes, we consider that g always has only one single node, thus we denote a d-ring by Rd(i)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo></mrow></math> instead of Rd(g)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></math>.
The d-rings generalize the concept of node neighborhood (see also [12], [13]). The first-level neighborhood of a node i, i.e. its ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> neighbors, corresponds to the 1-ring; its second-level neighborhood (the neighbors of its neighbors) corresponds to the 2-ring, and so forth. This notion of ‘concentric’ subgraphs is useful for selecting sentences that complement the central idea of a text. Thus we initially select for the extract an important node, the one with the highest degree ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>, called hub, and then select all nodes of its d-rings, from d=1<math><mrow is="true"><mi is="true">d</mi><mo is="true">=</mo><mn is="true">1</mn></mrow></math> to d=dmax<math><mrow is="true"><mi is="true">d</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">max</mi></mrow></msub></mrow></math>, where dmax<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">max</mi></mrow></msub></mrow></math> is defined according to the compression rate of the extract. This is the basis of the following three strategies: (i) the first, called CN-Ringsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>, uses the sentence location to select those sentences that appear first in the text when it is not possible to entirely include in the extract the outermost Rd(hub)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi mathvariant="italic" is="true">hub</mi><mo stretchy="false" is="true">)</mo></mrow></math> (the one with d=dmax<math><mrow is="true"><mi is="true">d</mi><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">d</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">max</mi></mrow></msub></mrow></math>); (ii) the second, called CN-Ringsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, uses the degree to extract the sentences with the highest ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> when it is not possible to completely include the outermost Rd(hub)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi mathvariant="italic" is="true">hub</mi><mo stretchy="false" is="true">)</mo></mrow></math>; (iii) the third, called CN-Ringslk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">lk</mi></mrow></msup></mrow></math>, selects from every Rd(hub)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">R</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi mathvariant="italic" is="true">hub</mi><mo stretchy="false" is="true">)</mo></mrow></math> only the nodes with degree no lower than the average network degree, and also extracts the sentences that appear first in the source text when the outermost d-ring does not fit into the extract. Although d-rings are not exactly network measurements, they are also used to define the sentence ranking in Step 4 of the CN-Summ method.
It may be argued that selecting sentences related to the hub will probably produce a redundant and not broadly informative summary. An important assumption we have made in the design of CN-Summ, and that was explained previously in this paper (Section 3), is that if two sentences are connected in the network they probably complement each other, rather than providing the same meaning. Therefore, the ring-based summarizers, as well as the ones to be presented in the two following sections (based on k-cores and w-cuts), aim at complementing the core information of the text.
A subgraph g of a graph G is a k-core if every node i of g has degree ki⩾k<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">⩾</mo><mi is="true">k</mi></mrow></math>. This subgraph, denoted by corek(G)<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">core</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math>, must also be the greatest subgraph of G that has this property [8]. The assumption here is that a k-core represents a set of closely related sentences. To obtain a corek(G)<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">core</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math>, it suffices to recursively exclude every node of G whose degree is lower than k. For the purposes of summarization, we also define a slightly different version of the k-core, denoted by corek′(G)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi mathvariant="italic" is="true">core</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math>, which is the largest connected component of corek(G)<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">core</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math> (for every pair of nodes in a connected component there exists at least one path that connects them).
A non-empty corek′(G)<math><mrow is="true"><msubsup is="true"><mrow is="true"><mi mathvariant="italic" is="true">core</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mo is="true">′</mo></mrow></msubsup><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math> with the maximum possible k, called the innermost k-core, is a subgraph that consists of densely connected nodes. Extending to subgraphs the interpretation given to the hub in Section 3.4, we now assume that the innermost k-core is relevant for summarization because it seems to be a nuclear group of sentences that express the main idea of the source text. We then define a new summarizing procedure that initially includes in the extract all the nodes that belong to the innermost k-core. Instead of using d-rings as in the previous section, we keep on calculating k-cores with sequentially decreasing k. The sentences of each new k-core are also included in the extract, except the ones that have already been inserted, until the compression rate is fulfilled. Notice that this algorithm gradually relaxes k-cores, thus allowing the main k-core to be complemented with other sentences. Two variations are proposed: (i) CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>, that selects the sentences of a k-core that appear first in the source text when the last k-core is not allowed to be included completely in the extract because of compression rate restrictions, and (ii) CN-Coresk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, that instead selects the sentences with the highest degrees ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>.
Inspired by the idea behind k-cores, we defined another type of subgraph called w-cut. The k-core is used to find nuclear groups of nodes using only node degrees, while w-cut is defined to identify groups of closely related nodes using edge weights. We require that the w-cut of a graph G, identified by cutw(G)<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">cut</mi></mrow><mrow is="true"><mi is="true">w</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math>, be a subgraph whose edge weights wij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> are not lower than w. Moreover, cutw(G)<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">cut</mi></mrow><mrow is="true"><mi is="true">w</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">G</mi><mo stretchy="false" is="true">)</mo></mrow></math> must be the greatest connected component of G after the exclusion of the edges that do not meet the weight criterion. The strategies based on w-cuts are analogous to the ones based on k-cores, from which two more strategies are obtained, namely CN-Cutsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> and CN-Cutsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>.
Another concept borrowed from the complex networks field is the notion of communities[16], which correspond to groups of nodes that are highly interconnected, while different groups are scarcely connected to each other. We argue that communities might correspond to the topics conveyed by the text. To make the definition of communities more accurate we use a quantity Q, called network modularity [10]. It is initially obtained from the following fraction:(7)∑ijaijδ(ci,cj)∑ijaij=12M∑ijaijδ(ci,cj),<math><mfrac is="true"><mrow is="true"><msub is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><msub is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></mfrac><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">M</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></munder><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext></math>that measures the proportion of edges connecting intra-community nodes, where ci<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> is the community number that node i belongs to, δ(a,b)<math><mrow is="true"><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><mi is="true">a</mi><mtext is="true">,</mtext><mi is="true">b</mi><mo stretchy="false" is="true">)</mo></mrow></math> is 1 if a=b<math><mrow is="true"><mi is="true">a</mi><mo is="true">=</mo><mi is="true">b</mi></mrow></math> or 0 if a≠b<math><mrow is="true"><mi is="true">a</mi><mo is="true">≠</mo><mi is="true">b</mi></mrow></math>, and M is the total number of edges in the network M=12∑ijaij<math><mrow is="true"><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mi is="true">M</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn></mrow></mfrac><msub is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></mfenced></mrow></mrow></math>. It is worth pointing out that this δ<math><mrow is="true"><mi is="true">δ</mi></mrow></math> has nothing to do with the one in Section 3.4. The modularity Q of a network G is obtained by subtracting from Eq. (7) its expected value in a random network,(8)Q=12M∑ijaij-kikj2Mδ(ci,cj),<math><mi is="true">Q</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">M</mi></mrow></mfrac><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></munder><mrow is="true"><mfenced open="[" close="]" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">-</mo><mfrac is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">M</mi></mrow></mfrac></mrow></mfenced></mrow><mi is="true">δ</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mtext is="true">,</mtext></math>where ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> is the degree of i,kj<math><mrow is="true"><mi is="true">i</mi><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></math> is the degree of j and kikj/2M<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">/</mo><mn is="true">2</mn><mi is="true">M</mi></mrow></math> is the probability that an edge (i,j)<math><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">i</mi><mtext is="true">,</mtext><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow></math> exists in a random network that preserves the degree of the nodes of G. When Q>0<math><mrow is="true"><mi is="true">Q</mi><mo is="true">&gt;</mo><mn is="true">0</mn></mrow></math> the modularity of G is greater than the expected for its random counterpart, and a value of Q greater than 0.3 indicates good modular structure in G. We adopted the greedy algorithm of Clauset et al. [10] to divide a network into communities. This procedure starts with N unitary communities (i.e. comprised of single nodes) and at each step it joins two communities to produce a new network partition with the highest possible value of Q, until only one community remains. Finally, the division of a network into communities adopted is the one that shows the highest Q calculated by the algorithm.
Unlike k-cores and w-cuts, a community structure associates every node with a single community. Therefore, a community division is a partition of a network, which can be seen as a set of interconnected subnetworks. For the purpose of summarization, communities supposedly represent the topic structure of the source text. Although we did not verify experimentally this assumption, the corresponding strategy, CN-Communities, aims at covering the entire topic structure of a text, thus avoiding topic redundancy. It selects a number of sentences from each community, which is proportional to the community size, to satisfy the compression rate. Finally, when a community is to be only partially included in the extract, preference is given to nodes with highest degree ki<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math>.
It is known that the combination of methods using a voting scheme can improve individual performances (e.g. combination of classifiers [31]). Thus, our last strategy, called CN-Voting, joins all previous strategies in an integrated voting approach, giving priority to the sentences that consistently appear at the top of the sentence rankings defined by each strategy. Therefore, CN-Voting needs to store all 14 sentence rankings. A ranking is the set of ordinal positions assigned to sentences by one strategy, i.e. the first sentence included in the extract has ordinal position 1, the second has ordinal position 2 and so forth, until the total of N sentences are selected and numbered. Therefore, the lower the sum Σ(s)<math><mrow is="true"><mi is="true">Σ</mi><mo stretchy="false" is="true">(</mo><mi is="true">s</mi><mo stretchy="false" is="true">)</mo></mrow></math> of all 14 positions of a given sentence s, the higher is the relevance attributed to it by most of the 14 voting strategies. The sentences selected by this voting approach should represent what the other strategies (or at least most of them) agree to be relevant for an extract. Since this is a position-based voting, the n sentences with the lowest Σ(s)<math><mrow is="true"><mi is="true">Σ</mi><mo stretchy="false" is="true">(</mo><mi is="true">s</mi><mo stretchy="false" is="true">)</mo></mrow></math> are selected to compose the extract, where n is defined by the compression rate.
Two evaluation experiments were carried out using TeMário corpus [54], which comprises 100 newspaper articles in Brazilian Portuguese with an average article size of 613 words, or 29 sentences. For each text there is a pair of reference summaries: an abstract written by a human and an automatically generated extract, created by the tool GEI [55]. GEI builds an extract by selecting source sentences that are closer to the sentences of the manually written abstract. The proximity is quantified by the cosine similarity measurement, computed between word frequency vectors. This automatically generated extract is therefore guided by the contents of the human abstract, making it an acceptable reference for evaluations as an approximation of a human summary [40]. Our experiments compare CN-Summ with other extractive summarizers previously evaluated with the same corpus. The first experiment determines Precision/Recall scores (Section 4.1), whereas the second experiment employs a Rouge metric (Section 4.2). Complementary experiments were also carried out to evaluate the effect of different compression rates on the performance of CN-Summ (Section 4.3). In order to illustrate the type of extracts obtained with CN-Summ, two examples taken from TeMário were included in a publicly available document.1
The first experiment uses the reference extracts to compute Precision, Recall and F-measure [62] for the extracts automatically generated by CN-Summ strategies. We denote the set of sentences of the reference extract by Er<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></mrow></math> and the set of sentences of the automatic extract by Ea<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub></mrow></math>. The Precision of Ea<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub></mrow></math> is P(Ea)=|Er∩Ea|/|Ea|<math><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">∩</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mo is="true">/</mo><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">|</mo></mrow></math> and its Recall is R(Ea)=|Er∩Ea|/|Er|<math><mrow is="true"><mi is="true">R</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">∩</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mo is="true">/</mo><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo stretchy="false" is="true">|</mo></mrow></math>, i.e. they relate the number of sentences that co-occur in both Er<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></mrow></math> and Ea<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub></mrow></math> to the total number of sentences of the automatic or reference extract, respectively. The F-measure adopted combines these two metrics in a balanced way, being defined by F(Ea)=[2P(Ea)R(Ea)]/[P(Ea)+R(Ea)]<math><mrow is="true"><mi is="true">F</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mo stretchy="false" is="true">[</mo><mn is="true">2</mn><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mi is="true">R</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">]</mo><mo is="true">/</mo><mo stretchy="false" is="true">[</mo><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo is="true">+</mo><mi is="true">R</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">E</mi></mrow><mrow is="true"><mi is="true">a</mi></mrow></msub><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">]</mo></mrow></math>. Extracts were obtained by removing 70% of the source sentences (a compression rate of 30%), following previous summarization experiments that also use the TeMário corpus [35], [60]. Hence, we are able to compare our results with published ones.
In this experiment, the 15 versions of CN-Summ are compared with two baselines and six other extractive systems. If n is the number of sentences of the extract, the first baseline system, called Top Baseline, includes in the extract the first n sentences of the source text, while the second, called Random Baseline, randomly selects n sentences of the source text. Other systems evaluated under the same experimental conditions are ClassSumm, NeuralSumm, GistSumm, TF-ISF-Summ, SuPor and its improved version SuPor-v2. ClassSumm [32] is an extractive summarizer that uses a machine learning algorithm to determine the most relevant segments of a text, similarly to the technique employed by Kupiec et al. [30]. Sentence features such as length, location, proper names, anaphor occurrence in the beginning of the sentence and cosine similarity to the title were used to induce a Naïve Bayes classifier [30], [34]. NeuralSumm [57] uses a SOM (Self-Organizing Map) neural network to classify text sentences as essential, complementary or superfluous. Essential sentences are selected to compose an extract, while the superfluous are not. The inclusion of complementary sentences depends on the compression rate. GistSumm [56] is an extractive summarizer that assigns a score to each sentence of the text according to one of two methods: keywords or average keywords. The first one scores each sentence according to the sum of the frequencies of its words. The second method normalizes the score of each sentence by its size, avoiding a bias for longer sentences in the summarization process. The highest scored sentence by any of the scoring methods is said to be the “gist sentence”, i.e. the sentence in the text that best expresses its main idea. Additional sentences are selected if they obey two criteria: correlation with the main idea, by sharing at least one word with the gist sentence; and relevance, by having a score above a threshold, which is computed as the average of all sentences scores in the text. Another system, called TS-ISF-Summ [33], computes sentence relevance by using the tf.isf metric, which is the well-known tf.idf metric of information retrieval [62] applied to a collection of sentences instead of a collection of documents. Topics are also detected and the chosen sentences of each topic are those containing the most frequent words of the topic. SuPor [59] employs a Naïve Bayes classifier to assign weights to sentences (i.e. the probability of being included in the extract), and computes features such as sentence length, word frequency, sentence location, occurrence of proper nouns and lexical chaining (this last feature uses the WordNet lexicon [45] and an ontology to identify semantic relationships between nouns). The top-ranked sentences are included in the extract. Its improved version, SuPor-v2 [35], can generate more informative extracts by handling sentence features in a flexible way. Moreover, it uses a feature selection algorithm to shrink the feature space. All these systems were previously evaluated with the same TeMário corpus of 100 texts [35], [60], and the average Precision, Recall and F-measure scores of each one are reproduced in Table 1. This table also includes the results of the two baseline systems, as well as the results of all CN-Summ strategies.
In what follows we discuss the average scores of all systems, while in the end of this subsection we analyze the statistical significance of the differences between average F-measures. Some of CN-Summ versions are among the best systems for Portuguese summarization, with F-measure around 42%. Considering only our systems, the best results were produced, as expected, by CN-Voting, which has higher average Precision, but lower average Recall, than the overall best system (SuPor-v2). CN-SPwc<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math> also presents a reasonable F-measure (42.4%), highlighting the importance of shortest paths in this experiment (CN-SP and CN-SPwi<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math> have a little lower F-measure). CN-Ringsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math> has a good F-measure (42.2%) as well, while other variations of ring-based strategies have lower scores. The degree seems to positively influence the performance of some systems, since the d-rings are obtained from the most connected node, and the systems that directly use this measurement (CN-Degree and CN-Strength) have good performance. Groups of highly interconnected nodes also appear to be relevant for summarization, as shown by the results of the strategies based on w-cuts and k-cores. Moreover, these systems are more positively influenced by the degree rather than by the sentence location, since their best variations are CN-Cutsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math> and CN-Coresk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>. The versions based on locality index (CN-LI) and communities (CN-Communities) achieved F-measures close to the score of the Top Baseline system. Perhaps the reason for this lower performance is the modest modularity of the networks (Q, defined in Section 3.7, is 0.26 on average), since these two strategies rely on the modular structure of a network.
When comparing our summarization approach with others, CN-Voting has higher Precision than ClassSumm, SuPor and SuPor-v2, and higher Recall than ClassSumm. All these other systems are based on machine learning, thus requiring a training phase, and use substantially more complex resources. Both versions of SuPor demand, for example, information from lexical chains (i.e. word semantics) and from importance of topics, while ClassSumm uses an argument structure of the source text. CN-Summ, on the other hand, does not employ any semantic information nor demands a training stage. Nevertheless, SuPor-v2 exhibited higher scores than the other systems, reinforcing the good performance of its previous version [60]. A remarkable result is that all CN-Summ versions outperformed TF-ISF-Summ, GistSumm and NeuralSumm, systems that also work with shallow linguistic resources. Also worth noticing is that these three systems have lower scores than the Top Baseline.
For a statistical significance analysis of these results, we included in Fig. 2 the p-values (in gray scale) of the corresponding t-tests [47], [64] performed between all CN-Summ strategies. We chose the paired t-test between average F-measures with a null hypothesis of ‘equal averages’. Thus, a low p-value (near zero, since 0⩽p-value⩽1<math><mrow is="true"><mn is="true">0</mn><mo is="true">⩽</mo><mi is="true">p</mi><mtext is="true">-value</mtext><mo is="true">⩽</mo><mn is="true">1</mn></mrow></math>) indicates a significant difference between two average F-measures. We were unable to include some systems in this analysis, since we do not have access to the complete experimental data generated by other authors (e.g. for ClassSumm). Nevertheless, we obtained the full data for SuPor-v2, which allowed a direct comparison with our systems. Fig. 2 shows the results in a tabular form where each square indicates the p-value of the t-test between two systems. We consider that a statistically significant result should have p-value ⩽0.05<math><mrow is="true"><mo is="true">⩽</mo><mn is="true">0.05</mn></mrow></math>. Such specific results are indicated in the following analysis, since Fig. 2 does not show them numerically.
Significant differences were obtained between the Random Baseline and all CN-Summ strategies. Nevertheless, the CN-Summ strategies CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>, CN-LI, CN-Communities and CN-Ringsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> cannot be regarded as statistically different from the Top Baseline. Moreover, these four systems are not statistically different from each other (see the dark group of squares in the lower right portion of Fig. 2). Significant differences existed between these four strategies and some top-scoring systems (SuPor-v2, CN-Voting, CN-SPwc,CN-Ringsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, CN-Degree and CN-Strength). Other differences were found between CN-Voting and CN-Coresk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, and between CN-LI and CN-Coresk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>. In summary, CN-Summ strategies may be classified into two groups, viz.: systems with average F-measure higher or lower than 40% (see Table 1). The lowest p-values are placed, in general, between these two groups, whereas the highest p-values are generally located inside those groups (Fig. 2 clearly shows two groups of high p-values). In other words, the main interpretation of these results is that systems inside these groups are not statistically different from each other. Furthermore, it is worth pointing out that, in this experiment, SuPor-v2 cannot be considered better than CN-Voting, CN-SPwc,CN-Ringsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, CN-Degree and CN-SPwi<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math>, because the corresponding p-values are all larger than 0.05. This means that some of CN-Summ strategies perform as well as SuPor-v2, one of the best summarizers for Brazilian Portuguese texts reported so far.
In the first experiment we could not compare the performance of CN-Summ with some systems for Brazilian Portuguese [36], [43], since the latter were evaluated in a different environment. Now, instead of using Precision/Recall, in this second experiment we applied a Rouge metric to evaluate summary informativeness [37]. More specifically, we have chosen the unigram-based recall metric Rouge-1, taking as reference the human-made abstracts of TeMário corpus. In order to have as meaningful a comparison as possible with published data, we did not remove stopwords from the extracts before computing Rouge-1. This is also the reason why in this experiment we considered a different compression rate, which is based on the number of words of the reference abstracts of TeMário. This rate ensures that each automatic extract has approximately the same number of words of the reference abstract. Moreover, CN-Summ is not allowed to include a sentence in the extract when it exceeds the compression rate limit. That sentence is ignored and the selection process continues picking up sentences until one that fits into the extract is found. Therefore, no broken sentence is selected by a CN-Summ strategy.
Table 2 shows the average Rouge-1 scores obtained with the CN-Summ versions, with the two baselines and six other extractive systems: SuPor-v2 [36], the best three variations of Mihalcea’s method, presented in Section 2[43], namely PageRank Backward, HITSA Backward and HITSH Forward, in addition to two modified versions of Mihalcea’s PageRank Undirected, called TextRank + Thesaurus and TextRank + Stem + StopwordsRem [36]. For short, the last two are named here TextRank + T and TextRank + S + S, respectively. The former uses the entries of a thesaurus to define edges between sentences, while the latter applies a pre-processing step of stemming and stopwords removal.
Once again, some of CN-Summ strategies are close to the top-scoring systems, with Rouge-1 ≈ 0.5. CN-Voting is again the best of our systems (on average), with a score of 0.5031. As in the previous experiment, the degree seems to play an important role on summarization, because CN-Strength and CN-Degree achieved good results, viz. 0.5020 and 0.5003, respectively. The degree is also relevant to CN-Ringslk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">lk</mi></mrow></msup></mrow></math>, a summarization strategy that completes the set of CN-Summ versions with Rouge-1 score higher than 0.5. But now the best variation of k-cores is the one using location of sentences (CN-Coresl)<math><mrow is="true"><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup><mo stretchy="false" is="true">)</mo></mrow></math> instead of node degree. Shortest paths show a poorer performance, closer to the Top Baseline. The versions based on w-cuts have relatively lower results than in the previous experiment, figuring now among the worst systems (near the Random Baseline), while CN-LI and CN-Communities have again low scores. In general, the variations of the strategies based on degrees, shortest paths, d-rings and k-cores consistently show good performances in both experiments. In other words, the number of connections (degree), the distance to other nodes (shortest paths), the distance to the hub (d-rings) and nuclear groups of nodes (k-cores) seem to be important to choose nodes (sentences) for the summarization of newspaper articles. Thus, our hypotheses proposed in Sections 3.1 Degree strategies: CN-Degree and CN-Strength, 3.2 Shortest path strategies: CN-SP, CN-SP, 3.4 , 3.5 appear to be valid.
Another important result of this experiment is the relative improvement of the Top Baseline performance when compared to the first experiment. The changes in the compression rate and in the evaluation metric put many systems below this baseline. It can also be noticed that SuPor-v2, PageRank and TextRank appear at the top of Table 2, while HITS systems are at a little lower position. SuPor-v2 has consistently the best scores in both experiments, probably because it uses deep linguistic knowledge, as already mentioned. PageRank Backward, which does not require deep linguistic resources, shows the generality of the PageRank algorithm, which was created to rank Web pages. Moreover, the addition of simple resources like a stemmer and a stoplist in PageRank (which is what TextRank + S + S does), increased its score from 0.5121 to 0.5426. Thus it is likely that the performance of CN-Summ may be improved with addition of a thesaurus, analogous to what has been done in TextRank + T. Nevertheless, this experiment shows that CN-Summ is already competitive when compared with the best systems based on linguistically shallow resources (TextRank + S + S, PageRank Backward and HITS variations). Indeed, in the first experiment some CN-Summ strategies performed better, on average, than TF-ISF-Summ, GistSumm and NeuralSumm, which are other linguistically shallow systems.
The significance of the results presented in this section are shown in Fig. 3 using the same approach discussed in Section 4.1, i.e. with a paired t-test to assess the statistical difference between average Rouge-1 scores. For lack of the full data for some systems, e.g. the scores per text generated by PageRank Backward, we could not include them all in the analysis. Fig. 3 is complemented by the confidence intervals generated by the software Rouge, which are shown in Table 2. An average Rouge-1 is inside the confidence interval with a 95% confidence level.
Statistically significant results (p-value ⩽0.05<math><mrow is="true"><mo is="true">⩽</mo><mn is="true">0.05</mn></mrow></math>) are scarcer in this experiment than in the first. This means that the majority of summarizers included in Fig. 3 are not significantly different. This result is confirmed by the confidence intervals of Table 2, where they almost always overlap one another. Significant differences were found between the Random Baseline and all other systems, but not between the Top Baseline and CN-Summ strategies. Thus, as observed earlier in this section, the Top Baseline performs similarly to our systems in this experiment. Low p-values were found between CN-Cutsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> and CN-Voting, between CN-LI and the four top-scoring systems, and between CN-Cutsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math> and the seven top-scoring systems. These results show that significant differences were only found between a few top-scoring and a few low-scoring systems. Ultimately, Fig. 3 demonstrates that almost all CN-Summ strategies, from CN-Voting to CN-Communities, are not significantly different from each other: every p-value in this case is greater than 0.05. These results are quite different from the ones obtained in the first experiment, where we were able to identify many significant differences among summarizers.
In this subsection we report additional experiments carried out to investigate the effect of the compression rate on the performance of all CN-Summ strategies. We assessed the quality of shorter extracts employing the same framework as the experiments discussed in Sections 4.1 First experiment, 4.2 Second experiment, thus allowing the identification of which systems perform better in more restricted environments.
In Section 4.1 a compression rate of 30% was employed, i.e. 70% of source sentences were removed to generate extracts. Furthermore, co-selection scores (i.e. Precision, Recall and F-measure) were applied to assess the informativeness of extracts. We now complement this experiment by using three different compression rates: 20%, 10% and 5%. It is worth pointing out that, in these new experiments, we were only able to compare CN-Summ with the two baseline systems, namely Top Baseline and Random Baseline. Moreover, paired t-tests were again performed between all average F-measures, therefore all results reported in this subsection refer to statistically significant differences between systems (i.e. p-value ⩽0.05<math><mrow is="true"><mo is="true">⩽</mo><mn is="true">0.05</mn></mrow></math>). When considering the new compression rate of 20%, we could notice that CN-Voting continues to be the highest scoring system, with F-measure equal to 34.6%. The Top Baseline and the Random Baseline are the systems with the lowest F-measures, 30.8% and 23.8%, respectively, and all systems are able to (statistically) improve on the Random Baseline. Nevertheless, the Top Baseline is now closer to CN-Summ systems, since only four of our summarizers are significantly better than this baseline. In general, almost all CN-Summ strategies are equivalent to each other in this experiment.
More restrictive compression rates continue to increase the performance of the Top Baseline. For instance, when using a 10% compression rate, the second best system is the Top Baseline with an F-measure equal to 22.1%, close to (and not statistically different than) the score of the best system (CN-Voting, with 22.8%). Nevertheless, paired t-tests also show that CN-Voting achieves a significant improvement on some low-scoring systems (such as CN-Coresk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, CN-Communities and CN-LI), a feature not shared with the Top Baseline. The Random Baseline continues with the lowest score (F-measure = 13.1%), again being statistically outperformed by all other systems. The same behavior was observed when reducing the compression rate once more (i.e. to 5%), where the F-measure for the Random Baseline is 8.6%. The Top Baseline maintains its top performance with a score equal to 15.3%, the same of CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>. Indeed, both systems are the only ones to show significant improvements on other systems, such as CN-SPwi<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wi</mi></mrow></msup></mrow></math>, CN-Communities and CN-LI. All in all, these three complementary experiments show that CN-Summ strategies tend to be equivalent to each other when shorter summaries are generated, according to the statistical tests performed. Furthermore, their performances are likely to be equivalent to the observed for the Top Baseline, suggesting that our approach does not make an improvement over the Top Baseline for very short summaries. Nevertheless, CN-Summ consistently shows statistically significant improvements on the Random Baseline.
Another set of complementary experiments was carried out, this time changing the compression rate of the experiment reported in Section 4.2, where Rouge-1 scores were employed. In this case, the compression rate is based on the size of the reference abstract according to its number of words. In Section 4.2, we have restricted the automatically generated extract to contain the same number of words as the reference abstract. Thus, we now reduce the size of extracts to 80%, 40% and 20% of the size of the reference summary, as opposed to the 100% (i.e. same sizes) employed earlier in this paper. Once more, we could only compare CN-Summ with the Top and Random Baseline systems. When considering the 80% reduction of the compression rate, we observed that CN-Voting maintains its highest performance with a Rouge-1 score of 0.4334. Indeed, it statistically outperforms many other CN-Summ strategies, such as CN-SPwc<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math>, CN-LI and CN-Cutsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>. The Random Baseline is at the opposite side of the ranking, with a score of 0.3994, and all systems are able to significantly improve on it (again, in a statistical sense), a feature consistently observed in every experiment reported in this subsection. Moreover, the other systems, excluding the high- and low-scoring ones, are all equivalent to each other.
When employing a 40% reduction in the compression rate, the four top-scoring systems (i.e. CN-Strength, CN-Coresl,CN-SPwc<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math> and CN-Voting, with scores ranging from 0.2441 to 0.2424) are able to significantly improve on four other systems, including the Random Baseline (with score 0.2115). In this experiment, CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> outperforms the Top Baseline, which has a score of 0.2366. In no other Rouge-based experiment previously reported in this paper a CN-Summ strategy was able to statistically outperform the Top Baseline. When shrinking extracts to 20% of the original compression rate, CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> again improves on the Top Baseline (Rouge-1 scores are 0.1263 and 0.1200, respectively). Indeed, CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> has the best average score in this experiment, and the lowest average score was generated by the Random Baseline, with Rouge-1 equal to 0.1046. In summary, these experiments show that all CN-Summ strategies are able to outperform the Random Baseline, consistent with the experiments based on F-measure scores. Furthermore, almost all CN-Summ strategies are statistically equivalent to each other when considering average scores. Nevertheless, as shorter summaries are generated, CN-Coresl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math> starts to outperform the Top Baseline, an improvement not observed earlier in Rouge-based experiments.
We conclude the evaluation of CN-Summ with an analysis of correlation between its different strategies. This type of analysis depends only on the corpus employed, not on a fixed compression rate or specific informativeness score. To accomplish this, we calculated the Spearman rank correlation coefficient rs<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">s</mi></mrow></msub></mrow></math>[47] between every pair of CN-Summ strategies, whose results are shown in Fig. 4. Recall that each CN-Summ strategy ranks the sentences from the source text (see Section 3.8), i.e. the first selected sentence by a strategy has rank 1, the second selected sentence has rank 2, and so on. Thus, for a given source text, it is possible to evaluate the correlation between two CN-Summ strategies (or between the rankings defined by them) using the Spearman correlation coefficient. The Spearman coefficient rs<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">s</mi></mrow></msub></mrow></math> is a quantity between −1 and 1, where strong correlations result in |rs|<math><mrow is="true"><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">s</mi></mrow></msub><mo stretchy="false" is="true">|</mo></mrow></math> approaching 1, and the sign of rs<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">s</mi></mrow></msub></mrow></math> gives the inclination of the correlation, i.e. positive or negative. Since we have these correlations for every source text of the TeMário corpus, Fig. 4 shows the average correlation between all CN-Summ strategies, including CN-Voting. For the remainder of this section, we consider that a high correlation between summarizers has rs⩾0.9<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">s</mi></mrow></msub><mo is="true">⩾</mo><mn is="true">0.9</mn></mrow></math> on average. Notice that negative correlations do not exist in Fig. 4. It is worth pointing out that every high average rs<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">s</mi></mrow></msub></mrow></math> has low standard deviation (lower than 0.085) in this case, thus indicating that high average correlations do not vary considerably. Moreover, high correlations are all statistically significant in this experiment, since the corresponding p-values are lower than 2.3×10-4<math><mrow is="true"><mn is="true">2.3</mn><mo is="true">×</mo><msup is="true"><mrow is="true"><mn is="true">10</mn></mrow><mrow is="true"><mo is="true">-</mo><mn is="true">4</mn></mrow></msup></mrow></math>.
High correlations were deliberately placed in the top left portion of Fig. 4 to enhance visualization. The group of strategies starting at CN-Degree and ending at CN-Voting (please, refer to the figure) contain most of the high correlations found. Indeed, 27 out of 29 high correlations occur between these strategies (these numbers were computed disregarding the symmetry of the correlation matrix). This set of strategies includes other correlations lower than 0.9 (e.g. between CN-SP and CN-Cutsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>); however, they are almost as strong as the other correlations of this group (all higher than 0.85), allowing us to refer to it as a set of strongly correlated strategies. Interestingly, this set of strategies consistently shows the highest average F-measures in the first experiment (Table 1), although in the second experiment the performance of our approach is not so distinctive. The observed behavior in the first experiment may be caused by the important role played by the degree in these strategies. For instance, in addition to the strategies based solely on degrees (CN-Degree and CN-Strength), the ones based on shortest paths are strongly influenced by degrees, since highly connected nodes tend to be closer to other nodes. Moreover, the strategies CN-Ringsk,CN-Coresk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math> and CN-Cutsk<math><mrow is="true"><msup is="true"><mrow is="true"><mspace width="0.35em" height="0.8ex" is="true"></mspace><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math> are all based on highly connected nodes, although in different ways. Probably these strategies have biased CN-Voting, as they are the majority of CN-Summ strategies and would “vote” for the same sentences to be included in the extract. This idea is reinforced by the fact that CN-Voting is highly correlated with these strategies as well, and that it is not significantly better than the majority of them (see the significance tests in Section 4).
Two other strong correlations occur between CN-Ringslk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">lk</mi></mrow></msup></mrow></math> and CN-Ringsk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">k</mi></mrow></msup></mrow></math>, and between CN-Ringslk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">lk</mi></mrow></msup></mrow></math> and CN-Voting. Nevertheless, in general CN-Ringslk<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">lk</mi></mrow></msup></mrow></math> is not highly correlated with other CN-Summ strategies. Similar behaviors were found for strategies CN-LI, CN-Coresl,CN-Cutsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Cores</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup><mtext is="true">,</mtext><mspace width="0.35em" is="true"></mspace><msup is="true"><mrow is="true"><mtext is="true">CN-Cuts</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>, CN-Communities and CN-Ringsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>. This means that these strategies can be used to complement one of the strategies of the group of highly correlated ones. For instance, information about community structure could be used as a complementary parameter in CN-SPwc<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-SP</mtext></mrow><mrow is="true"><mi mathvariant="italic" is="true">wc</mi></mrow></msup></mrow></math>, thus assisting the selection of sentences based on shortest paths. The weakest correlations can be found in the lower right portion of Fig. 4. This indicates that the corresponding strategies (from CN-LI to CN-Ringsl<math><mrow is="true"><msup is="true"><mrow is="true"><mtext is="true">CN-Rings</mtext></mrow><mrow is="true"><mi is="true">l</mi></mrow></msup></mrow></math>) tend to generate rather different extracts. As the results of Section 4 show, they also tend to have low informativeness scores. Therefore, although each one was based on different network features, they shared the common feature of not generating good extracts for the source texts of TeMário.
We have described the use of complex networks concepts for extractive summarization, as well as its evaluation through standard informativeness scores and significance tests. A simple network representation of texts was defined, which requires only shallow text pre-processing. Thus, the potential of our approach could be assessed by maintaining the focus on the summarization algorithms rather than on the construction of the networks. The systems proposed already show reasonable results for the summarization of newspaper articles in Brazilian Portuguese. Using automatic evaluation metrics, it was possible to identify the most promising strategies: the ones based on degrees, shortest paths, d-rings and k-cores. A voting summarizer was also created, which encompasses all the 14 strategies proposed. Some of the CN-Summ versions performed as well as the best summarizers of Portuguese texts reported in the literature, with evaluation being made within the same experimental setting. An analysis of correlation between the proposed methods was also performed, allowing us to identify similar or complementary strategies.
The definition of the network is extremely important, with possible large impact on the performance of network-based summarization strategies. Such a definition may be improved in future research using more language resources. For example, if we stick to the idea of using a network of sentences, the strengthening or fine-tuning of sentence interconnections may be performed by employing: (i) anaphor resolution, which can be used to identify a link between anaphors and the corresponding antecedents in different sentences, thus creating edges currently ignored; (ii) recognition of multiword expressions, since we only identify single words in our pre-processing step and compound nouns are incorrectly treated as separate nouns; and (iii) a thesaurus or lexical chains, thus being able to detect semantic, lexical relationships such as synonyms/antonyms and hyponyms/hypernyms, which also allows the assignment of distinct edge weights for different types of lexical links.
Further improvements also include joining all CN-Summ strategies in a machine learning approach, possibly using the correlation analysis of Section 5 for feature selection. Another choice would be integrating two or more non-correlated summarization strategies into a new summarizer, aiming at complementing one another. For example, since some strategies do not try to cover the topic structure of the source text (e.g. shortest path and degree strategies), it would be useful for complementing them with grouping (topic) information given by the communities and locality index strategies, thus trying to avoid topic redundancy in the former strategies. Evaluations using other corpora and different languages may also be carried out to assess the generality of our approach. To some extent, the hypothesis of this work has been proven by the results: network measurements, which are neither language nor domain dependent, can be used for extractive summarization, and can lead to informativeness scores close to the more linguistically complex and computationally costly systems. This should perhaps be expected, as the measurements of complex networks have already been shown to capture important features of texts [3], [4], [5], [53].
The authors thank the scientific Brazilian agencies CNPq and FAPESP for supporting this work. L. Antiqueira is grateful for grants 132154/06-4 (CNPq) and 05/03361-8 (FAPESP). L. da F. Costa in recipient of grants 301303/06-1 (CNPq) and 05/00587-5 (FAPESP). We also thank D.S. Leite and L.H.M. Rino for providing detailed data about the evaluation of SuPor-v2 with F-measure scores.
An undirected graph or network G of N nodes and M edges can be represented by the adjacency matrix A, symmetric and of order N×N<math><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">N</mi></mrow></math>, whose elements aij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> and aji<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">a</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ji</mi></mrow></msub></mrow></math> are equal to 1 if there is an edge between nodes i and j, or equal to 0 otherwise. As explained in the beginning of Section 3, an edge exists if there is a co-occurrence of the same lemmatized noun between two sentences. If an edge has a numeric label, it is said to have a weight. Thus, another matrix can be employed, called the weight matrix W, which in our case is defined as follows. Let Pi={p1,p2,…,pni}<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mtext is="true">,</mtext><mo is="true">…</mo><mtext is="true">,</mtext><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math> be the set of ni<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math> lemmatized nouns of the ith sentence of the source text. The weight wij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> (or wji<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ji</mi></mrow></msub></mrow></math>) of the edge that links sentences i and j is the number of noun co-occurrences between them, i.e. wij=wji=|Pi∩Pj|<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ji</mi></mrow></msub><mo is="true">=</mo><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">∩</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">|</mo></mrow></math>. If wij=0<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></math>, no edge exists between nodes i and j. The weights wij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">w</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> are therefore elements of the symmetric matrix W of order N×N<math><mrow is="true"><mi is="true">N</mi><mo is="true">×</mo><mi is="true">N</mi></mrow></math>, which represents an undirected weighted network of a given source text.