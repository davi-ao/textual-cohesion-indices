The rapidly developing internet technology comes with the appearance of a large amount of text data every day, making text analysis a promising field for researchers. For text analysis, feature extraction is the basis for extracting the primary information to represent the original text. The main object of feature extraction is to reduce the dimension of the feature representation and eliminate the noise in the feature space. The features of text data refer to the language units (words) or phrases (Collobert et al., 2011), and even characters, which also perform good results in specific tasks (Zhang et al., 2015). There are numerous machine learning methods to achieve feature extraction, and their common advantage is that they are well adaptive to different tasks (Hassan et al., 2015, Junejo et al., 2016, Prihatini et al., 2018, Zhao and Mao, 2018, Gupta and Gupta, 2021, Yan et al., 2020). In recent years, the success of distributed representation of words (Mikolov et al., 2013, Le and Mikolov, 2014, Pennington et al., 2014, Devlin et al., 2019) has inspired researchers to learn the features of text through training on the neural network (Kim, 2014, Hu et al., 2014, Yin et al., 2016, Foland and Martin, 2017, Liang et al., 2017, Young et al., 2018). At the same time, the explosion of data on the internet provides large datasets for deep learning methods. Based on a large amount of training data, the pre-training and transformer have achieved a significant improvement on Natural Language Processing (NLP) (Vaswani et al., 2017, Howard and Ruder, 2018, Sun et al., 2020).
The excellent performance of most of the methods is based on considerately selected balanced data. However, the practical scenarios of text data analysis always come with the imbalanced distribution of different labels. For example, the comments on specific affairs may contain a majority of positive opinions and a minority of negative ones. The small number of negative comments may be of great value to helping the decision-makers make the right decisions. Moreover, the wrong decision making may lead to overwhelmingly serious consequences in imbalanced text data analysis. A cancer patient being misdiagnosed as healthy is far worse than the consequences of a healthy person being diagnosed with cancer in the medical field.
The imbalanced data always contain the part of majority classes that have many samples and the part of minority classes, which are represented by a much smaller number of samples than the majority classes. Because most of the models are constructed assuming that the training data is balanced, the performance is reduced on the imbalanced data. The performance reduction always happens in the minority classes as the models tend to neglect the samples of minority classes to get high accuracy of the majority classes (He, & Garcia, 2009). Though numerous researches are focusing on imbalanced numerical data (He and Garcia, 2009, Krawczyk, 2016, Gao et al., 2017), few studies claim that their model can be applied to text data analysis. Some effective methods for analyzing numerical data could not be directly used to text data because the information of text is contained in word sequences rather than numbers. Moreover, as far as we know, most of the methods for processing imbalanced text data must first convert a document to numerical data (Ogura et al., 2011, Iglesias et al., 2013, Naderalvojoud et al., 2015, Song et al., 2016, Li et al., 2018, Xiao et al., 2019), i.e., the Bag-of-Words (BoW) vectors (Iglesias et al., 2013, Naderalvojoud et al., 2015, Song et al., 2016). This two-step solution may lead to two kinds of information losses: the loss of conversion from text to vectors and the loss of sampling (when used).
To this end, we propose a simple yet useful feature extraction model to over-sample and expand the number of samples to deal with the problem of imbalanced text data from the angle of text data rather than numerical data. The proposed model uses a powerful network tool to generate synthetic samples through random walking. We contend that the random walk paths, which have been successfully used for graph embedding (Perozzi, 2014, Grover and Leskovec, 2016, Hamilton et al., 2017), will reconstruct the structural and statistical properties of a text. The main idea is to use the random walk paths of a document to represent its primary information. Though the random walk paths, which are used to capture the continuous feature representation of nodes in the network (Grover, & Leskovec, 2016), lose the word order in the documents, they keep the high-level structural and semantic information. The over-sampled documents within the same class contain specific high-level information while behaving in absolutely different word sequences, alleviating the effect of overfitting during training. We also introduce a new neural network layer to transfer the random paths to a form that is appropriate for a CNN model (we call their assembling as NCNN) to make the model adapt to the existing neural network framework.
The main contributions of this paper are summarized:
A network-based feature extraction model is proposed for processing imbalanced text data. As far as we know, we are the first to introduce a random walk strategy to deal with text data imbalance.
We explore the methods to combine random walk paths with CNN and propose the Polar Layer as their intermedia.
We introduce an electing strategy to improve the performance of NCNN further. In the experimental section, the NCNN model with an electing strategy achieves the best results.
By comparing with different methods of imbalanced text data, we certify the performance of the proposed model.
We organize the rest of this paper as follows. The next section reviews related works, including methods on imbalanced text data, complex networks, and random walks. In section 3, we introduce the proposed model and present the framework of NCNN. Section 4 shows the experimental results to verify the performance of our model. In section 5, we present the conclusions of this paper.
Over the last decades, handling data imbalance is always the focus of industry and academia. The methods to deal with such issues can be categorized into data sampling, algorithm modification, and cost-sensitive learning (He and Garcia, 2009, Krawczyk, 2016, Li et al., 2018). As for the text data, most of the studies concentrate on data sampling and algorithm modification. So we mainly review these two types of methods. To make them convenient for the review, we further divide these approaches into three groups: direct-sampling methods, term-weighting methods, and background-knowledge-based methods.
The direct-sampling methods aim to re-sample the data through recombination of the original data to produce new samples of minority classes or reduce existing samples of majority classes. A simple way of direct sampling is to copy or remove the randomly chosen samples from the original datasets to equalize the class membership. However, this approach only balances the number of samples between classes without changing the issues of feature imbalance, which can only get very limited improvement. SMOTE (Chawla et al., 2002) is first proposed for numerical data and can be used in text feature vectors (e.g., BoW) (Ma et al., 2018). The idea of SMOTE is to get a synthetic vector from two neighbors of the minority class. The digit value in each dimension of the synthetic vector is the weighted summation (the weight is generated randomly) of the two neighbors. Because a random process is introduced, the new samples from SMOTE are forced to be unspecific to lead a general decision region, which is useful to avoid overfitting. An HMM model is introduced in the literature (Iglesias et al., 2013) to generate feature words and their weight to get new synthetic samples, which helps get better results than SMOTE in dealing with some imbalanced text datasets. Wang et al. (2013) present an algorithm of boundary region cutting (BRC) to alleviate boundary ambiguity of two-class text classification. BRC is an under-sampling operation by which the samples in the dense boundary region of the majority class are randomly eliminated. Li et al. find that the random elimination in BRC can not effectively remove the samples in the majority class near minority class samples. So they propose a local dense mixed region cutting (LDMRC) algorithm (Li et al., 2019) to improve BRC. An ensemble method is proposed by Zuo et al. (2016) to improve the latent Dirichlet allocation (LDA) topic model on imbalanced short text data. The short texts in the same class are collected as a whole to construct a co-occurrence complex network. Then pseudo-document sets are sampled from the neighbors of every node in the network. Finally, the LDA model is trained based on the pseudo-document sets. Song et al. (2016) explore a combination of over-sampling and under-sampling based on the K-means clustering. The number of samples in the majority and minority classes is supposed to reach a certain value k. For the minority classes, the samples are clustered into two groups, and then a new sample is generated by SMOTE for the smaller group. This process is repeated until the number of samples reaches k. On the flip side, the samples are clustered into k groups for the majority classes, and k samples nearest to the cluster centers are left, yielding the balanced majority classes.
Term-weighting methods give feature words a specific weight to balance the role of each feature word. In literature (Ogura et al., 2011), the authors compare the influence of different metrics for feature selection in the imbalanced text data. The authors introduce three types of metrics, including six different metrics: χp2<math><msubsup is="true"><mi is="true">χ</mi><mi is="true">p</mi><mn is="true">2</mn></msubsup></math> and Gini index for Type-1; χ2<math><msup is="true"><mi is="true">χ</mi><mn is="true">2</mn></msup></math> and information gain for Type-II; and signed χ2<math><msup is="true"><mi is="true">χ</mi><mn is="true">2</mn></msup></math> and signed information gain for Type-III. The results show that Type-1 and Type-II achieve comparable performance (better than Type-II). And based on the results, they conclude that negative features are beneficial in classifying imbalanced text data. It should be noted that being positive or negative for the features refers to their contribution to a certain category, e.g., a positive (negative) feature for class i may specifically occur more (less) frequently in this class. TFIDF is a widely used weighting scheme because it is a more effective way than term frequency (TF) to represent the relative weight to the overall condition of the whole datasets by introducing inverse document frequency (IDF) (Ogura et al., 2011). However, TFIDF neglects the in-class information of the datasets and thus can not consider class-specific term weight. Naderalvojoud et al. (Naderalvojoud et al., 2015) propose a positive and negative-based term weighting scheme to take the category membership into account. This idea can mitigate the effects of unbalanced data better than TFIDF because the proposed term weighting scheme associates terms with each prior category information. Wu et al. (2014) present ForesTexter to handle the problem of text data imbalance in binary classification using a random forest algorithm. ForeTexter uses a support vector machine (SVM) classifier to classify features into two groups to split every branch successively. By splitting the samples into feature subspaces and retaining both negative and positive features in every subspace, this algorithm decreases the influence of data imbalance.
There are also background-knowledge-based methods that use domain-specific knowledge as supplementary information to improve the performance of domain-sensitive tasks on imbalanced text data. The idea of these methods is also to implement over-sample to balance the datasets like direct-sampling methods, while the difference occurs in the use of background knowledge. Literature (Li et al., 2018) considers the sentimental polarity of text sentiment classification and introduces “inversion” and “imitation” strategies to generate new samples. The “inversion” strategy converts a document in majority class into the text with opposite sentimental polarity, e.g., “I love you” to “I hate you.” On the contrary, the “imitation” strategy changes the sentimental words into their synonymous words, e.g., “I love you” to “I like you.” In recent years, deep learning methods are also introduced to tackle the issues of processing imbalanced text data. Xiao et al. (2019) use transfer learning to first train a model in the balanced data and then apply the model to the imbalanced data with fine-tuning. Because the pre-trained model contains information beyond the text datasets, it can achieve more stable performance than the model trained directly on the text datasets. New powerful generation models like GANs, BERT, and GPT-2 are pre-trained from a huge amount of text and thus are able to produce fluent pseudo text. Some researchers concentrate on generating new text samples directly through text generation model. One strategy is to generate new text through the generation model after a fine-tuning based on the imbalanced datasets (Shaikh et al., 2021). Tang et al. (2021) present another feasible strategy: first, translate a document into another language and then translate it back. The documents translated back have the same topics but different expressions. However, these two strategies both have their limitations. For the first strategy, fine-tuning, which still needs enough text samples as the training data, is essential for the final production. Meanwhile, the pre-trained models may highly extend the boundary of the target datasets, which introduces extra noise to the minority classes that don't contain enough samples for fine-tuning. For the second strategy, the translation of a document only increases the description styles instead of the new samples with the same topic. The authors introduce another operation like “imitation” mentioned above to enrich samples.
Above all, most direct-sampling methods and term-weighting methods have to deal with the problem of imbalance at the numerical level instead of the text level. The background-knowledge-based methods achieve the text level over-sampling. However, the need for domain-specific background knowledge increases the difficulty of their application on other text classification tasks.
Many systems in our world can be represented as networks whose nodes and edges are the system elements and their relations separately. With the complex relations among system elements, most networks of systems in the real world, like social relationships and author citation links, are complex networks. The complex network is a kind of graph that is neither regular nor random. Its global properties can not be inferred from the local interactions between certain node groups but emerge from the interactions of the whole elements (Cong, & Liu, 2014).
It is a hot field to study the natural language with complex networks, and a considerable number of researches have emerged in the last few years (Cong and Liu, 2014, Arruda et al., 2016a, Arruda et al., 2016b, Akimushkin et al., 2017, Garg and Kumar, 2018). A network model of text is constructed with the language units (words or phrases) as the nodes connected by their interrelations (Cong, & Liu, 2014). The complex networks can develop their advantages when used to capture the high-level structural and semantic information of the text. It is also proved that complex networks can enhance the performance of various tasks in text analysis (Amancio, 2015, Amancio et al., 2012, Amancio et al., 2012, Antiqueira et al., 2009, Arruda et al., 2016a, Arruda et al., 2016b, Yan et al., 2020, Yan et al., 2019). Through complex networks, the words lose their order in the original sequences but retain their structural and semantic information. For example, the words that co-occur in the source text will also act as neighbors in the complex networks, and the highly used collocations of the text will behave as motifs (Goh et al., 2018).
According to the kind of relations between language units, the complex network of text can be divided into static and dynamic networks (Yan et al., 2020). For static networks, the language units are connected when they keep a semantic relationship, e.g., hypernymy, meronymy, and synonymy relationship. The edges of the dynamic networks are formed when two language units co-occur in the actual language use (Cong, & Liu, 2014). For example, in the co-occurrence complex network, a sub-structure of dynamic networks, two nodes are connected if they co-occur in the source text within a certain distance. In this paper, the proposed methods are all based on the co-occurrence complex network.
Random walk is widely used in the analysis of complex networks. The idea of the random walk is simple and easy to be implemented. Let V represent all the nodes in a network model, and vi∈V<math><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">V</mi></mrow></math> represent a particular node. A random walk path with length k is generated after k times random jump with every jump aiming to an arbitrary neighbor of the node from the previous jump. In recent years, the random walk has been successfully used in node embedding of complex networks. DeepWalk (Perozzi et al., 2014) uses the random walk to generate several paths to learn the higher-order proximity between nodes. The target is to maximize the likelihood that a given node is centered around the 2m nodes along the random paths, where m nodes are to the left and m nodes are to the right. The node2vec (Grover, & Leskovec, 2016) preserves the ideas in DeepWalk. The main differences between DeepWalk and node2vec are that node2vec is based on a bias random walk and provides the options and trade-off between breadth-first searching (BFS) and depth-first searching (DFS). Walklets (Perozzi et al., 2016) also learn representations of vertices in a network through random walks. This model can capture multiple scales of relationships between networks, which is achieved by skipping some nodes during random walking. Many other models also capture the high-level information of networks through random walking (Li et al., 2016, Pan et al., 2016, Yang et al., 2016, Bojchevski et al., 2018, Xia et al., 2020), and the successful application of them verify that random walk is a powerful tool for analyzing complex networks.
Moreover, random walk is also used for over-sampling in imbalanced data classification. Zhang and Li (Zhang, & Li, 2014) propose a random walk over-sampling (RWO-Sampling) approach to balancing different class samples. This approach obeys the original minority class distribution and combines the random walk in the learned distribution with the original value for each attribute. Roshanfekr et al. (Roshanfekr et al., 2020) improve the RWO-Sampling method and develop a UGRWO-Sampling approach based on graphs to avoid the likelihood of over-fitting in RWO-Sampling. Our method is different from the above approaches in the following aspects. First, RWO-Sampling and UGRWO-Sampling are proposed for imbalanced numerical data with the assumption that the distribution of each attribute holds a Gaussian distribution. However, some vector representations for text like BoW are of high sparsity in some attributes and contain many zeros instead of keeping in line with the Gaussian distribution. Our method implements the random walk directly on the text complex network to avoid the distribution assumption, which is more suitable for text. Second, the random walk of the above approaches is directly based on the sample nodes, while our method is based on the features of each sample. Finally, our method is implemented before feature selection, while RWO-Sampling and UGRWO-Sampling are implemented after feature selection.
This section introduces the Polar Layer to transfer a document to multi-sampled random walk paths with a specific length. The output of the Polar Layer will be directly connected to a 2D CNN model, which is called NCNN in this paper.
Let D={d1,d2,...,d|D|}<math><mrow is="true"><mi is="true">D</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mi is="true">d</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mrow is="true"><mo stretchy="false" is="true">|</mo><mi is="true">D</mi><mo stretchy="false" is="true">|</mo></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></math> , where D is a collection of documents and di<math><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub></math> denotes the ith document in D. Given L={l1,l2,...,lc},c<<|D|<math><mrow is="true"><mi is="true">L</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mi is="true">l</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">l</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">,</mo><msub is="true"><mi is="true">l</mi><mi is="true">c</mi></msub><mo stretchy="false" is="true">}</mo><mo is="true">,</mo><mi is="true">c</mi><mo is="true">&lt;</mo><mo is="true">&lt;</mo><mo stretchy="false" is="true">|</mo><mi is="true">D</mi><mo stretchy="false" is="true">|</mo></mrow></math> , we can assign every document in D to L to make the documents classified into c categories. We define g=(V,E)<math><mrow is="true"><mi is="true">g</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo stretchy="false" is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo stretchy="false" is="true">)</mo></mrow></math> as a network, where V is the set of nodes and E⊆(V×V)<math><mrow is="true"><mi is="true">E</mi><mo is="true">⊆</mo><mo stretchy="false" is="true">(</mo><mi is="true">V</mi><mo is="true">×</mo><mi is="true">V</mi><mo stretchy="false" is="true">)</mo></mrow></math> is the set of edges. Each edge e∈E<math><mrow is="true"><mi is="true">e</mi><mo is="true">∈</mo><mi is="true">E</mi></mrow></math> is the relationship between two nodes. We can use an ordered pair to represent this relationship, i.e., eij=(vi,vj)<math><mrow is="true"><msub is="true"><mi is="true">e</mi><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo stretchy="false" is="true">)</mo></mrow></mrow></math>, and the strength of the relationship is denoted as ωij<math><msub is="true"><mi is="true">ω</mi><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></math>. In general, if there always exists the equivalence that ωij=ωji<math><mrow is="true"><msub is="true"><mi is="true">ω</mi><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msub is="true"><mi is="true">ω</mi><mrow is="true"><mi mathvariant="italic" is="true">ji</mi></mrow></msub></mrow></math> for all node pairs that contain edges, we call G is undirected. Otherwise, if there exist node pairs with ωij≠ωji<math><mrow is="true"><msub is="true"><mi is="true">ω</mi><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">≠</mo><msub is="true"><mi is="true">ω</mi><mrow is="true"><mi mathvariant="italic" is="true">ji</mi></mrow></msub></mrow></math> , we call G is directed. Let G={g1,g2,...,gn}<math><mrow is="true"><mi is="true">G</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo stretchy="false" is="true">{</mo><msub is="true"><mi is="true">g</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">g</mi><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">,</mo><msub is="true"><mi is="true">g</mi><mi is="true">n</mi></msub><mo stretchy="false" is="true">}</mo></mrow></math> be a set of networks, where gi is the representation of di. For convenience, we denote the relation between gi and di as gi=ψ(di),gi∈G,di∈D<math><mrow is="true"><msub is="true"><mi is="true">g</mi><mi is="true">i</mi></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ψ</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub><mo stretchy="false" is="true">)</mo></mrow><mo is="true">,</mo><msub is="true"><mi is="true">g</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">G</mi><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">D</mi></mrow></math>.
There are different ideas on how to represent a document as a network, e.g., the dynamic network and the static network (Cong and Liu, 2014, Yan et al., 2020). In this paper, unless specified, the following descriptions are all based on the co-occurrence dynamic network, where the edge, linking two nodes when they co-occur in the source text, is undirected. In the co-occurrence dynamic network, each node corresponds to a word, and the edges are the instance of the co-occurrence between nodes. For example, we show a network constructed from a short text “This is just a toy example. Here we show how a complex network of a text is constructed.” in Fig. 1. Note that the text is preprocessed: all letters are converted to lower case, and punctuations and special symbols are eliminated. In Fig. 1, the edge only refers to the co-occurrence of two nearest neighbors in the original text sequences, e.g., “this” and “is” are connected, “is” and “just” are connected, but “this” and “just” are not connected. Furthermore, because punctuations are removed in preprocessing, two words that are split by punctuation in the original text are still connected, e.g., “example” and “here” are connected with an edge. The nodes and edges are all unweighted, meaning that the weights of all nodes and edges are one regardless of the words' frequency in the source text.
There are two goals in this paper. First, we aim to define a representation of gi∈G<math><mrow is="true"><msub is="true"><mi is="true">g</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">G</mi></mrow></math>, from whom we can learn and extract features Fi={f1i,f2i,…,fn′i}<math><mrow is="true"><msub is="true"><mi is="true">F</mi><mi is="true">i</mi></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mi is="true">f</mi><mrow is="true"><mn is="true">1</mn><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mi is="true">f</mi><mrow is="true"><mn is="true">2</mn><mi is="true">i</mi></mrow></msub><mo is="true">,</mo><mo is="true">…</mo><mo is="true">,</mo><msub is="true"><mi is="true">f</mi><mrow is="true"><msup is="true"><mi is="true">n</mi><mo is="true">′</mo></msup><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">}</mo></mrow></mrow></math>. The classifier then assigns the corresponding di∈D<math><mrow is="true"><msub is="true"><mi is="true">d</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">D</mi></mrow></math> to the correct category lj∈L<math><mrow is="true"><msub is="true"><mi is="true">l</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi is="true">L</mi></mrow></math> according to Fi. Second, we aim to present a method to produce new samples from the text with label lj∈L<math><mrow is="true"><msub is="true"><mi is="true">l</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><mi is="true">L</mi></mrow></math>. And the extracted features Fi' from these new samples contain the representative information of the text to improve the performance of the classification task.
There are several benefits of using random walks to extract the information of a document. A document needs space to store all the words (including punctuation marks sometimes), while it only needs to store the word pairs that co-occur in the source document when represented as a network. The sampling of a node in the random walk paths only acquires the information of neighbors or the second-order neighbors (the neighbors of neighbors), which makes it possible to sample on a large document with proper time costs. The high-level structural and semantic similarities can be extracted flexibly with different searching strategies (Grover and Leskovec, 2016, Perozzi, 2014).
During a sampling process of random walk, given a start node u and the length l of random walk, let vi<math><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub></math> denotes the node in site i along the path. After vi<math><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub></math> is sampled, the next node vi+1<math><msub is="true"><mi is="true">v</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></math> will be generated with probability(1)P(vi+1=x|vi=y)=wx∑n∈N(y)wn,ifx∈N(y)0,else,<math><mrow is="true"><mi is="true">P</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">v</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">=</mo><mi is="true">x</mi><mo stretchy="false" is="true">|</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">=</mo><mi is="true">y</mi><mo stretchy="false" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfenced open="{" close="" is="true"><mrow is="true"><mrow is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mrow is="true"><mfrac is="true"><msub is="true"><mi is="true">w</mi><mi is="true">x</mi></msub><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">n</mi><mo is="true">∈</mo><mi is="true">N</mi><mo stretchy="false" is="true">(</mo><mi is="true">y</mi><mo stretchy="false" is="true">)</mo></mrow></msub><msub is="true"><mi is="true">w</mi><mi is="true">n</mi></msub></mrow></mfrac><mo is="true">,</mo><mrow is="true"><mspace width="0.333333em" is="true"></mspace><mtext is="true">if</mtext><mspace width="0.333333em" is="true"></mspace></mrow><mi is="true">x</mi><mo is="true">∈</mo><mi is="true">N</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">y</mi><mo stretchy="false" is="true">)</mo></mrow></mrow></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mn is="true">0</mn><mspace width="0.333333em" is="true"></mspace><mo is="true">,</mo><mrow is="true"><mspace width="0.333333em" is="true"></mspace><mtext is="true">else</mtext></mrow></mrow></mtd></mtr></mtable></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>where N(y)<math><mrow is="true"><mi is="true">N</mi><mo stretchy="false" is="true">(</mo><mi is="true">y</mi><mo stretchy="false" is="true">)</mo></mrow></math> denotes the neighbors of node y, and wx denotes the weight of x while searching. The most straightforward strategy is to assume that all neighbors have equal chances to be chosen for the next node, i.e., wx=1,x∈N(y)<math><mrow is="true"><msub is="true"><mi is="true">w</mi><mi is="true">x</mi></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mi is="true">x</mi><mo is="true">∈</mo><mi is="true">N</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">y</mi><mo stretchy="false" is="true">)</mo></mrow></mrow></math>. Indeed, there are some options for us to get specific structural equivalence. We can use a local search around a node to capture the similarity of neighbors' states (Amancio et al., 2012). The direction of the local search can be controlled by P, which is determined by choice of wx in Eqs. (1). Before going into the choice of wx in this paper, we first introduce two kinds of similarities.
In Fig. 2, we show two kinds of similarities between documents. There are two documents doc1 (left) and doc2 (right). The local similarity between doc1 and doc2 may exist in particular nodes' neighbors, A1 and A2. If we only consider the nearest neighbors of nodes, the local similarity of A1 and A2 can be counted by the overlapping of nearest neighbors, e.g., B1, B2, and B3. For example, some alternative words may have the same nearest neighbors. The short phrases like “source node,” “source text,” “source document” in this paper can also be written as “original node,” “original text,” “original document.” In this case, “source” and “original” play the role of A1 and A2, and “node,” “text,” and “document” are equivalent to B1, B2, and B3. In order to consider the high order of neighbors, the local search needs to extend far away from A1 (A2), i.e., the neighbors of neighbors. This kind of search strategy may reflect the similarity of certain topics. Take the topic of “pets” as an instance. This topic may describe a different aspect of pets, a dog and a cat all “like eating fishes,” “play with a toy ball,” and “sleep all day long,” where we can think of a dog and a cat as A1 and A2, and think of the above three short phrases as the orange, green, and blue block, as shown in Fig. 2.
To consider two kinds of similarities mentioned above, we introduce the 2nd order random walk proposed in the literature (Grover, & Leskovec, 2016). Assume that a random walk leaves node s and stands on node m; it decides the next step to the neighbors of m and the probability distribution in Eqs. (1) is biased by α with the following equation(2)α(m,e)=1pifdes=01ifdes=11qifdes=2,<math><mrow is="true"><mi is="true">α</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">m</mi><mo is="true">,</mo><mi is="true">e</mi><mo stretchy="false" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfenced open="{" close="" is="true"><mrow is="true"><mrow is="true"><mtable is="true"><mtr is="true"><mtd is="true"><mrow is="true"><mfrac is="true"><mn is="true">1</mn><mi is="true">p</mi></mfrac><mrow is="true"><mspace width="0.333333em" is="true"></mspace><mtext is="true">if</mtext><mspace width="0.333333em" is="true"></mspace></mrow><msub is="true"><mi is="true">d</mi><mrow is="true"><mi mathvariant="italic" is="true">es</mi></mrow></msub><mo is="true">=</mo><mn is="true">0</mn></mrow></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mn is="true">1</mn><mrow is="true"><mspace width="0.333333em" is="true"></mspace><mtext is="true">if</mtext><mspace width="0.333333em" is="true"></mspace></mrow><msub is="true"><mi is="true">d</mi><mrow is="true"><mi mathvariant="italic" is="true">es</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn></mrow></mtd></mtr><mtr is="true"><mtd is="true"><mrow is="true"><mfrac is="true"><mn is="true">1</mn><mi is="true">q</mi></mfrac><mrow is="true"><mspace width="0.333333em" is="true"></mspace><mtext is="true">if</mtext><mspace width="0.333333em" is="true"></mspace></mrow><msub is="true"><mi is="true">d</mi><mrow is="true"><mi mathvariant="italic" is="true">es</mi></mrow></msub><mo is="true">=</mo><mn is="true">2</mn></mrow></mtd></mtr></mtable></mrow></mrow></mfenced><mo is="true">,</mo></mrow></math>where e is the next node; p and q are the hyper-parameters; des<math><msub is="true"><mi is="true">d</mi><mrow is="true"><mi mathvariant="italic" is="true">es</mi></mrow></msub></math>is the shortest step from node e to node s (e.g., des=0<math><mrow is="true"><msub is="true"><mi is="true">d</mi><mrow is="true"><mi mathvariant="italic" is="true">es</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn></mrow></math> denotes that e = s). Now we go back to the choice of wx mentioned above. By making the weight of x equal to α, i.e., wx = α(y, x), the local search can be controlled to decide which similarity is considered in the probability distribution of Eqs. (1). We can adjust the search strategy by changing the value of p and q. The high value of p is to search high order of neighbors, and the high value of q is to search the nearest neighbors. As is shown in Fig. 3, the next step from m includes e1, e2, and e3. According to the shortest path from e1, e2, and e3 to s, it can be inferred that de1s = 0, de2s = 2, and de3s = 1, and thus α(m, e1) = 1/p, α(m, e2) = 1/q, and α(m, e3) = 1. Here, if we set p > q = 1, then node e2 has a larger probability of being chosen as the next step, as is shown in Eqs. (1), and the random walk tends to step to high order of neighbors of s. Otherwise, if we set q > p = 1, the random walk will step around the nearest neighbors of s. The influence of p and q on classification performance will be studied in the experimental section.
Algorithm 1 shows the step of generating the random paths to rebuild the information of a text. p and q are set to get a trade-off between the nearest neighbors and high order of neighbors. The words of document d are labeled to reduce the memory cost. Every document is sampled with several random paths (with path number h), and h random paths are concatenated to be a matrix M. M may be a (1 × hw) sequence or an (h × w) matrix. However, a sequence form of M can not consider different blocks of high-order of neighbors, so we choose an (h × w) matrix of M to be the output.
Algorithm 2 is to over-sample and expand the number of document samples. Note that g=ψ(d1,d2)<math><mrow is="true"><mi is="true">g</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ψ</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">d</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mn is="true">2</mn></msub><mo stretchy="false" is="true">)</mo></mrow></math> denotes to merge two networks of d1 and d2, including all the nodes and their edges. We can simply view it as two documents d1 and d2 being concatenated to form a more extended document d, and then represented as network g. Generally, d1 and d2 are asserted to belong to the same class, i.e., d1,d2∈li,li∈L<math><mrow is="true"><msub is="true"><mi is="true">d</mi><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mi is="true">d</mi><mn is="true">2</mn></msub><mo is="true">∈</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">L</mi></mrow></math>. We can use Algorithm 2 several times to expand the samples to a decent number. A toy example of the above algorithms is described in Fig. 4, where path length w = 5, path number h = 2, and the document are sampled two times. In the following description, we denote the process to get the random walk paths through Algorithm 1 or Algorithm 2 as M=PathGen(g)<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">P</mi><mi is="true">a</mi><mi is="true">t</mi><mi is="true">h</mi><mi is="true">G</mi><mi is="true">e</mi><mi is="true">n</mi><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></math>, and we can try this process several times and get different random paths, which is denoted as {M1,M2,...,Mn}=PathGenn(g)<math><mrow is="true"><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mi is="true">n</mi></msub><mo stretchy="false" is="true">}</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">P</mi><mi is="true">a</mi><mi is="true">t</mi><mi is="true">h</mi><mi is="true">G</mi><mi is="true">e</mi><msup is="true"><mi is="true">n</mi><mi is="true">n</mi></msup><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></mrow></math>.
A random path generation will transform every document sample into a 2D matrix M. One difficulty arises when deciding how to train this 2D matrix and extract features because the classical language model only supports a sequence of the document rather than a 2D matrix. To this end, we introduce a 2D convolution neural network (CNN) model to fit a 2D matrix. The 2D CNN model is widely used in computer vision tasks, in which a picture is stored as a 2D matrix. The 2D CNN model learns the features of 2D matrix by convolving the matrix with convolutional cores, which are smaller 2D matrices in width and height. Through convolving operation, the local features of the 2D matrix are assembled into many matrices with smaller width and height than the original 2D matrix. When the convolution is applied to text, the text CNN model will convolve a matrix containing only width from a sequence of words (Kim, 2014). Furthermore, the words of text are represented as vectors, and the input of CNN model for text is a sequence of word vectors. In this paper, the proposed network-based CNN (NCNN) model is specifically designed to process a 2D matrix M, in which each value is also represented as vectors and thus yield M′∈Rh×w×d<math><mrow is="true"><mrow is="true"><msup is="true"><mi mathvariant="bold-italic" is="true">M</mi><mo mathvariant="bold" is="true">′</mo></msup></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">×</mo><mi is="true">w</mi><mo is="true">×</mo><mi is="true">d</mi></mrow></msup></mrow></math>, where d is the dimensionality of vectors. In Fig. 5, we show the difference between the input of text CNN (a) and NCNN (b), where the x-axis denotes the direction of text sequence and random paths, z-axis denotes the dimensionality of word vectors. The input of NCNN has y-axis, which denotes the number of random walk paths.
Based on the input of NCNN, we propose the cross paths convolution (CPC) to learn the high-order similarity of neighbors. The CPC operation is that convolution is convolved among more than one random path, as shown in Fig. 6. The high-order similarity of neighbors may be extracted from more than one random path, and CPC can learn this information by assembling different random walk paths with Conv Core. In Fig. 6, different random paths starting from node 1 are generated and transformed into a 2D matrix. Then a Conv Core assembles the similarity information of orange, blue, and green blocks.
The CPC operation can be easily achieved using 2D CNN model, except for the first Convolution Layer because the input of NCNN has an extra dimension of the vectors' dimensionality. Here, we propose a Polar Layer as the medium between the input and the first Convolution Layer of the 2D CNN model. The Polar Layer carries out a convolution operation between a filter m∈Rh×wm×d<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">m</mi></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">×</mo><msub is="true"><mi is="true">w</mi><mi is="true">m</mi></msub><mo is="true">×</mo><mi is="true">d</mi></mrow></msup></mrow></math> and a matrix M′∈Rh×w×d<math><mrow is="true"><mrow is="true"><msup is="true"><mi mathvariant="bold-italic" is="true">M</mi><mo mathvariant="bold" is="true">′</mo></msup></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">×</mo><mi is="true">w</mi><mo is="true">×</mo><mi is="true">d</mi></mrow></msup></mrow></math> , which produces another matrix O∈Rh×wo<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">O</mi></mrow><mo is="true">∈</mo><msup is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">R</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">×</mo><msub is="true"><mi is="true">w</mi><mi is="true">o</mi></msub></mrow></msup></mrow></math>:(3)O(:,i)=∑j=1dm(:,j)°M′(:,i:i+wm-1,j).<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">O</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">:</mo><mo is="true">,</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">d</mi></munderover><mrow is="true"><mi mathvariant="bold-italic" is="true">m</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">:</mo><mo is="true">,</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow><mo is="true">°</mo><mrow is="true"><msup is="true"><mi mathvariant="bold-italic" is="true">M</mi><mo mathvariant="bold" is="true">′</mo></msup></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mo is="true">:</mo><mo is="true">,</mo><mi is="true">i</mi><mo is="true">:</mo><mi is="true">i</mi><mo is="true">+</mo><msub is="true"><mi is="true">w</mi><mi is="true">m</mi></msub><mo is="true">-</mo><mn is="true">1</mn><mo is="true">,</mo><mi is="true">j</mi><mo stretchy="false" is="true">)</mo></mrow><mo is="true">.</mo></mrow></math>
In Eqs. (3), “°<math><mo is="true">°</mo></math>” denotes the Hadamard product, and the values in “()” denote the indexes of the corresponding dimension. Note that “:” denotes indexing all values. wo is determined by the convolution operation. The structure of the Polar Layer is shown in Fig. 7, where the x-axis denotes the direction of this operation, the y-axis denotes the height (h) of M', and the z-axis denotes the dimension (d) of the vectors of the nodes. From Fig. 7, we can observe that the z-axis of the output from the Polar Layer is flattened, yielding a proper format for the processing of the Convolution Layer in the 2D CNN model. In the following description, we represent the operation of the Polar Layer as O=m⊗M′<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">O</mi></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">m</mi></mrow><mo is="true">⊗</mo><mrow is="true"><msup is="true"><mi mathvariant="bold-italic" is="true">M</mi><mo mathvariant="bold" is="true">′</mo></msup></mrow></mrow></math>.
Fig. 8 shows the structure of the Network-based Convolution Neural Network (NCNN) model. Note that the shape marked in parentheses is the output shape of each layer, where N denotes the number of samples (with length Sl) and hi, wi are determined by the convolution or max-pooling operation. We can view a Polar Layer output as the input of a 2D Convolution Layer (Conv2D) with only one channel (generally, the matrix of a picture has three channels). After two Conv2D layers, two fully-connected layers are connected to produce the classification results.
The random walk sampling can participate in not only feature extracting but also predicting. In predicting tasks like text classification, given a document d, a prediction model is used to predict the label l of d. Generally, the prediction model calculates the probability p(li)<math><mrow is="true"><mi is="true">p</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo stretchy="false" is="true">)</mo></mrow></math> of d belonging to label li∈L<math><mrow is="true"><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">L</mi></mrow></math>, and then the label with maximum probability is selected as the predicted label, i.e., l=argmax{(p(li))},li∈L<math><mrow is="true"><mi is="true">l</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mo is="true">arg</mo><mo movablelimits="true" is="true">max</mo><mrow is="true"><mo stretchy="false" is="true">{</mo><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">p</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo stretchy="false" is="true">)</mo></mrow><mo stretchy="false" is="true">)</mo></mrow><mo stretchy="false" is="true">}</mo></mrow><mo is="true">,</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo is="true">∈</mo><mi is="true">L</mi></mrow></math>. In this paper, the last layer of the CNN model for classification produces the probability distribution of all labels. The label with the maximum probability is the final selected label as the prediction result.ALGORITHM 1 Path Generation for Source TextInput p, q, document d, path length w, path number hOutput M(h×w)<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">h</mi><mo is="true">×</mo><mi is="true">w</mi><mo stretchy="false" is="true">)</mo></mrow></msub></math>1: Represent d as g, i.e., g=ψ(d)<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">g</mi></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ψ</mi><mo stretchy="false" is="true">(</mo><mrow is="true"><mi mathvariant="bold-italic" is="true">d</mi></mrow><mo stretchy="false" is="true">)</mo></mrow></math>2: Initialize path_num, length as 0; initialize M as null; initialize path as null3: while path_num < h do4: if length is 0 then5: randomly choose a start node s6: append s to path7: else8: choose the next node v using (1) and (2) with p, q, g9: length = length + 110: append v to path11: if length is w then12: path_num = path_num + 113: append path to M14: initialize path as null; initialize length as 015: end if16: end if17: output: M(h×w)<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">h</mi><mo is="true">×</mo><mi is="true">w</mi><mo stretchy="false" is="true">)</mo></mrow></msub></math>18:end whileALGORITHM 2 Path Generation for Over-samplingInput p, q, document d1, and d2, path length w, path number hOutput M(h×w)<math><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">h</mi><mo is="true">×</mo><mi is="true">w</mi><mo stretchy="false" is="true">)</mo></mrow></msub></math>1. Represent d1 and d2 as g, i.e., g=ψ(d1,d2)<math><mrow is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">g</mi></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ψ</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">d</mi></mrow><mrow is="true"><mn mathvariant="bold" is="true">1</mn></mrow></msub><msub is="true"><mrow is="true"><mo mathvariant="bold" is="true">,</mo><mi mathvariant="bold-italic" is="true">d</mi></mrow><mrow is="true"><mn mathvariant="bold" is="true">2</mn></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math>1. Follow steps 2 and 3 of Algorithm 1 to get the output matrix MNext, we describe an electing strategy for predicting classification results. By using the random walk sampling, we generate n samples of d. Then, the corresponding label is predicted for every sample, and the final predicted label for d is chosen from them. Fig. 9 shows the process of the electing strategy. We introduce two strategies to obtain the final label: ES-mean and ES-max.
ES-mean strategy first gets the probability that n samples are predicted to belong to label li, and then uses Eqs. (4) to calculate the probability of Mk belonging to label li, where Mk∈{M1,M2,...,Mn}=PathGenn(g)<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mi is="true">k</mi></msub><mo is="true">∈</mo><mrow is="true"><mo stretchy="false" is="true">{</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mn is="true">1</mn></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mn is="true">2</mn></msub><mo is="true">,</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">M</mi></mrow><mi is="true">n</mi></msub><mo stretchy="false" is="true">}</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">P</mi><mi is="true">a</mi><mi is="true">t</mi><mi is="true">h</mi><mi is="true">G</mi><mi is="true">e</mi><msup is="true"><mi is="true">n</mi><mi is="true">n</mi></msup><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">g</mi><mo stretchy="false" is="true">)</mo></mrow></mrow></math>, g=ψ(d)<math><mrow is="true"><mi is="true">g</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi is="true">ψ</mi><mo stretchy="false" is="true">(</mo><mi is="true">d</mi><mo stretchy="false" is="true">)</mo></mrow></math>.(4)p(li)=1n∑k=1npk(li)<math><mrow is="true"><mi is="true">p</mi><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo stretchy="false" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mn is="true">1</mn><mi is="true">n</mi></mfrac><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">k</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">n</mi></munderover><msub is="true"><mi is="true">p</mi><mi is="true">k</mi></msub><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo stretchy="false" is="true">)</mo></mrow></mrow></math>
ES-max strategy first gets predicted labels from n samples and then chooses the most occurred label as the final label of document d, as Eqs. (5) shows, where xki=1<math><mrow is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi mathvariant="italic" is="true">ki</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">1</mn></mrow></math> if argmax\{ (pk(li))}=lk<math><mrow is="true"><mrow is="true"><mtext is="true">argmax\{ (</mtext></mrow><msub is="true"><mi is="true">p</mi><mi is="true">k</mi></msub><mrow is="true"><mo stretchy="false" is="true">(</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">}</mo></mrow><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msub is="true"><mi is="true">l</mi><mi is="true">k</mi></msub></mrow></math>, and xki=0<math><mrow is="true"><msub is="true"><mi is="true">x</mi><mrow is="true"><mi mathvariant="italic" is="true">ki</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mn is="true">0</mn></mrow></math> otherwise.(5)p(li)=∑kxki∑k∑jxkj=∑kxkin<math><mi is="true">p</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">l</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mi is="true">k</mi></msub><msub is="true"><mi is="true">x</mi><mi mathvariant="italic" is="true">ki</mi></msub></mrow><mrow is="true"><msub is="true"><mo is="true">∑</mo><mi is="true">k</mi></msub><msub is="true"><mo is="true">∑</mo><mi is="true">j</mi></msub><msub is="true"><mi is="true">x</mi><mi mathvariant="italic" is="true">kj</mi></msub></mrow></mfrac><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub is="true"><mo is="true">∑</mo><mi is="true">k</mi></msub><msub is="true"><mi is="true">x</mi><mrow is="true"><mi is="true">k</mi><mi is="true">i</mi></mrow></msub></mrow><mi is="true">n</mi></mfrac></math>
We can see the electing strategy as the assembling of the output from the last layer of CNN. By using electing strategy, the prediction result is decided after many trials, and the final result is the most conservative choice from multiple attempts.
In this section, we introduce two datasets and test the performance of the proposed model by comparing it with different baselines in text classification. The first dataset is Reuters 21578, which contains a highly uneven distribution of samples in different classes. The other dataset, WebKb, is the balanced dataset. With the same idea proposed in literature (Wu et al., 2014), we manually transform the dataset into imbalanced dataset by eliminating some samples in certain classes.
Reuters 21578 is manually collected and classified from Reuters Ltd. The source dataset suffers from a very skewed class distribution. So we choose a refined dataset of Reuters 21578, which is called R52 (Craven et al., 2003). In Table 1, we show part of the classes in R52, in which the number of samples in “earn” is ten times more than in “bop.” Moreover, some classes like “nickel,” “platinum” have samples less than 5. So R52 still has an uneven class distribution, which may cause inefficiency training in some classes.
WebKb is a series of texts from web pages collected by the World Wide Knowledge Base project (Craven et al., 2003). This dataset contains four classes, each containing enough samples for training because applying many approaches for solving data imbalance directly on the source datasets can only get a very limited improvement on performance. So, we eliminate 90% of the samples in the “project” class, which contains the minimum number of samples in the source data, to ensure the dataset is imbalanced. In Table 2, we show the number of samples in each class before and after eliminating samples.
All the methods used in this paper are implemented in Windows 10 based on Python 3.7 with TensorFlow 2.0, which is used to build the neural network. All the models and methods are run on a desktop computer that is configured for Intel® Core™ i5-6600 CPU @ 3.30 GHz, RAM 16.0 GB, GTX2060 6 GB.
We choose Accuracy, F1-measure, Precision, and Recall as the evaluation measures. Table 3 shows the details of these metrics. In this paper, F1-measure, Precision and Recall are calculated considering their macro average and micro average because the datasets contain more than two labels (multi-class). The micro average calculates metrics globally by counting the total TP, FN, and FP, while the macro average calculates metrics for each label and gets their unweighted mean. Note that the value of Accuracy, micro F1-measure, micro Precision, and micro Recall are equal in multi-classification. So the results will only show Accuracy, macro F1-measure, macro Precision, and macro Recall.
We use the following models to be the classifiers. Note that some models are only for vector representation of documents, and these models all use SVM as classifiers. Table 4 describes the structures of all the classifiers.
CNN is a convolution neural network for text classification. The architecture of CNN is similar to the model proposed by (Kim, 2014) with fine-tuning in parameters.
BoW.tf is a bag-of-words representation model with term frequency as the features.
BoW.tfidf (Naderalvojoud et al., 2015) is a bag-of-words representation model with TFIDF values as the features.
We choose the following methods to solve data imbalance and their combination with the above classifiers as the comparison methods. Moreover, their combination is represented as a “method-classifier” form, e.g., a CNN classifier combined with RO will be described as “RO-CNN.”
RO: Random over-sampling (RO) method is a method for directly duplicating the samples in certain classes of the datasets.
SMOTE (Chawla et al., 2002): Synthetic minority over-sampling technique (SMOTE) generates new synthetic examples for minority classes, which is different from RO.
PNF (Naderalvojoud et al., 2015): PNF is a term weighting approach to solve data imbalance problems instead of over-sampling.
BDSK (Song et al., 2016): BSDK is a bi-directional sampling method based on K-means and SMOTE (for over-sampling), which over-samples minority classes and under-samples the majority classes to a certain number. In this paper, we only use its over-sampling strategy because the under-sampling leads to performance reduction.
RWO (Zhang, & Li, 2014): A random walk over-sampling method for numerical data. In this paper, we apply this approach to the BoW representation of text.
TextGen (Shaikh et al., 2021): A background-based method for over-sampling. This method generates new texts through the generation model after a fine-tuning based on the imbalanced datasets. In this paper, the generation model is based on the model from https://minimaxir.com/2018/05/text-neural-networks/.
The proposed model is represented as “NCNN,” and its combination with an electing strategy is represented as “n-NCNN” for ES-mean and “x-NCNN” for ES-max.
We use the sklearn module to calculate the Accuracy, F1-measure, Recall, and Precision value and use its svm.SVC function to implement SVM with its default parameters except for c. We search c in {0.1, 0.5, 1, 2, 5, 10} to choose the best performance as the final results.
Note that the Embedding layer of NCNN adopts a pre-trained embedding from a large corpus, including R52 and WebKb, to avoid changing the feature space of the original data when training the random walk paths. The data is preprocessed by removing the stop words for BoW.tf and BoW.tfidf. For CNN and NCNN, the data will keep all the words. The training epochs of CNN, RO-CNN, TextGen-CNN, and NCNN are set to 10.
For R52, the over-sampling methods are only applied to the classes of training data with the number of samples fewer than 400. These classes are over-sampled to make the number of samples reach 400. We keep the classes with the number of samples fewer than 400 as it is. Though this may not completely solve the problem of data imbalance, it can get good results without too many samples to be trained, which will need a much higher time cost. We also tried under-sampling the majority classes to 400, but results showed that this would cause a performance reduction. For WebKb, the over-sampling methods are applied to all the classes whose number of samples is fewer than 1097 (the number of samples in the majority class). Table 5 and Table 6 show the details of the increased number of the expanded data. Table 5 only lists four classes to show the difference between classes that are over-sampled and those not over-sampled.
Table 7 and Table 8 show the results achieved with different methods. The proposed methods, including NCNN, n-NCNN, and x-NCNN, are implemented with p = 1 and q = 1 (equal to a random walk). Because the RWO, SMOTE, PNF, and BDSK methods can only be applied to numerical data, CNN only has the combination of RO and TextGen, i.e., RO-CNN and TextGen-CNN. Moreover, PNF is only combined with BoW.tf because the PNF method is a term weighting method different from TFIDF. PNF takes the category membership into account, while TFIDF does not. We show the results of CNN in the first line of Table 7 and Table 8: CNN achieves the best performance among CNN, BoW.tf, and BoW.tfidf when no over-sampling strategy is implemented on the original data. Moreover, the results show that all the over-sampling strategies can get a performance improvement.
The deep learning methods, like RO-CNN, TextGen-CNN, NCNN, n-NCNN, and x-NCNN, all achieve excellent results in Accuracy. However, RO-CNN and TextGen-CNN perform poorly in macro F1-measure, macro Recall, and macro Precision for R52. For WebKb, TextGen-CNN gets a better promotion compared with RO-CNN. As mentioned above, TextGen-CNN is a knowledge-based method for over-sampling, which learns the styles of the samples belonging to a class and generates new texts to increase the number of samples based on a powerful text generation model. However, we can observe that this method can not work well on R52. R52 contains many minority classes with a few samples (17 classes contain less than ten samples), which can not guarantee that a generation model is well-trained based on these samples. In Table 9, the mean values of F1-measure with respect to different sizes of classes for R52 are listed. The value for “C1-10” of TextGen-CNN is 0.0000, denoting that the generation model generates wrong samples for the classes with too few samples (”C1-10” denotes the classes with the number of samples less than 10). We can view the Accuracy measure as the overall accuracy of all the classes, where the minority classes have little effect on the final results because the number of samples will influence the effect. On the contrary, the macro mean value of metrics, including macro F1-measure, macro Recall, and macro Precision, take every class into equal account, regardless of the number of samples. So the macro mean value can reflect how a method for the problem of imbalanced data performs on the minority classes. The poor behavior of RO-CNN in the macro mean value of metrics indicates that a RO strategy will not always work because the simple copy of samples may not correctly reflect the real feature space of training data.
For the methods based on the BoW model, SMOTE performs better than RO in Accuracy, macro F1-measure, and macro Recall. BDSK is also a SMOTE-based method, which uses K-means to over-sample the samples in similarity feature distribution. Because SMOTE can only be applied to numerical data, text data must be transferred into numerical form, the performance of which will be affected by feature extraction. Also, the transformation from text data into numerical data may lose valuable information. NCNN is to over-sample directly on the text data, which catches the high-level semantical information of the text. The performance of NCNN gets a significant improvement compared with RO and SMOTE. RWO, an improved over-sampling way with the same synthetic sampling idea as SMOTE, is a random walk based method. In section 2.3, we compare its random walk with our proposed method: the random walk of RWO is different from NCNN in many aspects. The results show that SMOTE performs better than RWO. Furthermore, TFIDF may be more suitable for RWO than TF features.
The term weighting methods, like TFIDF and PNF, consider the prior term weighting information in the training data and overcome the impact of data imbalance. TFIDF is a standard weighting strategy that has been proved to be effective in many classification tasks. However, TFIDF does not take into account the category information. So it still needs the over-sampling strategy to get a considerable performance. PNF considers the difference between classes, and from the results, we can observe that PNF achieves better performance than TFIDF. Also, PNF gives a significant promotion on Accuracy for both two datasets. But the promotion of minority classes is limited, e.g., F1-measure of “C1-10” (R52) and “project” (WebKb) in Table 8 and Table 9.
The proposed methods, including NCNN, x-NCNN, and n-NCNN, all get good results. The best value of Accuracy, macro F1-measure, macro Recall, and macro Precision are all obtained by the proposed methods. The proposed methods are marked in italics in Tables, and the best values are marked in bold. Though RO-CNN also over-samples on the text data instead of numerical data, the F1-measure between NCNN and CNN is profoundly different. We may attribute this difference to the high-level operation of NCNN. NCNN over-samples more extending samples through Algorithm 2, which produces profoundly different samples even from the same couple of samples. The variety of samples can help with reducing the impact of overfitting. An electing strategy further improves the performance of NCNN, certifying its effectiveness in classification tasks.
We also explore the F1-measure of every class and the values of each method. We aim to find out what affects the F1-measure in Table 7 and Table 8. We give the results in Fig. 10 (for R52) and Table 10 (for WebKb). For R52, because it is lengthiness to present the results of 52 classes, we present the number of classes whose F1-measure values are in a certain range. The number of classes with F1-measure value in range “>0.9”, “>0.7&<=0.9”, “>0.4&<=0.7”, “>0&<=0.4” and “=0” is counted separately. And then, the values are normalized into 0–1, as is shown in Fig. 10. A high proportion in the bottom region of the column indicates a high performance of the corresponding method. The results clearly show the advantage in range 0.7–0.9 of NCNN, n-NCNN, and x-NCNN. In Table 9, we show the mean values of F1-measure with respect to different sizes of classes for R52. Results show that the proposed method is competitive in majority classes with other methods and has an advantage in minority classes. In Table 10, F1-measure value with respect to each class for WebKb is presented. These results once again confirm the advantage of the proposed method in minority classes.
Next, we analyze the influence of p and q, which are two hyper-parameters that change the choice of the next step during the generation of a random path, as described in Eqs. (2).
Fig. 11 and Fig. 12 show the results from NCNN, n-NCNN, and x-NCNN for R52 and WebKb separately, with the same setup as the above experiments. The experiment is implemented several times with different p and q: we first fix p to 1 and change q from 1 to 5, then fix q to 1 and change p from 1 to 5.
From the results, we can observe that value of metrics achieved from NCNN, n-NCNN, and x-NCNN for R52 follow an M−shaped trend with the change of p / q. The shallow point is located in p / q = 1 / 1 of the x-axis, indicating that the purely random walk can not achieve the best in capturing the high-level structural and semantic information of the text. Moreover, the peaks exist in the points with p larger or smaller than q. For WebKb, we can also observe that the best performance is achieved with p larger or smaller than q, though the trend is different from R52.
We show the above results to argue that a targeted walk aimed at the similarity of nearest neighbors or faraway neighbors can better capture the high-level information instead of the purely random walk. For R52, the best performance occurs in x-NCNN when p / q = 3 / 1. The value of Accuracy is 0.9474, and the value of macro F1-measure is 0.7899. For WebKb, the best performance occurs in n-NCNN when p / q = 1 / 5. The value of Accuracy is 0.8966, and the value of macro F1-measure is 0.8780.
Though this paper use p = 1 and q = 1 to get all the results in the experimental section, the influence of p and q can shed light on how to fine-tune the two hyper-parameters. We can try many times with different p and q, and use validation data to judge which set of values is the best. We can also skip some choices, like p = 1, q = 1, and p = 2, q = 1, according to the influence trend mention above. Then, we choose p and q that get best validation results to predict for the test data.
This study proposed a network-based feature extraction model for text analysis to deal with imbalanced text data. The random walk was introduced to generate new synthetic samples. An NCNN model was then proposed to combine the novel feature extraction model with the classical deep learning method (CNN). A Polar Layer is designed to fit the data from random walk paths to CNN.
By comparing with RO, SMOTE, BDSK, PNF, RWO, TextGen combined with CNN and other classification models, we proved the effectiveness of the proposed NCNN model on the imbalanced data. We also introduced an electing strategy for predicting to improve accuracy, which is useful in some tasks like classification, especially when the number of data is limited. The results showed that the NCNN model with an electing strategy achieved a significant improvement in performance.
Exploration was also conducted to find the best combination of p and q for the proposed model (NCNN, n-NCNN, and x-NCNN). We observed that the trend following the change of p / q presents an M-shaped curve. The best performance occurred when p / q = 3 / 1 for R52 and p / q = 1 / 5 for WebKb, indicating that a targeted walk could capture the high-level information better than what a random walk could do.
The limitation of our study is that the random walk paths lose the sequence information of the original text, which originates in the way of network construction. The future work will concentrate on combining the original text with the random walk paths to introduce the sequence order, and then apply the model to more broad text analysis tasks.
Keping Li: Writing – original draft, Supervision, Project administration, Funding acquisition. Dongyang Yan: Conceptualization, Methodology, Writing – original draft, Writing – review & editing, Software, Formal analysis, Investigation, Resources. Yanyan Liu: Validation, Resources, Writing – review & editing. Qiaozhen Zhu: Validation, Data curation, Visualization, Writing – review & editing.
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
This work was supported by the National Natural Science Foundation of China (Grant Nos.71942006, 71621001), and the Research Foundation of State Key Laboratory of Railway Traffic Control and Safety, China, Beijing Jiaotong University (Grant No. RCS2021ZT001). The authors sincerely thank Prof. Tiezhu Jin for his meticulous polishing of the article.