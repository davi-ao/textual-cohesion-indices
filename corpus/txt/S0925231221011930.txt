Semantic segmentation is a fundamental task with a multitude of applications [24], [52] in computer vision, wherein indoor semantic segmentation is one of the most challenging problems due to complex and varied objects with severe occlusions. This task may greatly benefit from the availability of RGB-D data, which has recently been increasing thanks to the widespread use of depth sensors.
In recent years, with the flourishing of Convolutional Neural Networks (CNNs), various CNN-based approaches have been proposed to exploit depth information for RGB-D segmentation. The depth information can be directly concatenated with RGB and further processed with a single CNN module [29], which treats RGB and depth in a symmetric way. However, RGB and depth are inherently different from each other: the RGB values capture the photometric appearance properties in the projected image space, while D, the depth channel, is a geometric property. Though the pixels of a patch are in spatial proximity on the image plane, they are not necessarily geometrically coherent in 3D space. Thus, simply concatenating RGB and depth as input features violates the local correlation assumption of effective convolution, that is, the pixels/neurons in a local region are highly correlated. Operating a set of uncorrelated values by directly using the same convolution filters seems sub-optimal.
To address the asymmetric modality problem between RGB and depth information, most studies focus on designing dedicated CNN architectures that process RGB and depth with two separate streams, The obtained features are often concatenated for further processing [46], [9], [47], [35], resulting incremental computational cost due to the overuse of two learning flows. In contrast, we propose to fuse RGB and depth early by attributing RGB patches with weights learnt from depth patches, and consume with single flows. The fused data can then be well consumed by CNNs designed for RGB segmentation with minimal modification (see Fig. 3).
More specifically, our method treats RGB and depth information asymmetrically by fusing the two types of information in a multiplicative fashion, which enriches the discriminative power of local image patches and makes the convolution more geometry-aware in the learning process. An RGB-D segmentation result with and without the RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is shown in Fig. 1.
A key characteristic of our method lies in its simplicity (see Fig. 2 (b2)). Our approach merely adds a component that re-weights the local pixel intensity values of a patch on-the-fly before it is fed into a standard encoder-decoder CNN. The re-weighting is accomplished by a simple multi-layer perceptron (MLP) [37] that learns to infer the weights as a function of the depth channels. In a nutshell, our proposed module can be plugged into any existing RGB segmentation models, and the overall model can then be trained in an end-to-end manner.
Some inspiration for our method may be drawn from non-linear edge-preserving filters, such as the bilateral filter [44], which have proven highly effective for a variety of computational photography tasks. The bilateral filter averages local pixels using weights that depend not only on their spatial proximity but also on the similarity between their values, thus obtaining more relevant averages. Inspired by the effectiveness of bilateral filtering, we present a technique that learns per-patch pixel correlation weights from the geometry of their corresponding 3D points. The learned correlation weights are used to multiply the pixels in the patch, making the image patches geometry-aware and hence more discriminative to each other, even if their original color appearances are similar (see Fig. 5). Then, the convolution is applied on the weighted image patches.
We conducted extensive RGB-D segmentation experiments and ablation studies on the challenging NYU-Dv2 [39], SUN-RGBD [40] and Cityscapes [11] benchmarks. The proposed approach is shown to be effective for regions where the RGB color of different objects are quite indistinguishable (see Fig. 1). We show that the proposed RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D has a consistent improvement over several strong baselines. We released the codes and involved parameter settings to facilitate others to repeat this work1
CNNs have been widely and successfully used for semantic segmentation of RGB images [29], [2], [6], [20]. As a pioneering attemptation, Gupta et al. [17] proposed to enhance geometry information and represent RGB-D images as RGB-HHA images, where the geometric channels encode Horizontal disparity, Height above ground and normal Angle to the vertical axis, in place of a single Depth channel. CNNs designed for RGB images can be easily adapted to take the original RGB and the additional HHA channels as input, and thus extended to support RGB-HHA images [29] (Fig. 2 (b1)). These approaches treat RGB and geometry channels in the same and symmetric manner.
CNNs designed for RGB-D images usually recognize the fact that RGB and geometry channels are different from each other, and feed them into two parallel CNN streams, fusing the resulting features before the up-sampling layers [47] (Fig. 2 (a1)) or before the final prediction layers [9], [45], [42] (Fig. 2 (a2)). These two-stream methods often adopt the additive fusion, for example, [45] presents the Modality and Component Aware Feature Fusion, which uses an additive fusion by incorporating different levels of structure sparsity regularization from different modalities. [42] focuses on the low-level differences between RGB and HHA data and combines the RGB and depth features in an additive way by projecting them in a common latent space. Besides, Park et al. [31] and Chen et al. [8] proposed MMF-Net and S-A gate operation, respectively, to fuse RGB and depth features in multi-levels. These approaches treat RGB and geometry information differently due to the use of separate networks. They are yet faced with two limitations: 1) it is hard to decide when is the best stage for the fusion to happen; and 2) the two-stream or multi-level way often causes increasing computation (refer to Fig. 3 for more information).
RGB-D data can also be represented in 3D space, as either colored voxels or a point cloud. In these representations, the RGB values become an attribute attached to the geometry one. Various neural networks have been extended to consume voxels [53], [41] or point clouds [32], [33], [26]. These approaches treat RGB and geometry in an inherently different way: the geometry tells where the feature extractors (usually convolution operator) are applied, while the RGB is taken as the input to the extractors. However, lifting depth data into 3D and processing it is ineffective, as the 3D oriented networks are often of significantly heavier computation.
To better leverage geometry data while minimizing computational complexity in 3D, the geometry has been used for attributing features [28], [47] or the network is designed and regularized to be more geometry-aware [23]. For example, Lin et al. [28] split the RGB image into layers with different “scene-resolution” by depth information, and then CNNs are applied at different resolution levels. Wang et al. [47] proposed to weight pixels based on a hand-crafted Gaussian function of the depth difference between two corresponding pixels, while we propose to learn the weighting function from the geometry of 3D points corresponding to pixels in patches. Jiao et al. [23] proposed a multi-task learning scheme, which exploits the geometry embedding learning to predict the depth information, and thus boosts the prediction accuracy of semantic segmentation. This work addresses the problem from the network design and the regularization view, which is orthogonal to our approach. [49] presents a novel operator called malleable 2.5D convolution, to learn the receptive field along the depth-axis. [13], [25] employ the attention mechanism to compute attention maps which are multiplied by input feature maps for adaptive feature fusion to address the asymmetric modality problem. However, this type of methods usually results in a huge increase in computation while our method is based on the patch rather than the whole feature map and adds only a small amount of computation to the original.
Method Intuition. The raw input to RGB-D segmentation is an RGB-D image I which is composed of the RGB channels IRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> and depth channel ID<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math>. The RGB values capture the photometric appearance properties in the projected image space, while D, the depth channel, expresses the geometric property. In fact, the HHA2 channels IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> are derived from ID<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math> and usually employed to replace the depth channel. Compared to the pure depth information, the IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> are shown to be more effective in encoding geometry information than the raw depth channel [17], and thus are widely used in previous methods [29], [27], [51], [9]. However, HHA, as well as the pure depth information, is naturally different from the RGB information. That is, the RGB expresses the photometric appearance property, while the depth and HHA focus more on the geometric characteristic. Consequently, it is unreasonable by directly concatenating RGB with HHA/depth for further processing via a single CNN, leading to sub-optimal performance. Therefore, in this work, we propose to firstly enable the RGB features with the geometric property and then integrate them with the HHA/depth features. To this end, we leverage the 3D coordinates from each pixel to yield geometry-wise weights, which are thereafter employed on the raw RGB features for perceiving the geometric information. In the following, we will provide the network input first, followed by the proposed re-weighting approach. We end this section with the detailed network architectures.
Existing methods take as inputs the RGB and HHA/depth features. Nevertheless, we argue that it is sub-optimal to directly concatenate these two sets of features. Instead, the RGB features we used in our network is multiplied with the geometry-wise weights, which are learned based on the 3D coordinates of pixels. Particularly, we take the function used in [34], [22], [50], [30], which is leveraged to generate the 3D coordinates pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> of a point from its projection onto the image.(1)pz=pd/S,px=(pc-Cc)/Fx×pz,py=(pr-Cr)/Fy×pz,<math><mrow is="true"><mtable is="true"><mtr is="true"><mtd columnalign="left" is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">z</mi></mrow></msub><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub><mo is="true">/</mo><mi mathvariant="double-struck" is="true">S</mi><mo is="true">,</mo></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">x</mi></mrow></msub><mo is="true">=</mo><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo is="true">-</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">C</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo stretchy="true" is="true">)</mo><mo is="true">/</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">x</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">z</mi></mrow></msub><mo is="true">,</mo></mtd></mtr><mtr is="true"><mtd columnalign="left" is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">y</mi></mrow></msub><mo is="true">=</mo><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">-</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo stretchy="true" is="true">)</mo><mo is="true">/</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">y</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">z</mi></mrow></msub><mo is="true">,</mo></mtd></mtr></mtable></mrow></math>These parameters can be classified into three groups. 1) [px,py,pz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">x</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">y</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">z</mi></mrow></msub></mrow></math>] are the 3D coordinates of a point in the camera coordinate system, [pc,pr<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></mrow></math>] represent its projection onto the image according to the pinhole camera model and pd<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">d</mi></mrow></msub></mrow></math> is the depth value of this pixel. 2) [Fx,Fy,Cc,Fr<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">x</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">y</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">C</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></mrow></math>] are the parameters of the camera. More specifically, Fx<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">x</mi></mrow></msub></mrow></math> and Fy<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">F</mi></mrow><mrow is="true"><mi is="true">y</mi></mrow></msub></mrow></math> are the focal length along the x and y directions, Cc<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">C</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow></msub></mrow></math> and Cr<math><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="double-struck" is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></mrow></math> are coordinates of the principal point. 3) The only tunable parameter S<math><mrow is="true"><mi mathvariant="double-struck" is="true">S</mi></mrow></math> denotes the pre-defined re-scaling scalar which is not model-sensitive (since additional normalization can be employed), and we set S<math><mrow is="true"><mi mathvariant="double-struck" is="true">S</mi></mrow></math> as the maximum value of the depth value in practice to convert pz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">z</mi></mrow></msub></mrow></math> into the range of 0 to 1.
Compared to the IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math>, the 3D coordinate channel Ixyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> is more closely related to the actual position of objects in 3D space. Thanks to the usage of Ixyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> to learn the weights of IRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math>, the new weighted IRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> is enhanced with more geometric information, and the inputs to our method becomes (IRGB,IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> or ID,Ixyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>).
To facilitate the sliding window fashion in CNNs, our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D also operates on patch (PRGB,PGeo,Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">Geo</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>) of each image, where PGeo<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">Geo</mi></mrow></msub></mrow></math> denotes the geometry channel features which could be PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> or PD<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math>. Since HHA usually produces better results than the raw depth, we thus adopt PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> as PGeo<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">Geo</mi></mrow></msub></mrow></math> in the following subsections, i.e., (PRGB,PHHA,Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>) patch features. The different performance with inputs of PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> and PD<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math> is compared in Section 4.
The ordinary convolution of an RGB image patch PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> may be denoted as f=Conv(K,PRGB)<math><mrow is="true"><mi is="true">f</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mi mathvariant="italic" is="true">Conv</mi><mo stretchy="true" is="true">(</mo><mi is="true">K</mi><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo stretchy="true" is="true">)</mo></mrow></math>, where K denotes the learnable kernels with the shape of k1×k2×Cin×Cout<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">in</mi></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">out</mi></mrow></msub></mrow></math>. k1<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub></mrow></math> and k2<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math> are the spatial dimensions of the kernel, Cin<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">in</mi></mrow></msub></mrow></math> and Cout<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">out</mi></mrow></msub></mrow></math> are the number of channels in the input and output feature map, respectively, the PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> are tensors in shape k1×k2×3<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">×</mo><mn is="true">3</mn></mrow></math> correspondingly, and f denotes the features extracted from the patch. The approach in Fig. 2b1) may be given as:(2)f=Conv(K,[PRGB,PHHA]),<math><mrow is="true"><mi is="true">f</mi><mo linebreak="goodbreak" is="true">=</mo><mi mathvariant="italic" is="true">Conv</mi><mo stretchy="true" is="true">(</mo><mi is="true">K</mi><mo is="true">,</mo><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub><mo stretchy="true" is="true">]</mo><mo stretchy="true" is="true">)</mo><mo is="true">,</mo></mrow></math>where [·,·]<math><mrow is="true"><mo stretchy="true" is="true">[</mo><mo is="true">·</mo><mo is="true">,</mo><mo is="true">·</mo><mo stretchy="true" is="true">]</mo></mrow></math> denotes concatenation along the channel dimension, and both PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> and PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> are tensors in shape k1×k2×3<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">×</mo><mn is="true">3</mn></mrow></math>. Note that only additive interaction between color and geometry information is enabled in this formulation. More precisely, the features are formed as linear combinations of RGB and geometry, and applying non-linear activation on these combinations.
The RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D fusion architecture we propose, as shown in Fig. 2b2), is:(3)f=Conv(K,[PRGBW,PHHA]),<math><mrow is="true"><mi is="true">f</mi><mo linebreak="goodbreak" is="true">=</mo><mi mathvariant="italic" is="true">Conv</mi><mo stretchy="true" is="true">(</mo><mi is="true">K</mi><mo is="true">,</mo><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub><mo stretchy="true" is="true">]</mo><mo stretchy="true" is="true">)</mo><mo is="true">,</mo></mrow></math>where PRGBW<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub></mrow></math> (a tensor with same shape as PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math>, i.e., k1×k2×3<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub><mo is="true">×</mo><mn is="true">3</mn></mrow></math>) are the depth-weighted RGB patches.
For per-patch RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D operation we propose, as shown in Fig. 4, is:(4)PRGBW=PRGB⊗Wdep,<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mrow is="true"><mo is="true">⊗</mo></mrow><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub><mo is="true">,</mo></mrow></math>where Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> (a tensor in shape k1×k2<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msub><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">k</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math>) are the depth weights learned from Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>, and ⊗<math><mrow is="true"><mo is="true">⊗</mo></mrow></math> signifies spatial multiplication. More precisely, the spatial multiplication can be depicted as PRGBW(i,j,c)=PRGB(i,j,c)×Wdep(i,j)<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub><mo stretchy="true" is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">,</mo><mi is="true">c</mi><mo stretchy="true" is="true">)</mo><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo stretchy="true" is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">,</mo><mi is="true">c</mi><mo stretchy="true" is="true">)</mo><mo is="true">×</mo><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub><mo stretchy="true" is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo stretchy="true" is="true">)</mo></mrow></math>, where i,j,c<math><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">,</mo><mi is="true">c</mi></mrow></math> are the indices to the element in the corresponding tensors. The only difference between Eqs. (2), (3) is the depth weighting achieved by the multiplication with Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math>. We show that the weighted PRGBW<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub></mrow></math> patches are more discriminative than the original PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> patches in Fig. 5. Note that both additive and multiplicative interactions between RGB and depth are modeled in this formulation.
Note that the “pixels” of Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> are points in 3D space, and we denote the point corresponding to the center pixel inside this patch as pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>. And Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> is designed to reflect the local geometric correlations across each patch, we thus translate Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> to the local coordinate system of pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>, yielding P‾xyz=Pxyz-pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo linebreak="badbreak" is="true">-</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>. Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> is learned from P‾xyz<math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>, rather than from Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>, with:(5)Wdep=MLP(W,[P‾xyz,P‾xyz2]),<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mi mathvariant="italic" is="true">MLP</mi><mo stretchy="true" is="true">(</mo><mi is="true">W</mi><mo is="true">,</mo><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo stretchy="true" is="true">]</mo><mo stretchy="true" is="true">)</mo><mo is="true">,</mo></mrow></math>where P‾xyz2<math><mrow is="true"><msubsup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></mrow></math> is the element-wise square of P‾xyz,MLP(·)<math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo is="true">,</mo><mi mathvariant="italic" is="true">MLP</mi><mo stretchy="true" is="true">(</mo><mo is="true">·</mo><mo stretchy="true" is="true">)</mo></mrow></math> is a multi-layer perceptron [37] and W denotes the learnable weights. Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> is activated with the sigmoid function, and a bias of 0.5 is added to shift the mean to 1. We empirically found that concatenating P‾xyz2<math><mrow is="true"><msubsup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></mrow></math> to the input for learning Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> improves the performance. We suspect the reason is that P‾xyz2<math><mrow is="true"><msubsup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></mrow></math> are important elements for representing geometry entities like L2<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">L</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msub></mrow></math> distances, and feeding them into the MLP makes it more aware of higher order geometry structures for producing more effective weights.
Note that both Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> (hence Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math>) and PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> are derived from the depth channel, but they are used in very different and complementary ways in our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D. PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> is more of a representation for the semantic scene layout, especially the horizontal above ground component, while Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> focuses more on the local geometry for addressing challenging segmentation details. The algorithm of the overall RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D operation is illustrated as follows:Algorithm 1: Overall RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D operationInput: The RGB, Depth and HHA images, i.e., IRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math>, ID<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math> and IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math>Output: The output feature map f<math><mrow is="true"><mi is="true">f</mi></mrow></math>1. Extract patches [PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math>,PD<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math>,PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math>] from [IRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math>,ID<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math>,IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math>].2. Calculate Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> with Eq. (1).3. Transfer to local coordinate P‾xyz=Pxyz-pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo linebreak="badbreak" is="true">-</mo><msub is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>.4. Concatenate P‾xyz<math><mrow is="true"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> and P‾xyz2<math><mrow is="true"><msubsup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></mrow></math>.5. Learn the depth-weight Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> with MLP(W,[P‾xyz,P‾xyz2])<math><mrow is="true"><mi mathvariant="italic" is="true">MLP</mi><mo stretchy="true" is="true">(</mo><mi is="true">W</mi><mo is="true">,</mo><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub><mo is="true">,</mo><msubsup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mo stretchy="true" is="true">‾</mo></mrow></mover></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><mo stretchy="true" is="true">]</mo><mo stretchy="true" is="true">)</mo></mrow></math>6. Shift the mean of Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> to 1 by Sigmoid(Wdep)+0.5<math><mrow is="true"><mi mathvariant="italic" is="true">Sigmoid</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub><mo stretchy="true" is="true">)</mo><mo linebreak="badbreak" is="true">+</mo><mn is="true">0.5</mn></mrow></math>.7. Weight PRGB<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub></mrow></math> with Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> by PRGBW<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub></mrow></math> withPRGB⊗Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo is="true">⊗</mo><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math>.8. Concatenate PRGBW<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub></mrow></math> and PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math>.9. Learn the output feature map f<math><mrow is="true"><mi is="true">f</mi></mrow></math> with Conv<math><mrow is="true"><mi mathvariant="italic" is="true">Conv</mi></mrow></math> (K,<math><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">K</mi><mo is="true">,</mo></mrow></math> [PRGBW,<math><mrow is="true"><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub><mo is="true">,</mo></mrow></math> PHHA])<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub><mo stretchy="true" is="true">]</mo><mo stretchy="true" is="true">)</mo></mrow></math>.
The proposed RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is a simple and lightweight module for attributing RGB patches with depth information. The depth-weighted RGB patches are replacement for the RGB patches in vanilla convolution. Thus, theoretically, RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D layer can be easily plugged in any CNNs as a replacement of the vanilla convolution that consumes RGB images.
We opt to plug and test our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D into a network of the style shown in Fig. 2b1), so as to demonstrate the effectiveness of the proposed module in the case with the fewest clutter factors. This ends up with the style shown in Fig. 2b2). We build RGB-D segmentation networks of this style using U-Net [36] and DeepLab series [3], [4], [5], [6] architectures.
Datasets. In order to validate the effectiveness of our proposed method, we conducted extensive experiments on two benchmark RGB-D indoor datasets: NYU-Dv2 [39], SUN-RGBD [40] and Cityscapes [11] datasets. NYU-Dv2 contains 1,449 RGB-D scene images, where 795 images are split for training and 654 images for testing. We adopted the 40-class setting provided by [16], where all pixels are labeled with 40 classes. SUN-RGBD is composed of 10,355 RGB-D indoor images with 37 categories for each pixel label. We followed the widely used setting in [40] to split the dataset into a training set of 5285 images and a testing set of 5050 images. The Cityscapes dataset consists of 5,000 street scene images which is fine-annotated with pixel-level labels of 19 semantic classes, each image is of high resolution (2048 ×<math><mrow is="true"><mo is="true">×</mo></mrow></math> 1024), making the training more challenging due to limited GPU memory. Cityscapes contains approximately 30×<math><mrow is="true"><mo is="true">×</mo></mrow></math> and 4×<math><mrow is="true"><mo is="true">×</mo></mrow></math> of the image pixels of NYU-Dv2 and SUN-RGBD, respectively.
Implementation Details. To increase the diversity of the training images, we augmented the training data with 4 common strategies: left–right flipping, cropping, scaling for all channels and color jitter for RGB images. The resolution of the input images (including IRGB,IHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> and Ixyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">I</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>) is 425 ×<math><mrow is="true"><mo is="true">×</mo></mrow></math> 560. In testing phrase, we adopted both single-scale and multi-scale testing. For the latter one, left–right flipped images and five scales are exploited: 0.5, 0.75, 1.0, 1.25, 1.5.
Metrics. Suppose there are K+1<math><mrow is="true"><mi is="true">K</mi><mo linebreak="badbreak" is="true">+</mo><mn is="true">1</mn></mrow></math> classes in total3, and Nij<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></math> represents the number of pixels that belong to class i and are predicted to be class j in the testing set. Note that i and j can be identical. We evaluated the performance of the proposed method with three most popular metrics for semantic segmentation:
PixelAcc=∑i=1KNii∑i=1K∑j=1KNij<math><mrow is="true"><mi mathvariant="italic" is="true">PixelAcc</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ii</mi></mrow></msub></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></mfrac></mrow></math>;
MeanAcc=1K∑i=1KNii∑j=1KNij<math><mrow is="true"><mi mathvariant="italic" is="true">MeanAcc</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></mfrac><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><mfrac is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ii</mi></mrow></msub></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub></mrow></mfrac></mrow></math>;
MeanIoU=1K∑i=1KNii∑j=1KNij+∑j=1KNji-Nii<math><mrow is="true"><mi mathvariant="italic" is="true">MeanIoU</mi><mo linebreak="goodbreak" linebreakstyle="after" is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></mfrac><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><mfrac is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ii</mi></mrow></msub></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ij</mi></mrow></msub><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">K</mi></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ji</mi></mrow></msub><mo is="true">-</mo><msub is="true"><mrow is="true"><mi is="true">N</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">ii</mi></mrow></msub></mrow></mfrac></mrow></math>.
Except for the performance related metrics, we also the Param.) and (MACC.) into consideration. Param.(M) denotes the number of network parameter (in how many millions), which is related to the memory usage of overall network. MACC.(B) denotes the Multiply-and-ACCumulate which is responsible for the computational cost analysis (in how many billions).
Comparison protocol. For the main results of this section, we implemented our method using Tensorflow [1]. Pre-trained DeepLab V3+ [6] with Xception [10] is used as the backbone model. The first convolution layer is adapted to take 6 channels — weighted RGB and HHA channels, and the kernel parameters of this layer are randomly initialized with Xavier initialization [15]. We adopt the same network that consumes the original (un-weighted) RGB and HHA channels as our baseline. Note that the performance of a CNN based network can be affected by a wide range of factors. To demonstrate the effectiveness of RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D, we merely plugged RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D into the baseline methods, without any change to other settings. In other words, The learning of Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> and using it to multiply the RGB patches are the only differences between the baseline and our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D approach. This guarantees that the observed performance change is due to the application of RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D, but not other factors. Furthermore, this also means that no hyper-parameter is tuned to favor RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D. Following this protocol, we demonstrate the effectiveness of RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D on several benchmark datasets and popular RGB semantic segmentation architectures.
Training. The parameters inherited from the baseline are fine-tuned, and the parameters of the MLP in Eq. 5 are randomly initialized. The SGD optimizer is used, the batch size is set to be 4, and the learning rate is fixed to be 1.5e-3 and 2.5e-3 for NYU-Dv2 and SUN-RGBD, respectively.
Experiments on NYU-Dv2 Dataset. We show the results of our method and several recently developed methods with single-scale and multi-scale testing in Table 1 and Table 2, respectively. Note that the Deeplab V3 + baseline we chose to work on is a very strong approach. From Table 1, we can observe that for single-scale testing, our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D approach adds only a negligible amount of parameters (+0.001 M) and computation (+0.07B MACC) to the baseline, while bringing a notable improvement (+0.9 Pixel Acc., Mean Acc., and Mean IoU) over it. In Table 2, we compared with other methods which are delicately designed for RGB-D segmentation with multi-scale testing. For example, for RDFNet [31], the improvement of RDFnet-152 (+0.4 Pixel Acc., +0.6 Mean Acc., +1.0 Mean IoU.) is brought by 32 M additional parameters than RDFNet-101, while our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D approach has made a large improvement (+0.8 Pixel Acc., +1.4 Mean Acc., +1.2 Mean Iou.) with only a small amount of parameters (+0.001 M) instead. In addition, we also show the IoU results on each category in Table 3. From this table, we can observe that our method observe other methods in most categories.
Fig. 6 shows the qualitative comparisons on the NYU-Dv2 test set. As shown in this figure, the depth information can be well utilized to extract the features of the objects, especially the detailed boundaries of the objects, by utilizing the RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D. For example, in (a), the color of the pillow is very similar to that of the sofa, especially when the lighting condition is poor. Similar case is observed in (b), where the table legs are in shadow and barely visible from the RGB image. It is difficult to determine the correct labels of the pixels in these cases, even if the HHA channels are concatenated and used in an additive way with RGB channels. Thin structures like the horizontal bar of the chair in (c) are challenging for segmentation in general. There is a tendency for them to be “smoothed” by the neighboring regions and classified to share the same label as them. In our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D approach, the weighting learned from the depth information can effectively attribute the RGB patches, making them more geometry-aware, thus resolving the ambiguities in these difficult cases. The gradually varied colors at the box on the table in (d) and the cabinet in (e) pose challenges in making accurate segmentation boundaries. The weights learned from the depth help the network respect the discontinuities and make the accurate cut following the geometric hints.
To better understand how our approach improvements over the baseline, we show the per-class IoU comparisons in Fig. 8. As shown in the figure, our method is superior to the baseline in majority of the object categories. Representative results also show that the performance of RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D may be even lower than the baseline. We found that RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D does not perform well for the following reasons: the depth information is not reliable, such as the mirror; the depth information cannot be distinguished, such as the floor mat; or the training data has too few samples, and the similar categories cannot be distinguished. For example, curtains and shower curtains belong to such similar categories.
SUN-RGBD dataset. The results and comparisons on this larger scale dataset is shown in Table 4. Here, again, the baseline model we chose to work on is competitive to ResNet-101-3DGNN [34] and RDF-152 [31], both of which are equipped with backbones having more than 100 layers. The number of parameters of our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is only 19% of the counterparts in RDF-152. In most cases, the RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is shown to bring notable improvement over the strong baseline. In addition, we also added the IoU results on each category on SUN-RGBD in Table 5. From this table, we can observe that our method is superior than the baseline in most categories.
Cityscapes dataset. We have tested the effectiveness of the proposed method on a large dataset – Cityscapes [11], and reported the results in Table 6. From Table 6, we can observe that our method can outperform the baseline one, demonstrating that our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is capable of dealing with large datasets.
To understand the consequence of applying the learned weights on the RGB patches, we used t-SNE to visualize the patches before and after the weighting in Fig. 5. In the situation of Fig. 5(a), the green and yellow points express the feature of pixels of the sofa and pillow class, respectively. Since their color appearance in the original image is quite similar, the segmentation of these two classes is rather difficult. This leads to many overlapping points before using our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D in (a). As show in Fig. 5(b), the learned correlation weights are used to multiply the pixels features in the patch, making the image patches geometry-aware and hence more discriminative to each other (these two sets of points becomes more separable). The network learns the weights to “bloom” the patches in the feature space and facilitate the segmentation task.
To further understand the weights that our method learns, we visualize the variance of the weights learned in each of the examples in Fig. 7. Note that the weights Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> for each center pixel is a 3×3<math><mrow is="true"><mn is="true">3</mn><mo is="true">×</mo><mn is="true">3</mn></mrow></math> tensor, so the variance of it tells how differently the 3×3<math><mrow is="true"><mn is="true">3</mn><mo is="true">×</mo><mn is="true">3</mn></mrow></math> pixels in the corresponding patch are weighted. We can find that the variance of the weights is relatively large in places where the depth changes greatly (in most cases, the geometric edges of the objects). It has a similar look as the edges from the depth map. This means that the pixels of RGB patches in these areas are weighted more differently than those in “flat” areas, indicating that our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D can make effective use of depth information.
Different Baselines. Our proposed RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is a general module for boosting RGB-D semantic segmentation. To prove the generalization capability of our method, we have applied our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D over several representative semantic segmentation baselines, i.e., DeepLab V3 plus [6] + Xception [10], DeepLab V3 [5] + ResNet [18] and U-Net [36] + MobileNetV2 [38], and reported the performance in Table 7. We can observe that our method brings performance improvement in all baselines, demonstrating that our method is endowed with a strong generalization ability.
The DeepLab V3 baseline is adopted in the same way as that of DeepLab V3 plus. However, to fit the network into our GPUs, we changed the kernel size of the first layer from 7×7<math><mrow is="true"><mn is="true">7</mn><mo is="true">×</mo><mn is="true">7</mn></mrow></math> to 3×3<math><mrow is="true"><mn is="true">3</mn><mo is="true">×</mo><mn is="true">3</mn></mrow></math>. Note that similar improvement is observed on the DeepLab V3 baseline as that on DeepLab V3 + baseline.
We have also adopted U-Net with MobileNetV2 backbone as one of the baselines, because both the segmentation frameworks and backbones are quite different: U-Net and DeepLab series are quite different, and MobileNetV2 and Res-Net are quite different. Note that MobileNetV2 is a lightweight architecture designed for mobile devices. In this setting, we only use pre-trained parameters in the convolution part and randomly initialized the parameters in the entire deconvolution part with the Xavier initialization. For this baseline, more than half of the parameters are randomly initialized, which makes it challenging to achieve high performance on this relatively small NYU-Dv2 dataset. In this stress test, our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D still brings performance improvement, which shows that our method has a strong generalization ability.
Performance with noisy geometric information. We have conducted an ablation experiment with noisy geometric information introduced. To this end, we added the Gaussian noise to the geometric channel, i.e., the HHA and the 3D coordinate values, and showed the results in Table 8. We can observe that the performance of both the baseline and our method is degraded when the noise is introduced, but the proposed RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D still outperforms the baseline method, indicating that our method is more stable with the tolerance to random noise.
Performance without HHA Channels. We conducted experiments to verify the effectiveness of our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D equipped with the raw depth channel rather than the HHA one. Under this setting, we use the PD<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">D</mi></mrow></msub></mrow></math> instead of PHHA<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub></mrow></math> to represent the PGeo<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">Geo</mi></mrow></msub></mrow></math>,. From Table 9, we can observe that our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D consistently outperforms both the baseline (i.e., Deeplabv3+) and other prior methods with a large margin on all the three metrics. This validates that our RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is endowed with a strong generalization capacity, which enabling it to perform well even without the HHA channels. Besides, we can also discover that the HHA channel does have advantage over the raw depth one. For example, the Mean IoU. of baseline method drops from 47.8% to 40.9% when replacing the HHA with depth channel.
Size of Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math>. In the DeepLab V3 [5] baseline, the first convolution layer uses kernel size 3×3<math><mrow is="true"><mn is="true">3</mn><mo is="true">×</mo><mn is="true">3</mn></mrow></math>, and the inputs to it are concatenated 3×3<math><mrow is="true"><mn is="true">3</mn><mo is="true">×</mo><mn is="true">3</mn></mrow></math> patches of weighted RGB and HHA, i.e., [PRGBW,PHHA]<math><mrow is="true"><mo stretchy="true" is="true">[</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">HHA</mi></mrow></msub><mo stretchy="true" is="true">]</mo></mrow></math>. Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> is supposed to be of the same size as PRGBW<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">RGB</mi></mrow><mrow is="true"><mi is="true">W</mi></mrow></msub></mrow></msub></mrow></math>. However, Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> used for computing Wdep<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">dep</mi></mrow></msub></mrow></math> is not necessarily of the same size. We tested different sizes of Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> and the experimental results are illustrated in Table 10. For the NYU-Dv2 dataset, We can observe that larger size (i.e., 5×5<math><mrow is="true"><mn is="true">5</mn><mo is="true">×</mo><mn is="true">5</mn></mrow></math>) works worse than the smaller one for the NYU-Dv2 dataset. One reason behind this is that large size of Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> leads to over-fitting. On the contrary, the model with smaller size of Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> outperforms the one with larger size on SUN-RGBD dataset. We suspect the reason is SUN-RGBD is of lower quality than NYU-Dv2, and using a larger size for Pxyz<math><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">xyz</mi></mrow></msub></mrow></math> results in better tolerance for the noisy data.
Learned vs. Hand-crafted Weights. We compare the effectiveness of the learned weights against the hand-crafted weights as proposed in Eq. (3) of [47], and summarize the results in Table 11. Interestingly, there is a performance drop due to the use of hand-crafted weights. We suspect the reason is that Eq. (3) of [47] computes the weight for each pixel only from the depth difference between the pixel and the center pixel, i.e., it only accounts for pair-wise pixel correlations, which is not as stable as taking entire patches into consideration, especially in the presence of noisy depth.
We proposed RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D for attributing RGB patches with weights learnt from depth patches, which enables the adoption of RGB segmentation networks for effective RGB-D segmentation. The proposed method learns pixel correlation weights within each image patch from its corresponding 3D geometry, and then applies convolution on correlation-weighted patches. Extensive experiments and ablation studies on RGB-D semantic segmentation benchmarks show that the proposed RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D can consistently boost the performance over various baselines, with negligible addition of parameters and computation.
RGB×<math><mrow is="true"><mo is="true">×</mo></mrow></math>D is a simple yet effective approach for fusing geometry information into RGB patches. Furthermore, it boils down to a versatile module that may be easily added to any existing RGB segmentation network. Thus, with this approach, future advances in RGB segmentation architectures can be easily transferred to processing of RGB-D data, greatly reducing the effort that would otherwise be spent on designing dedicated networks for RGB-D segmentation.
Jinming Cao: Conceptualization, Methodology, Writing - original draft. Hanchao Leng: Software, Data curation, Writing - original draft. Daniel Cohen-Or: Writing - review & editing, Validation. Dani Lischinski: Writing - review & editing, Formal analysis. Ying Chen: Writing - review & editing, Supervision. Changhe Tu: Supervision, Resources, Project administration. Yangyan Li: Investigation, Writing - review & editing, Supervision.
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
This work is supported by the National Science Foundation of China General Program grant No.61772317, 61772318 and 62072284, “Qilu” Young Talent Program of Shandong University, and the Research Intern Program of Alibaba Group.