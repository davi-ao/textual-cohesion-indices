Artificial intelligence (AI) has become a hot topic in recent years, which is expected to usher in the fourth digital revolution for mankind. Signaling tremendous power for societal change, AI has aroused interests and concerns from various sectors of the society, including scientists, businesses, politicians, media, and the general public. Discussions and debates on AI have been around its applications, regulations, consequences, among others. “The rise of powerful AI will either be the best or the worst thing ever to happen to humanity. We do not yet know which,” commented Professor Stephen Hawking (Saffell, 2016).
Despite the mounting interest, public understanding of AI and its impact is in an early stage (Zhang and Dafoe, 2019). For most people, media serve as a pivotal source of information on AI. As such, the way media cover AI may greatly influence public perceptions of and responses to the technology. In the past, research has demonstrated that media portrayals affect the vast majority of the public to form opinions or judgments about an emerging technology. For instance, Scheufele and Lewenstein (2005) showed that cognitive heuristics provided by media significantly influenced public views of nanotechnology in terms of related risks, benefits, and funding. To our knowledge, there is little research on the role of media in communicating information about AI to the public. A lack of research in this regard may compromise the understanding of this emerging technology and its potential, complex, and enormous impact on the future.
This study attempts to analyze 40 years’ news coverage on artificial intelligence in four mainstream English newspapers and to investigate how media report AI technology in general. We first review literature related to emerging technologies and media coverage thereof, and then propose four main research questions. Next, we describe our sampled data and mixed-methods analysis of 1776 pieces of news. Third, we present major research findings from both computer-assisted text mining and manual coding. Finally, implications of major findings are discussed in reference to the historical, cultural, and societal contexts of emerging technologies.
Emerging technologies have become a concept of its own, denoting a wide range of creations or inventions in human history. However, past research clearly lacks a rigorous definition of emerging technologies which are often equated with a host of other related terms (e.g., new media, digital media, communication technologies). Borah (2017) reviewed previous literature, published from 1998 to 2013, on emerging communication technologies, and noted the following problems: a lack of theoretical underpinnings, vagueness of defining key concepts/terms, and excessive use of nonprobability samples. Instead of using the term-emerging communication technologies, Borah classified a variety of platforms or devices (e.g., the Internet in general, video games, mobile phones) as emerging technologies.
In response to the conceptual confusion, Rotolo et al. (2015) argued to define emerging technologies (e.g., nanotechnology, synthetic biology) by incorporating five major technological attributes: radical novelty, relatively fast rate of growth, coherence of identity, prominent impact, and uncertainty/ambiguity. The authors contended that emerging technologies are not a static property but rather entail a process of evolvement. More concretely, emerging technologies usually undergo different stages of development, and may not be considered emergent anymore after certain stages. Such an argument echoes the idea of “issue-attention cycle” connoting how a scientific issue emerges in media coverage, arouses public concern, and gradually fades away in prominence (Cacciatore et al., 2012). More recently, Brey (2017) described emerging technologies, as opposed to entrenched technologies, as any system that is new, innovative, still in development, and promising to have a large impact on the society. Taken together, it is reasonable to argue that AI is an emerging and enabling technology (cf., stand-alone technologies), as its applications are beginning to diffuse to various sectors such as health care, human communication, finance, retail, automobile, etc..
Past decades have seen media coverage of a variety of technologies during their emerging stage. Arceneaux and Weiss (2010), for instance, examined the American press coverage of the social interactive platform-Twitter-during its emerging period of time, from 2006 to 2009. The study found that positive themes about Twitter dominated the early stage of coverage, emphasizing two major benefits-brevity of messages and speed of communication-of the platform. In contrast, few articles focused on purely negative themes such as information overload and unanticipated consequences. Compared with Twitter, stem cell research received considerably less U.S. media attention in its nascent phase. In fact, prior to 1998, press coverage of stem cells was less than that of most other technology–related topics, such as climate change and nuclear energy (Nisbet et al., 2003).
Media coverage of emerging technologies could be cyclical, event-driven, and orienting to their rich, social, and political implications. Comparing coverage of nanotechnologies between American print newspapers and online media, Cacciatore et al. (2012) found that while newspaper coverage began to decline, Google news coverage was growing. Furthermore, discourses of online news are more diverse than that in traditional newspapers. By analyzing newspapers in Germany, Switzerland, and Austria, Metag and Marcinkowski (2014) observed that media tend to cover nanotechnology in a predominantly positive and affirmative tone. Journalists advocated the scientific, medical, and economic benefits brought by nanotechnology, but only tangentially mentioned the risks associated with the technology. When it comes to biotechnology, there is scant media coverage of the controversies about it but more coverage of its benefits than potential risks. Nisbet and Lewenstein (2002) examined American media and reported that the coverage of biotechnology was overwhelmingly positive, with emphases on its scientific progress and economic prospect. In contrast, topics related to ethics and public accountability were less frequently discussed in the media. Nisbet et al. (2003) contended that media attention is more likely to be directed to issues that can be narratized and dramatized. As such, emerging technologies such as AI are likely to be in the spotlight of media because of their rich social implications.
Media coverage on emerging technologies may exemplify different patterns across time. For instance, Arceneaux and Weiss (2010) summarized three types of media narratives of emerging communication technologies: positive becoming negative, negative becoming positive, and positive continually. Media coverage of a scientific issue could last longer when journalists are able to position it within a larger narrative and identify suitable “recycled thematic formats and storytelling conventions” (Nisbet et al., 2003, p.65). Furthermore, with different stakeholders’ voices (e.g., researchers, governments, and companies) joining in the media, discourses on emerging technologies like AI could grow more diverse, dynamic, and sophisticated.
In 1955, the Dartmouth research team defined AI as a set of scientific technologies and abilities involved in making a machine behave in ways as intelligent as a human. With the potential impacts of AI being so manifold, massive, and unpredictable, media have come to introduce, contextualize, and position the technology through various angles or media frames (Werner & Cornelissen, 2014). The power of media frames about technologies has been well documented in past research. For instance, Scheufele and Lewenstein (2005) found that media frames of nanotechnology had a stronger impact on public opinion than had mere factual information.
Past research has shown that a variety of linguistic tools-such as terminological, argumentative, and emotive components-can contribute to media framing of emerging technologies. Studying media frames about climate change, Koteyko et al. (2010) scrutinized compounds premised on the root word of “carbon,” and identified three clusters of compounds conveying such themes as finance (e.g., carbon finance), lifestyle (e.g., carbon footprint), and attitudes (e.g., carbon morality). The authors called further attention to the roles of lexical combinations, terminological tool connoting various perspectives held by different stakeholders.
Beck and Vowe (1995) examined media views on the then-emerging technology-multimedia-and identified argumentation patterns such as economically optimistic outlook and political criticism. Rössler (2001) analyzed how German news magazines framed “the Internet” during the early diffusion state of the medium. The study revealed a strong media bias toward positive or euphoric tones when covering the Internet. In contrast, apocalyptic views only accounted for about 5% of the total coverage. When discussing the impacts of the Internet, a stronger emphasis was put on its role in emancipating individuals, as opposed to its functions (e.g., democratization and polarization) at the societal level. The author further pointed out that a technology can be framed by media through presentation (e.g., salience of placement), subtopics/themes, cognitive attributes (e.g., argumentation pattern), and affective attributes (e.g., tones about the impacts of a technology). The argumentation patterns summarized by Beck and Vowe, 1995, Rössler, 2001 provide a discussion basis for examining media framing of AI.
In this study, we will examine the roles of both lexical combinations and argumentation patterns in media coverage of AI. Lexical compounds can provide a particular angle on wedge issues like emerging technologies by functioning as vivid metaphors and imageries. Recent research (e.g., Carbonell et al., 2016) has looked into metaphors characterizing AI (e.g., “computational systems are brains.”), but seldom touched on the role of lexical compounds. As a framing device, lexical compounds enable both conceptual framing and lexical cohesion. On the other hand, the argumentation patterns summarized by Beck and Vowe, 1995, Rössler, 2001 may provide insight into the positions and relations of different actors or stakeholders pertaining to an issue such as climate change or AI.
Traditional media have long been criticized for prioritizing certain stakeholders over others on reporting an issue like emerging technologies (Brennen et al., 2018). Metag and Marcinkowski (2014) observed that the most mentioned actors in newspaper coverage of nanotechnology are scientists, economic actors, and journalists. The authors found that, in 76% of the studied articles, only one actor was mentioned, predominantly being scientists. Furthermore, journalists are slightly less positive in evaluating nanotechnology than are scientists and economic actors. As AI diffuses into a myriad of walks of human society, it is worthwhile to investigate how different stakeholders (e.g., scientists, politicians, business leaders) speak out on media, and how these stakeholders interact with each other (Makridakis, 2017). As Cath et al. (2018) commented, “good AI society” entails the collective efforts of key stakeholders including the governments, corporate sectors, citizens, and the research community. Exploring the presence of stakeholders in the media would provide insight into media’s framing of and positioning on AI technologies in particular, and enhance the understanding of media reporting on emerging technologies in general.
In light of the above literature, the following research questions are proposed:
RQ1: what is the general amount and pattern of media coverage on AI?
RQ2: what are the major topics about AI presented in the media?
RQ3: what are the main lexical compounds and argumentation patterns on AI presented in the media?
RQ4: what are the main actors, and the network thereof, pertaining to AI in the media?
We searched for articles on AI through the LexisNexis database, with a particular focus on the following news outlets: the New York Times, Washington Post, the Guardian, and USA Today. Selection of the newspapers was based on the following considerations: First of all, we consulted a list of English-language newspapers included in the online traffic ranking developed by Comscore, an U.S.-based media analytics company. With the diffusion of new technologies, more people have turned to online websites, including online newspapers, for news. As such, newspapers attracting a large online viewership is more likely to be influential. We also checked other ranking systems such as the eBizMBA Ranking (see http://www.ebizmba.com/articles/news-websites) to cross-validate the popularity of the newspapers screened for analysis. Second, for each of the top 15 popular news outlets, we counted AI-related articles and excluded from analysis the newspapers with less than 30 relevant articles or a majority related articles being short briefs. For instance, the People’s Daily online, based in China, has a large online traffic but very few of its articles focused on AI. Excluding newspapers with few AI-related articles would make the coverage comparable across different news outlets retained for study. Third, considering U.S. is a leading country in AI development, we reasoned that newspapers published in the States are more likely to cover this topic. Such a reasoning was supported by the initial search results from the database, and hence more U.S.-based newspapers were retained for analysis. Fourth, circulation and popularity of the media aside, we also consulted the list of newspapers that prior studies focused on to study the coverage of other emerging technologies (e.g., Nisbet et al., 2003). By taking into account the aforementioned factors including business rankings, the amount of AI-related articles, and academic studies, we narrowed down to a list of newspapers that suits the purpose of the present study. We employed a host of AI-related keywords (e.g., artificial intelligence, AI, machine learning, robot) to search in the LexisNexis database and downloaded full articles of all search results. A complete list of the keywords is available upon request from the authors.
A total of 11,956 news articles were obtained through keyword search. We then screened the articles by removing the following: web log, book review, obituary, letter, correction, biography, and other genres not classified as news. After the first round of screening, 9927 news articles were retained spanning from June 16th of 1977 to January 31th of 2019.
After reading 100 sampled pieces of news, we found that many of those articles only mentioned AI in passing, without presenting any substantial discussion of the topic. To focus on the news substantially relevant to AI, we applied the following rule: a keyword from our compiled list (e.g., AI, artificial intelligence, machine learning, robot) must appear at least once in the title or the lead paragraph, and at least twice in the full text. This approach has proven to be effective in past studies (Burscher, Vliegenthart, & de Vreese, 2016). After the second-round of screening, we retained a total of 1776 news articles (see Table 1).
Using the Python programming language, we applied automatic content analysis, the method proposed by Burscher et al. (2016), to extract major topics about AI presented in the media. First, we lemmatized all words of the full text and removed common English stopwords. During the process of word lemmatization, we carried out part-of-speech tagging and only retained nouns, adjectives or adverbs in the articles. We also used Stanford natural language toolkit and applied named-entity recognition to process all names of persons, organizations, and countries as well as dates (Manning et al., 2014).
Next, we used the TF*IDF method to turn each news article into a vector. The TF*IDF method calculates the importance of a word in the entire corpus based on the frequency of the word in the text and the frequency of the word in the corpus. TF (Term Frequency) indicates how often a keyword occurs in one article. IDF (Inverse Document Frequency) is the reciprocal of the number of times a keyword appears in all articles in the entire corpus. TF*IDF filters out common, irrelevant words while preserving important ones that affect the entire text.
Afterward, we used the k-means clustering to explore the topics covered in these articles. The idea of the k-means is very straightforward. For a given sample set, the algorithm divides it into k clusters to minimize the distance among samples in each cluster and maximize the distance among the clusters. Assuming the cluster is divided into (C1<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mn is="true">1</mn></msub></mrow></math>, C2<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mn is="true">2</mn></msub></mrow></math>, … Ck<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mi is="true">k</mi></msub></mrow></math>), the goal is to minimize the squared errorE<math><mrow is="true"><mi mathvariant="normal" is="true">E</mi></mrow></math>:E=∑i=1k∑x∈Ci‖x-μi‖2<math><mrow is="true"><mi mathvariant="normal" is="true">E</mi><mo linebreak="goodbreak" is="true">=</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">k</mi></munderover><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">x</mi><mo is="true">∈</mo><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub></mrow></munder><msup is="true"><mrow is="true"><mrow is="true"><mo stretchy="false" is="true">‖</mo><mi is="true">x</mi><mo is="true">-</mo></mrow><msub is="true"><mi is="true">μ</mi><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="false" is="true">‖</mo></mrow></mrow><mn is="true">2</mn></msup></mrow></math>where x<math><mrow is="true"><mi is="true">x</mi></mrow></math>is one document in Ci<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub></mrow></math>and μi<math><mrow is="true"><msub is="true"><mi is="true">μ</mi><mi is="true">i</mi></msub></mrow></math>is the mean vector of the cluster Ci<math><mrow is="true"><msub is="true"><mi is="true">C</mi><mi is="true">i</mi></msub></mrow></math>, sometimes called the centroid. One method to validate the number of clusters is the elbow method. The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k, and, for each value of k, to calculate the error sum of squares (SSE), which is the sum of the squared differences between each observation and its group's mean. Then, we plotted the SSE values to help identify the optimum number of clusters. We conducted the analysis through Python programming, using main libraries including sklearn, StanfordCoreNLP, nltk, spacy, numpy and genism.
To tease out the nature of media framing of AI, we turned to manual coding to identify lexical compounds and argumentation patterns in the articles (Carbonell et al., 2016, Rössler, 2001). Initially, we read 200 articles to identify meaningful descriptions and create a list of lexical compounds. Our next step was to continue to locate compounds until the list reached saturation. To safeguard the completeness of the compiled list, we also supplemented additional candidate compounds by examining the list of high-frequency words-particularly nouns and adjectives-provided by the automatic content analysis. After all the authors reached consensus, the final list of lexical compounds was generated and applied to tally the frequency of each compound in the retained sample of 500 articles.
As for argumentation patterns, we started with the typology proposed by Rössler (2001). Two of the lead authors discussed the definition of each argumentation pattern (e.g., apocalyptic, relativizing, euphoric) together, and then independently read the same sample of articles line-by-line to identify “thought units” matching the patterns. A thought unit could be short or long, depending on whether it conveyed complete meaning of a thought. For example, the following words were coded as one unit, “The technological advancements disrupting established business sectors are now shaking up the world of war — where robots, swarming drones and weapons enhanced by artificial intelligence might one-day rule the skies and seas.” (Washington Post, June 17, 2016). Although this sentence was long, it conveyed one essential thought (i.e., apocalyptic view) about the disruptive consequences of AI. After comparing their independent coding results, the two authors revised the definition and operationalization of each argumentation pattern until no further information can be added to refining the typology. Before formal coding, another 150 articles were selected for training and intercoder reliability testing purposes. With the finalized coding scheme, the intercoder reliability Krippendorff’s alpha on different categories ranged from 0.79 to 0.95. Any discrepancy of coding was discussed and resolved between the coders. Recent research employing Rössler’s typology to study new media lent further support to our way of refining and identifying argumentation patterns. For instance, Chia (2019) adapted the typology to examine the patterns of argumentation on cyber vigilantism. Chia operationalized argumentation patterns (e.g., apocalyptic and relativizing) by distinguishing and interpreting thought units in media articles.
To address the relationships between stakeholders, we examined two networks pertaining to nations and organizations, respectively. First, we extracted named entities from the news articles through spaCy, a python-based NLP toolkit. Synonymous names (e.g., “USA” and “United States”) are merged as a common name such as America. Next, with each country or organization being a node and the number of times that two entities appearing in the same news article being the weight of the link, we constructed networks across all the media. The networks were created with the Gephi network visualization software (Bastian et al., 2009). For the sake of clarity, we only presented the entities ranked high according to the number of occurrences in the figures. We classified the organizations into three categories: governmental organizations, business sectors, and research communities, with each type of nodes being colored differently.
To explore the potential associations between topics, argumentations, actors, and newspapers, we used multiple correspondence analysis (MCA) to visualize the underlying structures of our dataset. MCA (Abdi & Valentin, 2007) is used to analyze a set of observations described by a set of categorical dependent variables. It expands categorical variables into binary variables and then creates a low-dimensional representation of the associations between them. These relationships have no a priori expectations especially on specific areas of the multidimensional space. In addition, the method provides a graphical representation of the relationships in 2D plots, rendering interpretation of the results more intuitive and straightforward. The closer two categories are on the map, the stronger association between the categories can be inferred. We used the two-dimensional MCA plot to demonstrate the relationships between the topics, argumentations, actors, and newspapers. The analysis was implemented through the {FactoMineR} package of R(http://factominer.free.fr/).
To answer question 1, we plotted the distribution of articles across the media outlets and the years. Fig. 1 showed that media coverage spiked drastically in recent five years. Among the media, New York Times covered AI most intensely, followed by the Guardian and the Washington Post. Media coverage of AI has spanned several decades, from 1970s to the present, and the coverage during the early two decades was very sparse. In 2001, there was a sudden spike in the intensity of the coverage. A close look of the data revealed that the increase of the coverage in that year mainly came from the Guardian, with a large share of news articles covering movie and game reviews.
As for RQ 2, the k-means cluster plot showed that the inflection point or the elbow is around the location where k = 14. After comparing the solution of k = 14 with those of adjacent k points (e.g., k = 12, 13, 15, 16), we found that the topics derived from the 14-cluster solution are more interpretable and meaningful.
Presented in Table 2a, Table 2b are the 14 topics retained for final interpretation. For each topic, we followed the procedure proposed by Dudo et al. (2011) and presented a list of root words for description. Specifically, three coders individually referenced the solution of latent topic modeling and generated lists of possible root words. Then, the coders worked together to discuss their lists and reached consensus on the most representative words for each theme. To sum up, we provided up to 10 keywords depicting the meaning of each theme. Chi-square test showed that there was a statistically significant difference in the distribution of the topics (x2<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">x</mi></mrow><mn is="true">2</mn></msup></mrow></math>(13) = 1566.16, p < .001). Further tests showed that the three topics (robot, brain science, and regulation) contributed the most to the overall variance of distribution. Put differently, these three topics appeared most frequently in the news articles. Results also showed that New York Times and the Guardian covered a wider range of topics than did other newspapers.
To answer RQ 3, we identified more than one hundred different lexical compounds associated with AI. After a close reading of the context in which these compounds were embedded, the coders discussed and compiled a list of key compounds, along with the frequencies of each compound in the corpus (see Table 3). As Koteyko et al. (2010) contended, using creative compounds is a powerful way for media frames to manifest themselves. Some compounds-such as superintelligence, AI superiority, AI revolution, and strong AI-point to the dominance or superior power of AI. According to the Washington Post (Emba, 2015), “Computing optimists estimates say a true strong AI will emerge between 2029 and 2040, with almost all AI researchers predicting that the first machine with human-level intelligence will arrive at least before the century's end.” In our findings, compounds-such as evil AI or artificial stupidity-connote the consequential damages AI may bring about, as well as the ethical concerns about the technology. For instance, the New York Times (Steyerl, 2018) commented that “Just as socialism in practice has been a far cry from the glorious promises of revolutionaries, artificial stupidity is a mediocre and greedy version of the sublime machine intelligence intended to elevate and improve human life.” Finally, compounds such as human-level/human-like AI or narrow AI speak to the attempts to scientifically define the attributes or domains of AI. The Guardian (Mackintosh, 2003) did an interview with Ray Kurzweil and asked him whether AI was experiencing a renaissance. As an authority on artificial intelligence, Ray viewed this era as narrow AI, “where systems are performing intelligent functions that used to require human intelligence,” and he further described that “Intelligent systems can fly and land airplanes or make financial investment decisions.”
As for the argument patterns on AI, a total of nine patterns emerged from coded texts. Media coverage on emerging technologies tends to present both the utopian and dystopian extremes (The Royal Society, 2019). Our results seem to comport with such an argument. A total of 733 argumentative statements were identified, summarized, and presented in Table 4a, Table 4b.
Among the nine argumentation patterns, the most prevalent is labeled as pragmatic patterns, accounting for about 38.6% of the accumulated arguments. This argument speaks to a wide range of practical applications of AI (e.g., nurse-robots, robot vacuum cleaner), emphasizing the everyday utility of AI in private and business life. Ranked second is the pattern labeled as relativizing. This argument, accounting for 16.1% of the total, stresses the limited benefits of AI and warns that its utility is overestimated and may lead to a hype bubble or marketing bluff. Other argument patterns consist of economically optimistic (10.8%), political criticism (8.9%), euphoric (6.8%), government support (5.2%), economically pessimistic (5.2%), apocalyptic (4.9%), and international competition (3.5%).
Chi-square test showed that there was a statistically significant difference in the distribution of the argumentation patterns (x2<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">x</mi></mrow><mn is="true">2</mn></msup></mrow></math>(8) = 640.19, p < .001). Further test showed that the three topics, including pragmatic, relativizing, and optimistic views, contributed the most to the overall differences of distribution. The majority of pairwise comparisons are statistically significant. For instance, there was a statistically significant difference of distribution between economically optimistic and economically pessimistic arguments (x2<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">x</mi></mrow><mn is="true">2</mn></msup></mrow></math>(1) = 14.86, p < .001).
RQ 4 addresses the main stakeholders pertaining to AI in the media. In Fig. 2a, Fig. 2b, each node represents a stakeholder and the linkage refers to the frequency of co-occurrence of two stakeholders in the news articles. Chi-square test showed a statistically significant difference in the distribution of the actors-business entities, research communities, and governments (x2<math><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">x</mi></mrow><mn is="true">2</mn></msup></mrow></math>(2) = 197.29, p < .001). Business-related people or entities were mentioned most frequently in the media, as compared to other stakeholders.
Social network analysis revealed that the U.S., China, Russia, Japan, and Germany are the nodes ranked high on the following measures: degree centrality, closeness centrality, betweenness centrality, and eigencentrality (Borgatti et al., 2018). In other words, these countries possess more vital positions in the network of nations (see Table 5). Similarly, the U.S., China, and Japan were the three nations mentioned most frequently in our manually coded data. Fig. 2b showed that the three types of actors (business, research institutes, and government agencies) are well connected with each other. Among all the actors, giant corporations (e.g., Google and Facebook) emerged to be major players. In summary, the results of social network analysis comport with what we found from manual coding.
In Fig. 3a, the results of MCA revealed significant differences of reporting topics across newspapers. The New York Times tended to focus more business; the guardian was more focused on robots and algorithms; and the other two newspapers were more related to reporting studies and applications of AI. Fig. 3b showed some interesting patterns between nations and reporting AI: Compared to other nations, the U.S. was closely related to media reporting of more diverse topics; China was more related to media coverage of AI-related business; whereas Turkey, Iraq and Iran were closer to the reportage of risks and weapons pertaining to AI. Fig. 3c, on the other hand, illustrates the association between the topics and the argumentations. For example, closely related to the topic of job and economy is the economically optimistic argumentation. In contrast, the argumentation featuring government support is closely related to the topic of regulation and policy of AI.
Our study shows that media coverage of AI has been ongoing for a long time, spanning more than four decades. Despite the long history of reporting, the coverage has become intensive only in recent five years. Our analysis also reveals how the media regularly define AI as a viable solution to common problems (e.g., economy, health) in everyday life. Such a state of affairs attests to the status of AI as an emerging technology.
Media coverage of AI cuts across a variety of topics, among which the most prevalent are robots/humanoid, brain science/intelligence, and government regulation. A large share of the identified AI themes aligns well with those presented by Dudo et al. (2011) on American newspapers’ coverage of nanotechnology. Those authors identified a total of nine themes of nanotechnology and classified them into two general categories: content themes and conceptual themes. The content themes comprise business, national security, health, and environment. The conceptual ones include risks, benefits, regulations, research, and uncertainty. Similarly, AI-related reporting reflects interests in both content and conceptual themes. By comparing media coverage of AI with that of nanotechnology, our study further reveals that reporting practices of the two emerging technologies share more commonalities in conceptual themes than in content aspects.
Over the years, media have taken a mixed and seemingly balanced approach to covering AI. For instance, journalists have used a large range of lexical compounds and argumentation patterns to construct vibrant public discourses about the identities and implications of AI. Such a finding echoes prior studies on the vital roles of the two framing devices in the media. As Koteyko et al. (2010) argued, creative compounds exercise their roles of communication in both conceptual framing of the topic and communicative structuring of the lexical cohesion. By creating compounds such as superintelligence and AI revolution, media draw immediate attention to the attributes (e.g., radical novelty, identity of power, and prominent impact) characterizing emerging technologies. In the same vein, using emotive compounds such as AI killer and AI bias, media can easily shift the focus from joys about the new technology to the grave uncertainty/ambiguity surrounding AI. Usage of these compounds points to the widespread and startling implications of AI technologies (Rotolo et al., 2015). On the one hand, media convey the promise and hope that AI may bring about. Consider a workless future and immortal life, wherein mankind will transcend their biological limitations and amplify their creativity (Kurzweil, 2005). At the other end of the spectrum is the apocalyptic pattern, characterized as dystopian nightmares of humanity loss and robot uprisings. Tesla chief executive Elon Musk maintains that “we are summoning the demon with artificial intelligence” (McFarland, 2014).
Although some news tends to veer toward the melodramatic and sensational side of AI technologies, our manually coded data have shown that benefits-oriented argumentation patterns (e.g., euphoric, pragmatic, and economically optimistic arguments) appeared more frequently than those emphasizing risks or limitations (e.g., economically pessimistic and relativizing arguments). This predominantly positive attitude goes along with the pattern found in media coverage of nanotechnology, which also concentrated more on its benefits than on risks (Dudo et al., 2011). Yet, different from prior research on emerging technologies, our analysis identified an argumentation pattern that was left out of Rössler’s (2001) typology: international competition. Precisely because of the substantial impact/uncertainty involved in AI technologies, all four newspapers noted possible fundamental changes in future geopolitical landscape. Probably more telling than international political conflicts, media discourses have been dominated by a few actors including government agencies, business giants, and research institutes. Missing are the voices from stakeholders such as ordinary citizens, anti-AI activists, etc.
Our study provides insight into the theories and methods of emerging technologies. Studies of the line have been suffering a lack of rigorous definition and conceptualization of emerging technologies, evincing the need for further research on explanatory factors, underlying rules, and evaluative outcomes of AI adoption. Based on a systematic analysis of empirical data, our study shows that the massive surge in diffusion of AI technologies brings both benefits and risks to the society at a much great scale. In a more theoretical sense, the five attributes propounded by Rotolo et al. (2015) could be a good departing point for exploring media communication of AI. By mapping out the framing devices (e.g., lexical compounds, argumentation patterns) about AI in the media, our research further underscores a key attribute of emerging technologies-coherence of identity-manifesting the underlying interests, concerns, and views on AI.
Focusing on an increasingly popular technology, not only does the present study shed light on media coverage of AI but also of emerging technologies in general. In other words, our study could serve as a case to compare with when researchers are interested in exploring other types of emerging technologies. The impact of media coverage of an emerging technology could be profound when the technology links to reality extensively, stimulates plural interpretations, and stokes controversy. For instance, our study showed that the most mentioned stakeholder in newspaper coverage of AI is business people or entities. In contrast, scientists were mentioned most frequently in newspaper coverage of nanotechnology (Metag & Marcinkowski, 2014). Such a comparison is likely due to the extensive applications of AI technologies in the real world outside the academy. It would certainly be interesting to expand our analysis by systematically comparing how newspapers cover AI and other technologies, but it goes beyond the scope of the current study. Thus, we call for additional research exploring and comparing the coverage patterns and characteristics of multiple emerging technologies.
More importantly, the present study provided insight into framing theory, particularly lexical and argumentative devices for framing. Framing devices are so diverse that future research should explore the nature of each, as well as simultaneous usage thereof. In the present study, we looked at how lexical compounds, argumentation patterns, and network of actors work together to frame AI. The general trend found in our analysis does not necessarily guarantee that AI technologies are framed the same by different stakeholders. Instead, it remains understudied how media framing takes place across key stakeholders and at different levels, such as lexical, argumentative, and network levels. Further research addressing these questions will inform the building of framing theory in the context of emerging technologies.
Our findings suggest that AI is more sophisticated as compared to other emerging technologies in human history. The striking differences are characterized by the wide applications of the technology as well as the complex contexts it is embedded. Makridakis (2017) compared AI revolution with industrial and digital revolutions, and argued that the impact of AI would be more extensive and far-reaching. The author also contended that AI would influence developing countries more than developed ones. As AI is coming into force in various ways rapidly, it is imperative to treat AI as a fully public issue which warrants intensive discussion, debate, and reflection.
Consider, the example, AI use in Education. The technology can help professors to identify the most constructive coaching strategies, to prevent cheating in exams, and to handle repetitive tasks such as answering students’ questions. Nevertheless, is it really a better option if future generations are educated by smart machines than by humans? Is it wise to leave questions of ethics, equities, privacy, arts and creativity to AI systems? In news industry, AI has already assisted newspapers such as The Los Angeles Times in writing articles. It is predicted that AI technologies will soon be able to produce videos in which a picture featuring a moving person can be overlaid to another sequence of a clip. If so, the technology may push the idea of fake news to a whole new level. AI-related challenges are so fuzzily defined that media can and shall play a big role in providing people with quality information to make sense of what AI is, what it will be, and what it could be. Meanwhile, journalists need multiple sources and contributors from different realms to help produce in-depth articles to inform and educate the public on AI technology.
Several caveats should be discussed when interpreting the research findings. First, the study only covered a limited number of English-language newspapers, so that the findings may not be generalizable to other media outlets. Second, although we followed an analysis procedure used in prior studies to screen articles, the process could be stringent and, thus, leave out some interesting and informative articles. Third, our manual coding is limited to lexical compounds and argumentation patterns. It would be of interest to extend the analysis to other framing devices, such as values, messengers, simultaneous metaphors in contextualizing AI.
Future research should continue to investigate how media cover AI, similarly or dissimilarly, as compared to other emerging technologies. With the diffusion of social media, it would be of value to look at how AI is discussed differently across mainstream media and social media platforms (cf., Cacciatore et al. 2012). Future investigation can focus on the coverage of AI technology across a more diverse media platforms including, but not limited to, newspapers, magazines, online news portals, social media, among others.
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
This work was supported by National Social Science Fund of China, China, grant ID: 18CTQ027.