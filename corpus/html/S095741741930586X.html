<div class="Body u-font-serif" id="body"><div><section id="sec0001"><h2 id="sctt0004" class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><p id="p0005"><em>Keywords</em> are special words that are typically embedded in documents and provide a compact and precise representation of the document content. Author-specified keywords for research articles and blogs not only convey the topics that the document covers, but are also used by search engines and document databases to efficiently locate information. <em>Keywords</em> are also used for categorizing and clustering stories in news industry, document summarization, and recommendations. Keywords can also aid in constructing titles for articles, assigning tags to blogs, and so on.</p><p id="p0006">Not all documents on the Web, however, are accompanied by keywords assigned by authors, in which case important and relevant terms have to be extracted from the document itself. Inundated with the massive volume of digital documents available on the Internet, it is in-feasible to manually extract keywords. Consequently, NLP researchers continually strive towards improving automated methods for <em>keyword extraction</em>(KE). Keyphrase extraction is considered as an extension of the keyword extraction task (<a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>).</p><p id="p0007">Arising from the problem of automated index generation (<a name="bbib0023" href="#bib0023" class="workspace-trigger">Luhn,&nbsp;1957</a>), earliest keyword extraction techniques used statistical methods (<a name="bbib0016" href="#bib0016" class="workspace-trigger">Herrera, Pury, 2008</a>, <a name="bbib0029" href="#bib0029" class="workspace-trigger">Ortuno, Carpena, Bernaola-Galván, Muñoz, Somoza, 2002</a><span>), which begot the advantage of language and domain independence. With recent popularity of <a href="/topics/computer-science/machine-learning-approach" title="Learn more about machine learning approaches from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning approaches</a><span>, supervised and <a href="/topics/computer-science/unsupervised-method" title="Learn more about unsupervised methods from ScienceDirect's AI-generated Topic Pages" class="topic-link">unsupervised methods</a> for keyword extraction have been in forefront of the research arena (</span></span><a name="bbib0004" href="#bib0004" class="workspace-trigger">Boudin, 2013</a>, <a name="bbib0007" href="#bib0007" class="workspace-trigger">Bulgarov, Caragea, 2015</a>, <a name="bbib0027" href="#bib0027" class="workspace-trigger">Mothe, Ramiandrisoa, Rasolomanana, 2018</a>). Supervised learning methods basically <em>identify</em><span> the keywords (or keyphrases) by modeling the problem as <a href="/topics/engineering/binary-classification-task" title="Learn more about binary classification task from ScienceDirect's AI-generated Topic Pages" class="topic-link">binary classification task</a>, while unsupervised methods </span><em>extract</em> keywords by quantifying and ranking the words’ <em>embedded-ness</em> in text.</p><p id="p0008"><span>Though enrichment of features in supervised approaches and growing sophistication in techniques have achieved enhanced performance, inadvertently the methods have promoted fixation for document structure, language, domain, and collection. Several state-of-the-art supervised algorithms for keyword extraction fail to accommodate a generic design because of one of the following three reasons. First, they require <a href="/topics/computer-science/linguistic-knowledge" title="Learn more about linguistic knowledge from ScienceDirect's AI-generated Topic Pages" class="topic-link">linguistic knowledge</a> and hence are dependent on the language tools (for example the works of </span><a name="bbib0011" href="#bib0011" class="workspace-trigger">Chuang, Manning, Heer, 2012</a>, <a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth, 2003</a>, <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen, Kan, 2007</a>, <a name="bbib0038" href="#bib0038" class="workspace-trigger">Zhang, 2008</a>). These methods generate language-dependent features that are specific to the language of the training set.</p><p id="p0009">Second, most of the existing supervised algorithms are domain dependent (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0018" href="#bib0018" class="workspace-trigger">Kim, Medelyan, Kan, Baldwin, 2010</a>, <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen, Kan, 2007</a>). For example, citations-enhanced keyword extraction (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>) works only when citation information is available. Thus, such techniques work well for scientific domain, but are not suitable for a generic domain that contains texts from news articles, blog articles, meeting transcripts, etc. <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen&nbsp;and Kan&nbsp;(2007)</a> extracted keyphrases from scientific papers by enriching the feature set with morphological information found in scientific text, which is also an example of domain-dependent keyphrase extraction.</p><p id="p0010">Third, existing supervised KE methods are collection-dependent because they use statistical features that are derived from the document collection (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0011" href="#bib0011" class="workspace-trigger">Chuang, Manning, Heer, 2012</a>, <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen, Kan, 2007</a>, <a name="bbib0033" href="#bib0033" class="workspace-trigger">Sterckx, Demeester, Develder, Caragea, 2016</a>). Frequency-based statistical features like tf-idf, positions of occurrence, etc. are collection sensitive, and they change drastically with a slight alteration of the training set.</p><p id="p0011">In addition to above three primary reasons, some algorithms require external information from sources like Wikipedia (<a name="bbib0025" href="#bib0025" class="workspace-trigger">Medelyan, Frank, Witten, 2009</a>, <a name="bbib0039" href="#bib0039" class="workspace-trigger">Zhang, Chang, Liu, Gollapalli, Li, Xiao, 2017</a>) or expert knowledge in the form of label-distribution to incorporate hints (e.g. a noun word occurring in the title) (<a name="bbib0015" href="#bib0015" class="workspace-trigger">Gollapalli,&nbsp;Li, &amp; Yang, 2017</a>). This leaves a research gap for generic keyword extractor that can be applied on any text without considering its language, domain, or corpora. Recognizing this gap, we investigate the feasibility of designing a keyphrase prediction model that is domain-, language-, and collection-independent.</p><p id="p0012">Graph-based unsupervised KE methods represent text as graph,<a name="bfn0001" href="#fn0001" class="workspace-trigger"><sup>1</sup></a> and rely on the node properties to discriminate between keywords and non-keywords (<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu, Caragea, 2017</a>, <a name="bbib0020" href="#bib0020" class="workspace-trigger">Litvak, Last, Aizenman, Gobits, Kandel, 2011</a>, <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>). These methods process one document at a time and are autonomous, which makes them collection and domain agnostic. However, these methods are dependent on the language tools as they perform POS tagging<a name="bfn0002" href="#fn0002" class="workspace-trigger"><sup>2</sup></a> for identifying candidate keywords (nouns and adjectives) (<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu, Caragea, 2017</a>, <a name="bbib0020" href="#bib0020" class="workspace-trigger">Litvak, Last, Aizenman, Gobits, Kandel, 2011</a>, <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>, <a name="bbib0034" href="#bib0034" class="workspace-trigger">Tixier, Malliaros, Vazirgiannis, 2016</a>). Because of this reason, graph based KE methods are not pliable for texts in resource-poor languages. It is noteworthy that unsupervised methods often report lower performance as compared to their supervised counterparts.</p><p id="p0013">In this research, we aim to bolster performance of supervised learning approach with the advantages of graph-based keyword extraction methods, sans the bias towards domain or collection underlying the training data. The idea is inspired by consistent success of graph-based KE methods, which are typically unsupervised and weak performers compared to their supervised counterparts. We build over the domain and collection independence of graph-based KE methods and use graph-based node properties as features to develop a supervised model with improved performance. Additionally, we eliminate the language dependency by using statistical properties to filter candidate keywords from the text. Specific contributions of our research are listed below.<dl class="list"><dt class="list-label">1.</dt><dd class="list-description"><p id="p0014">We devise supervised learning approach for automatic keyword extraction using graph-theoretic feature set (<a name="bsec0003" href="#sec0003" class="workspace-trigger">3 Methodology</a>, <a name="bsec0004" href="#sec0004" class="workspace-trigger">4 Modeling text as complex network</a>, <a name="bsec0007" href="#sec0007" class="workspace-trigger">5 Extracting properties of keywords</a>, <a name="bsec0014" href="#sec0014" class="workspace-trigger">6 Inducing the model</a>).</p></dd><dt class="list-label">2.</dt><dd class="list-description"><p id="p0015">We empirically validate our claim that the method is domain-, and collection-independent (<a name="bsec0015" href="#sec0015" class="workspace-trigger">Sections&nbsp;7</a> and <a name="bsec0018" href="#sec0018" class="workspace-trigger">8</a>).</p></dd><dt class="list-label">3.</dt><dd class="list-description"><p id="p0016">Post keyword extraction, we generate keyphrases from the predicted keywords and demonstrate that our method performs comparably with the state-of-the-art supervised keyphrase extraction approaches (<a name="bsec0023" href="#sec0023" class="workspace-trigger">Section&nbsp;8.3</a>).</p></dd><dt class="list-label">4.</dt><dd class="list-description"><p id="p0017">We evaluate the performance of our proposed method on texts from two India languages to establish language independence of the model (<a name="bsec0030" href="#sec0030" class="workspace-trigger">Section&nbsp;8.4</a>).</p></dd></dl></p><p id="p0018"><span>We do not delve into sophisticated deep learning based methods due to the limited volume of training set we have, and time required for training the model. We proceed with classic and simple <a href="/topics/computer-science/classification-machine-learning" title="Learn more about classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">classifiers</a> as a </span><a href="/topics/engineering/proof-of-concept" title="Learn more about proof of concept from ScienceDirect's AI-generated Topic Pages" class="topic-link">proof of concept</a><span>, and believe that use of <a href="/topics/computer-science/deep-learning-technique" title="Learn more about deep learning techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning techniques</a><span> will enhance the performance of the <a href="/topics/computer-science/predictive-model" title="Learn more about predictive model from ScienceDirect's AI-generated Topic Pages" class="topic-link">predictive model</a>.</span></span></p></section><section id="sec0002"><h2 id="sctt0005" class="u-h3 u-margin-l-top u-margin-xs-bottom">2. Related works</h2><p id="p0019"><span>Existing supervised methods for automatic keyword extraction tackle the problem as a phrase-based <a href="/topics/engineering/binary-classification-task" title="Learn more about binary classification task from ScienceDirect's AI-generated Topic Pages" class="topic-link">binary classification task</a>, where keyphrases (</span><em>n</em>-grams) are extracted from the documents (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth, 2003</a>, <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen, Kan, 2007</a>, <a name="bbib0033" href="#bib0033" class="workspace-trigger">Sterckx, Demeester, Develder, Caragea, 2016</a>, <a name="bbib0035" href="#bib0035" class="workspace-trigger">Turney, 1999</a>, <a name="bbib0036" href="#bib0036" class="workspace-trigger">Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</a><span>). These methods first create a labelled training set by constructing features for candidate phrases (or words) in the text and designate each phrase as either positive (keywords) or negative (non-keywords) by consulting the associated gold-standard list. The training set thus created is used to induce a <a href="/topics/computer-science/predictive-model" title="Learn more about predictive model from ScienceDirect's AI-generated Topic Pages" class="topic-link">predictive model</a>, which predicts word (or phrase) from unseen documents as keyword or non-keyword. Several algorithms for inducing a predictive model have been explored, including CRF and SVM (</span><a name="bbib0038" href="#bib0038" class="workspace-trigger">Zhang,&nbsp;2008</a>), Bagged decision tree (<a name="bbib0025" href="#bib0025" class="workspace-trigger">Medelyan&nbsp;et&nbsp;al., 2009</a>), Naïve Bayes (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0033" href="#bib0033" class="workspace-trigger">Sterckx, Demeester, Develder, Caragea, 2016</a>), etc.</p><p id="p0020">Since eliciting good quality features is crucial for performance of the trained model, feature construction is recognized as the focal task in creation of training set for supervised KE approaches. Wide variety of features have been proposed to obtain high quality training set for inducing well performing models, e.g., tf-idf, POS tags, n-gram features, etc. <a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth&nbsp;(2003)</a><span><span> reported that adding certain <a href="/topics/computer-science/linguistic-knowledge" title="Learn more about linguistic knowledge from ScienceDirect's AI-generated Topic Pages" class="topic-link">linguistic knowledge</a> (e.g., </span><a href="/topics/computer-science/syntactics" title="Learn more about syntactic from ScienceDirect's AI-generated Topic Pages" class="topic-link">syntactic</a> features) to the training set improves performance of the automatic keyword extractor as compared to relying only on statistics-based features such as, term frequency, n-grams, etc. </span><a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen&nbsp;and Kan&nbsp;(2007)</a> used morphological features of text in the training set in addition to simple statistics-based features, and designed a keyword extractor for scientific articles. <a name="bbib0025" href="#bib0025" class="workspace-trigger">Medelyan&nbsp;et&nbsp;al.&nbsp;(2009)</a> incorporated information from external sources like Wikipedia to improve the training set. In addition to these, structural features of the document (<a name="bbib0021" href="#bib0021" class="workspace-trigger">Lopez &amp; Romary,&nbsp;2010a</a>), knowledge about domain and collection (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen, Kan, 2007</a>), citation-information (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>), incorporating expert knowledge (<a name="bbib0015" href="#bib0015" class="workspace-trigger">Gollapalli&nbsp;et&nbsp;al., 2017</a>), and multidimensional information (<a name="bbib0039" href="#bib0039" class="workspace-trigger">Zhang&nbsp;et&nbsp;al., 2017</a>) are some popular methods for enriching the training set.</p><p id="p0021">Unsupervised KE techniques largely comprise graph-based methods, which transform the text into a graph (complex network) and use graph-theoretic properties to rank keywords. These methods are largely word-based (i.e. unigrams are extracted) (<a name="bbib0013" href="#bib0013" class="workspace-trigger">Duari, Bhatnagar, 2019</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>, <a name="bbib0034" href="#bib0034" class="workspace-trigger">Tixier, Malliaros, Vazirgiannis, 2016</a>), with a few being phrase-based (i.e. <em>n</em>-grams are extracted) (<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu, Caragea, 2017</a>, <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>). Node properties like PageRank (<a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea &amp; Tarau,&nbsp;2004</a>), PageRank along with position of the word in text (<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu &amp; Caragea,&nbsp;2017</a><span>), <a href="/topics/computer-science/degree-centrality" title="Learn more about degree centrality from ScienceDirect's AI-generated Topic Pages" class="topic-link">degree centrality</a> (</span><a name="bbib0020" href="#bib0020" class="workspace-trigger">Litvak&nbsp;et&nbsp;al., 2011</a>), coreness (<a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau &amp; Vazirgiannis,&nbsp;2015</a>), etc. have been studied extensively in the past. Network representation of the text leverages unsupervised keyword extraction methods because of their independence from the influence of domain of the document or corpus. We aim to overcome the domain and collection dependence of supervised KE methods by using graph-based node properties as features for training. Furthermore, we also overcome the problem of language dependency by using a statistical filter for candidate selection while maintaining the efficacy of supervised KE methods.</p></section><section id="sec0003"><h2 id="sctt0006" class="u-h3 u-margin-l-top u-margin-xs-bottom">3. Methodology</h2><p id="p0022"><span>Graph-based approaches for keyword extraction established that keywords possess certain properties, which impart special character to them. We hypothesize that succinct properties of keywords are revealed when the text is presented as graph. These properties are effective signals to discriminate between keywords and non-keywords. Accordingly, we employ node properties in the <a href="/topics/computer-science/graph-representation" title="Learn more about graph representation from ScienceDirect's AI-generated Topic Pages" class="topic-link">graph representation</a> of text as features to fortify against dependence on linguistic, domain, collection, or structural features of the document. We propose a supervised framework to extract keywords from single document, which consists of the following steps.</span><dl class="list"><dt class="list-label">1.</dt><dd class="list-description"><p id="p0023">Select candidate keywords from each document, and construct the corresponding graph-of-text (<a name="bsec0004" href="#sec0004" class="workspace-trigger">Section&nbsp;4</a>).</p></dd><dt class="list-label">2.</dt><dd class="list-description"><p id="p0024">Extract select node properties as features from each graph-of-text (<a name="bsec0007" href="#sec0007" class="workspace-trigger">Section&nbsp;5</a>).</p></dd><dt class="list-label">3.</dt><dd class="list-description"><p id="p0025"><span>Prepare the training set and induce a <a href="/topics/computer-science/predictive-model" title="Learn more about predictive model from ScienceDirect's AI-generated Topic Pages" class="topic-link">predictive model</a> (</span><a name="bsec0014" href="#sec0014" class="workspace-trigger">Section&nbsp;6</a>).</p></dd></dl></p><p id="p0026">Steps (i) and (ii) harbour innovative approaches that are detailed below. We use the model induced in step (iii) to predict keywords from unseen documents.</p></section><section id="sec0004"><h2 id="sctt0007" class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Modeling text as complex network</h2><p id="p0027"><span>Text is modeled as a complex system, where the basic units, i.e. words, interact among each other to bring out the ideas that the author intends to communicate. The interaction between words can be mapped using various relationships, such as statistical, semantic, <a href="/topics/computer-science/syntactics" title="Learn more about syntactic from ScienceDirect's AI-generated Topic Pages" class="topic-link">syntactic</a>, discourse, cognitive, etc. (</span><a name="bbib0003" href="#bib0003" class="workspace-trigger">Blanco &amp; Lioma,&nbsp;2012</a>). The most frequently used relation for automatic KE systems is co-occurrence based statistical relation (<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu, Caragea, 2017</a>, <a name="bbib0020" href="#bib0020" class="workspace-trigger">Litvak, Last, Aizenman, Gobits, Kandel, 2011</a>, <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>, <a name="bbib0034" href="#bib0034" class="workspace-trigger">Tixier, Malliaros, Vazirgiannis, 2016</a>).</p><p id="p0028">We use a parameter-free and language-agnostic approach for creating complex networks from text as proposed in our previous work (<a name="bbib0013" href="#bib0013" class="workspace-trigger">Duari &amp; Bhatnagar,&nbsp;2019</a>). The network representation of text is created by - (i)&nbsp;selecting a subset of words from the text as candidates (<a name="bsec0005" href="#sec0005" class="workspace-trigger">Section&nbsp;4.1</a>) and (ii)&nbsp;using these candidate keywords as nodes, and forging relationships between nodes to create edges (<a name="bsec0006" href="#sec0006" class="workspace-trigger">Section&nbsp;4.2</a>). We briefly describe the method below.</p><section id="sec0005"><h3 id="sctt0008" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.1. Selecting candidate keywords</h3><p id="p0029">In order to reduce the search space for possible keywords, we first eliminate frequently used non-content bearing words from processing. To do this, we perform <em>stopword removal</em> using a standard English stop words list<a name="bfn0003" href="#fn0003" class="workspace-trigger"><sup>3</sup></a> For non-English texts, a custom-curated stopwords list can be adopted to suit the requirement. We then apply a filter to identify candidate keywords from the remaining words. We use <em>σ</em>-index&nbsp;(<a name="bbib0029" href="#bib0029" class="workspace-trigger">Ortuno&nbsp;et&nbsp;al., 2002</a>) as a statistical filter to perform this task.</p><p id="p0030">The <em>σ</em>-index of a word computes normalized standard deviation of the word’s spacing distribution in successive occurrences, with higher values of <em>σ</em>-index indicating higher term relevance&nbsp;(<a name="bbib0029" href="#bib0029" class="workspace-trigger">Ortuno&nbsp;et&nbsp;al., 2002</a>). We adopt <a name="bbib0016" href="#bib0016" class="workspace-trigger">Herrera&nbsp;and Pury&nbsp;(2008)</a> implementation of <em>σ</em>-index, where the <em>σ</em>-index of a word <em>w</em> in a document <em>D</em> is defined as below.</p><p id="p0031">Let, <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mo is=&quot;true&quot;>|</mo><mi is=&quot;true&quot;>D</mi><mo is=&quot;true&quot;>|</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.38ex" height="2.709ex" viewBox="0 -847.3 3608.1 1166.2" role="img" focusable="false" style="vertical-align: -0.741ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(1166,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2222,0)"><use xlink:href="#MJMAIN-7C"></use></g><g is="true" transform="translate(2501,0)"><use xlink:href="#MJMATHI-44"></use></g><g is="true" transform="translate(3329,0)"><use xlink:href="#MJMAIN-7C"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">N</mi><mo linebreak="goodbreak" is="true">=</mo><mo is="true">|</mo><mi is="true">D</mi><mo is="true">|</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-1"><math><mrow is="true"><mi is="true">N</mi><mo linebreak="goodbreak" is="true">=</mo><mo is="true">|</mo><mi is="true">D</mi><mo is="true">|</mo></mrow></math></script></span> be the document length, <em>n</em> be the number of occurrences of <em>w</em>, and <em>p<sub>i</sub></em> be the position of <em>i</em>th occurrence of <em>w</em>. Note that <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mn is=&quot;true&quot;>0</mn></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>0</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.574ex" height="2.341ex" viewBox="-38.5 -741.6 2830.5 1007.7" role="img" focusable="false" style="vertical-align: -0.618ex; margin-left: -0.089ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><use transform="scale(0.707)" xlink:href="#MJMAIN-30"></use></g></g><g is="true" transform="translate(1235,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2291,0)"><use xlink:href="#MJMAIN-30"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">p</mi><mn is="true">0</mn></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">0</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-2"><math><mrow is="true"><msub is="true"><mi is="true">p</mi><mn is="true">0</mn></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">0</mn></mrow></math></script></span> and <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mi is=&quot;true&quot;>N</mi><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="13.743ex" height="2.341ex" viewBox="-38.5 -741.6 5917 1007.7" role="img" focusable="false" style="vertical-align: -0.618ex; margin-left: -0.089ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(424,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(975,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(2210,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3266,0)"><use xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(4377,0)"><use xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(5378,0)"><use xlink:href="#MJMAIN-31"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mi is="true">N</mi><mo linebreak="goodbreak" is="true">+</mo><mn is="true">1</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-3"><math><mrow is="true"><msub is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mi is="true">N</mi><mo linebreak="goodbreak" is="true">+</mo><mn is="true">1</mn></mrow></math></script></span>. Then <em>σ</em>(<em>w</em>) is computed as:<span class="display"><span id="eq0001" class="formula"><span class="label">(1)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C3;</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>w</mi><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>s</mi><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>w</mi><mo is=&quot;true&quot;>)</mo></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>w</mi><mo is=&quot;true&quot;>)</mo></mrow></mfrac><mtext is=&quot;true&quot;>,</mtext></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="13.219ex" height="4.426ex" viewBox="0 -1217.1 5691.4 1905.8" role="img" focusable="false" style="vertical-align: -1.6ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3C3"></use></g><g is="true" transform="translate(739,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(1106,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(2512,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3290,0)"><g transform="translate(397,0)"><rect stroke="none" width="1604" height="60" x="0" y="220"></rect><g is="true" transform="translate(107,586)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-73"></use></g><g is="true" transform="translate(331,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(607,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(1114,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(60,-441)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-3BC"></use></g><g is="true" transform="translate(426,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(702,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(1208,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-29"></use></g></g></g></g><g is="true" transform="translate(5412,0)"><use xlink:href="#MJMAIN-2C"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">σ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">s</mi><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mrow is="true"><mi is="true">μ</mi><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow></mfrac><mtext is="true">,</mtext></mrow></math></span></span><script type="math/mml" id="MathJax-Element-4"><math><mrow is="true"><mi is="true">σ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">s</mi><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mrow is="true"><mi is="true">μ</mi><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow></mfrac><mtext is="true">,</mtext></mrow></math></script></span></span></span>where <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>w</mi><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><msubsup is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>0</mn></mrow><mi is=&quot;true&quot;>n</mi></msubsup><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></msub><mo is=&quot;true&quot;>&amp;#x2212;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>)</mo></mrow></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></mfrac><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="27.305ex" height="4.181ex" viewBox="0 -1269.9 11756.2 1800.2" role="img" focusable="false" style="vertical-align: -1.232ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3BC"></use></g><g is="true" transform="translate(770,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(1106,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(2543,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3321,0)"><g transform="translate(397,0)"><rect stroke="none" width="4689" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,623)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(747,337)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(747,-203)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(172,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(562,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-30"></use></g></g></g><g is="true" transform="translate(1630,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-28" is="true"></use><g is="true" transform="translate(275,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(356,-163)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(172,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(562,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(1514,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(2064,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(356,-163)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g><use transform="scale(0.707)" xlink:href="#MJMAIN-29" is="true" x="3768" y="0"></use></g></g><g is="true" transform="translate(1680,-381)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(424,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(975,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g></g><g is="true" transform="translate(8807,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(9585,0)"><g transform="translate(397,0)"><rect stroke="none" width="1652" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,467)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(628,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(1178,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(161,-381)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(424,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(975,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">μ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mi is="true">n</mi></msubsup><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">−</mo><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></mfrac><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">N</mi><mo is="true">+</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-5"><math><mrow is="true"><mi is="true">μ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mi is="true">n</mi></msubsup><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">−</mo><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></mfrac><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">N</mi><mo is="true">+</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></mfrac></mrow></math></script></span> is the average distance between successive occurrences of <em>w</em> and <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>s</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>w</mi><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><msqrt is=&quot;true&quot;><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><msubsup is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>0</mn></mrow><mi is=&quot;true&quot;>n</mi></msubsup><msup is=&quot;true&quot;><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></msub><mo is=&quot;true&quot;>&amp;#x2212;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo is=&quot;true&quot;>&amp;#x2212;</mo><mi is=&quot;true&quot;>&amp;#x3BC;</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>w</mi><mo is=&quot;true&quot;>)</mo></mrow><mo is=&quot;true&quot;>)</mo></mrow><mn is=&quot;true&quot;>2</mn></msup></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn></mrow></mfrac></msqrt></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="28.961ex" height="6.144ex" viewBox="0 -1798.2 12469.2 2645.4" role="img" focusable="false" style="vertical-align: -1.968ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-73"></use></g><g is="true" transform="translate(636,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(1106,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(2409,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3465,0)"><use xlink:href="#MJSZ3-221A" x="0" y="197"></use><rect stroke="none" width="8002" height="60" x="1000" y="1588"></rect><g transform="translate(1000,0)"><g is="true"><g transform="translate(120,0)"><rect stroke="none" width="7762" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,623)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(747,337)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(747,-203)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(172,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(562,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-30"></use></g></g></g><g is="true" transform="translate(1796,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-28" is="true"></use><g is="true" transform="translate(275,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-28" is="true"></use><g is="true" transform="translate(275,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(356,-163)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(172,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(562,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(1514,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(2064,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(356,-163)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g><use transform="scale(0.707)" xlink:href="#MJMAIN-29" is="true" x="3768" y="0"></use></g><g is="true" transform="translate(3215,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(3765,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3BC"></use></g><g is="true" transform="translate(4192,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(275,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(782,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-29"></use></g></g><use transform="scale(0.707)" xlink:href="#MJMAIN-29" is="true" x="7424" y="0"></use></g><g is="true" transform="translate(5525,337)"><use transform="scale(0.5)" xlink:href="#MJMAIN-32"></use></g></g></g><g is="true" transform="translate(3216,-381)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(424,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(975,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msqrt is="true"><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mi is="true">n</mi></msubsup><msup is="true"><mrow is="true"><mo is="true">(</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">−</mo><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">−</mo><mi is="true">μ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mn is="true">2</mn></msup></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></mfrac></msqrt></mrow></math></span></span><script type="math/mml" id="MathJax-Element-6"><math><mrow is="true"><mi is="true">s</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><msqrt is="true"><mfrac is="true"><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">0</mn></mrow><mi is="true">n</mi></msubsup><msup is="true"><mrow is="true"><mo is="true">(</mo><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">p</mi><mrow is="true"><mi is="true">i</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub><mo is="true">−</mo><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo is="true">−</mo><mi is="true">μ</mi><mrow is="true"><mo is="true">(</mo><mi is="true">w</mi><mo is="true">)</mo></mrow><mo is="true">)</mo></mrow><mn is="true">2</mn></msup></mrow><mrow is="true"><mi is="true">n</mi><mo is="true">−</mo><mn is="true">1</mn></mrow></mfrac></msqrt></mrow></math></script></span> is the standard deviation of word occurrences. We retain top-33% words ranked by <em>σ</em>-index as candidate keywords, which drastically reduces the search space to one-third of the original length.</p><p id="p0032">Conventional graph-based keyword extraction methods use part-of-speech taggers and select nouns and adjectives as candidate keywords using linguistic tools&nbsp;(<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu, Caragea, 2017</a>, <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>). This makes these approaches dependent on the linguistic tools and inefficacious for resource-poor languages. The use of statistical filter, <em>σ</em>-index, in our proposed approach imparts language-independence to this phase, and thus makes the approach language agnostic.</p><p id="p0033">Please note that the computation of <em>σ</em>-index requires a word to occur at least twice in the document. This does not conflict with our goal because a word that occurs exactly once is unlikely to be a keyword. Furthermore, as words in short texts do not occur frequently, we omit the computation of <em>σ</em>-index for documents with less than 100 unique words excluding stopwords. In such situation, each word is considered as a candidate keyword.</p></section><section id="sec0006"><h3 id="sctt0009" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.2. Network construction</h3><p id="p0034"><span>We model text as a weighted, <a href="/topics/computer-science/directed-graphs" title="Learn more about undirected graph from ScienceDirect's AI-generated Topic Pages" class="topic-link">undirected graph</a> </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>G</mi><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>V</mi><mo is=&quot;true&quot;>,</mo><mi is=&quot;true&quot;>E</mi><mo is=&quot;true&quot;>,</mo><mi is=&quot;true&quot;>W</mi><mo is=&quot;true&quot;>)</mo><mo is=&quot;true&quot;>,</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="15.447ex" height="2.709ex" viewBox="0 -847.3 6650.9 1166.2" role="img" focusable="false" style="vertical-align: -0.741ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-47"></use></g><g is="true" transform="translate(1064,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2120,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(2510,0)"><use xlink:href="#MJMATHI-56"></use></g><g is="true" transform="translate(3279,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(3724,0)"><use xlink:href="#MJMATHI-45"></use></g><g is="true" transform="translate(4489,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(4934,0)"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(5982,0)"><use xlink:href="#MJMAIN-29"></use></g><g is="true" transform="translate(6372,0)"><use xlink:href="#MJMAIN-2C"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">G</mi><mo linebreak="goodbreak" is="true">=</mo><mo is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo is="true">,</mo><mi is="true">W</mi><mo is="true">)</mo><mo is="true">,</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-7"><math><mrow is="true"><mi is="true">G</mi><mo linebreak="goodbreak" is="true">=</mo><mo is="true">(</mo><mi is="true">V</mi><mo is="true">,</mo><mi is="true">E</mi><mo is="true">,</mo><mi is="true">W</mi><mo is="true">)</mo><mo is="true">,</mo></mrow></math></script></span> where <em>V</em> is the set of vertices that comprises the candidate keywords, <em>E</em> ∈ <em>V</em> × <em>V</em> is the set of edges, and <em>W</em><span> is the corresponding weighted <a href="/topics/computer-science/adjacency-matrix" title="Learn more about adjacency matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">adjacency matrix</a>. We use the conventional relationship of “co-occurrence” of words to define edges between the nodes. Two nodes (candidate words) are linked if they co-occur in a sliding window of user specified size&nbsp;(</span><a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>). In order to eliminate the user parameter (window size), we slide the window over two consecutive sentences (<a name="bbib0013" href="#bib0013" class="workspace-trigger">Duari &amp; Bhatnagar,&nbsp;2019</a>). This strategy begets advantages of capturing coherence in the flow of ideas that a sentence carries from its previous sentence. The links are weighted by the number of times the adjacent nodes (words) co-occur in the original text, and isolated nodes are ignored.</p><div><p id="p0035">It is noteworthy that short texts (1–3 sentences) result into highly dense networks which are often complete graphs. Network density decreases as the number of sentences increases. <a name="bfig0001" href="#fig0001" class="workspace-trigger">Fig.&nbsp;1</a> shows network of a short text containing three sentences.</p><figure class="figure text-xs" id="fig0001"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr1.jpg" height="218" alt="Fig. 1" aria-describedby="cap0001"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (811KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (811KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cap0001"><p id="sp0005"><span class="label">Fig. 1</span>. Network created from sample text in <a name="bfig0001" href="#fig0001" class="workspace-trigger">Fig.&nbsp;1</a>(a) (document id 6 from Hulth2003 dataset).</p></span></span></figure></div></section></section><section id="sec0007"><h2 id="sctt0010" class="u-h3 u-margin-l-top u-margin-xs-bottom">5. Extracting properties of keywords</h2><p id="p0036">Centrality of nodes in a network is a popular estimate of node importance. According to <a name="bbib0004" href="#bib0004" class="workspace-trigger">Boudin&nbsp;(2013)</a><span><span>, <a href="/topics/computer-science/degree-centrality" title="Learn more about degree centrality from ScienceDirect's AI-generated Topic Pages" class="topic-link">degree centrality</a> is conceptually the simplest and computationally most efficient </span><a href="/topics/computer-science/centrality-measure" title="Learn more about centrality measure from ScienceDirect's AI-generated Topic Pages" class="topic-link">centrality measure</a>. However, in the context of weighted graph-of-text, it is more appropriate to use weighted degree (strength) as a measure of node importance (</span><a name="bbib0001" href="#bib0001" class="workspace-trigger">Barrat,&nbsp;Barthelemy, Pastor-Satorras, &amp; Vespignani, 2004</a>). Strength in our setting captures how often the words co-occur with each other in adjacent sentences.</p><div><p id="p0037">Though strength effectively captures node importance, however, probability density distribution of strength for keywords and non-keywords for the training set prepared during our study clearly shows overlapping areas near high strength values (<a name="bfig0002" href="#fig0002" class="workspace-trigger">Fig.&nbsp;2</a>(a)<span>). The overlap indicates that strength alone is not an accurate <a href="/topics/computer-science/discriminators" title="Learn more about discriminator from ScienceDirect's AI-generated Topic Pages" class="topic-link">discriminator</a> between keywords and non-keywords. Next two plots (</span><a name="bfig0002" href="#fig0002" class="workspace-trigger">Fig.&nbsp;2</a><span><span>(b) and (c)) show similar observations for <a href="/topics/computer-science/eigenvector-centrality" title="Learn more about Eigenvector centrality from ScienceDirect's AI-generated Topic Pages" class="topic-link">Eigenvector centrality</a> and PageRank. This impels exploration of other node properties - Coreness, PositionRank, and </span><a href="/topics/computer-science/clustering-coefficient" title="Learn more about Clustering Coefficient from ScienceDirect's AI-generated Topic Pages" class="topic-link">Clustering Coefficient</a><span> - which would aid improvement in the quality of extracted keywords. It is noteworthy that we avoid centrality measures that require expensive all-pair <a href="/topics/computer-science/shortest-path-computation" title="Learn more about shortest path computations from ScienceDirect's AI-generated Topic Pages" class="topic-link">shortest path computations</a>. This maintains the frugality of feature extraction phase, enabling its potential for online usage.</span></span></p><figure class="figure text-xs" id="fig0002"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr2.jpg" height="734" alt="Fig. 2" aria-describedby="cap0002"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (798KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (798KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cap0002"><p id="sp0006"><span class="label">Fig. 2</span>. Density distributions of <a href="/topics/engineering/graph-node" title="Learn more about graph node from ScienceDirect's AI-generated Topic Pages" class="topic-link">graph node</a> properties for keywords and non-keywords using the SMOTE-balanced training set.</p></span></span></figure></div><p id="p0038">All these properties, except Clustering Coefficient, have been individually vetted by state-of-the-art unsupervised graph-based keyword extraction methods. Our goal in this work is to investigate the complex <a href="/topics/engineering/interplay" title="Learn more about interplay from ScienceDirect's AI-generated Topic Pages" class="topic-link">interplay</a> of these properties, which to the best of authors’ knowledge, has not been explored for discriminating between keywords and non-keywords. We describe each of the node properties below.</p><section id="sec0008"><h3 id="sctt0011" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.1. Strength of a node</h3><p id="p0039">Strength (weighted degree) of a node measures its <em>embedded-ness</em> at local level. For node <em>v<sub>i</sub></em> in a weighted network <em>G</em>, it is computed as&nbsp;(<a name="bbib0001" href="#bib0001" class="workspace-trigger">Barrat&nbsp;et&nbsp;al., 2004</a>):<span class="display"><span id="ueq0001" class="formula"><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>s</mi><mi is=&quot;true&quot;>t</mi><mi is=&quot;true&quot;>r</mi><mi is=&quot;true&quot;>e</mi><mi is=&quot;true&quot;>n</mi><mi is=&quot;true&quot;>g</mi><mi is=&quot;true&quot;>t</mi><mi is=&quot;true&quot;>h</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mi is=&quot;true&quot;>j</mi></munder><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mi is=&quot;true&quot;>j</mi></mrow></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mi is=&quot;true&quot;>j</mi></munder><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>i</mi></mrow></msub></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="32.856ex" height="3.199ex" viewBox="0 -847.3 14146.3 1377.5" role="img" focusable="false" style="vertical-align: -1.232ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-73"></use></g><g is="true" transform="translate(469,0)"><use xlink:href="#MJMATHI-74"></use></g><g is="true" transform="translate(831,0)"><use xlink:href="#MJMATHI-72"></use></g><g is="true" transform="translate(1282,0)"><use xlink:href="#MJMATHI-65"></use></g><g is="true" transform="translate(1749,0)"><use xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(2349,0)"><use xlink:href="#MJMATHI-67"></use></g><g is="true" transform="translate(2830,0)"><use xlink:href="#MJMATHI-74"></use></g><g is="true" transform="translate(3191,0)"><use xlink:href="#MJMATHI-68"></use></g><g is="true" transform="translate(3934,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(485,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1219,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(5821,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(6877,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(8492,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g></g><g is="true" transform="translate(10122,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(11178,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(12793,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(291,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">s</mi><mi is="true">t</mi><mi is="true">r</mi><mi is="true">e</mi><mi is="true">n</mi><mi is="true">g</mi><mi is="true">t</mi><mi is="true">h</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">∑</mo><mi is="true">j</mi></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">∑</mo><mi is="true">j</mi></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></math></span></span><script type="math/mml" id="MathJax-Element-8"><math><mrow is="true"><mi is="true">s</mi><mi is="true">t</mi><mi is="true">r</mi><mi is="true">e</mi><mi is="true">n</mi><mi is="true">g</mi><mi is="true">t</mi><mi is="true">h</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">∑</mo><mi is="true">j</mi></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">∑</mo><mi is="true">j</mi></munder><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub></mrow></math></script></span></span></span></p><p id="p0040">Here, <em>w<sub>ij</sub></em> is the corresponding entry in the weight matrix <em>W</em> for edge (<em>v<sub>i</sub>, v<sub>j</sub></em>). High strength is associated with a node being more central or important in the network. The indulging intuition is that a word which is co-occurring with many other words (i.e., has high degree/strength) is important and is likely to be a keyword.</p></section><section id="sec0009"><h3 id="sctt0012" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.2. Eigenvector centrality</h3><p id="p0041">Eigenvector centrality or <em>Prestige</em> of vertex <em>v<sub>i</sub></em> quantifies its <em>embedded-ness</em> in the network while recursively taking into account the prestige of its neighbors. Starting with initial prestige vector <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>p</mi><mo is=&quot;true&quot;>&amp;#x2192;</mo></mover><mn mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>0</mn></msub></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.724ex" height="3.445ex" viewBox="0 -1164.2 1603.2 1483.2" role="img" focusable="false" style="vertical-align: -0.741ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true" transform="translate(199,0)"><use xlink:href="#MJMATHBI-70"></use></g><use xlink:href="#MJMAIN-2192" is="true" x="95" y="545"></use></g><g is="true" transform="translate(1096,-231)"><use transform="scale(0.707)" xlink:href="#MJMAINB-30"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mn mathvariant="bold-italic" is="true">0</mn></msub></mrow></math></span></span><script type="math/mml" id="MathJax-Element-9"><math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mn mathvariant="bold-italic" is="true">0</mn></msub></mrow></math></script></span> where all nodes (words) are assigned equal <em>prestige</em>, <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>p</mi><mo is=&quot;true&quot;>&amp;#x2192;</mo></mover><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>k</mi></msub></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.771ex" height="3.445ex" viewBox="0 -1164.2 1623.7 1483.2" role="img" focusable="false" style="vertical-align: -0.741ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true" transform="translate(199,0)"><use xlink:href="#MJMATHBI-70"></use></g><use xlink:href="#MJMAIN-2192" is="true" x="95" y="545"></use></g><g is="true" transform="translate(1096,-231)"><use transform="scale(0.707)" xlink:href="#MJMATHBI-6B"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mi mathvariant="bold-italic" is="true">k</mi></msub></mrow></math></span></span><script type="math/mml" id="MathJax-Element-10"><math><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mi mathvariant="bold-italic" is="true">k</mi></msub></mrow></math></script></span> is computed recursively as follows till convergence is achieved&nbsp;(<a name="bbib0037" href="#bib0037" class="workspace-trigger">Zaki,&nbsp;Meira&nbsp;Jr, &amp; Meira, 2014</a>).<span class="display"><span id="ueq0002" class="formula"><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>p</mi><mo is=&quot;true&quot;>&amp;#x2192;</mo></mover><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>k</mi></msub></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><msup is=&quot;true&quot;><mi is=&quot;true&quot;>W</mi><mi is=&quot;true&quot;>T</mi></msup><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>p</mi><mo is=&quot;true&quot;>&amp;#x2192;</mo></mover><mrow is=&quot;true&quot;><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>k</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>1</mn></mrow></msub></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><msup is=&quot;true&quot;><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msup is=&quot;true&quot;><mi is=&quot;true&quot;>W</mi><mi is=&quot;true&quot;>k</mi></msup><mo is=&quot;true&quot;>)</mo></mrow><mi is=&quot;true&quot;>T</mi></msup><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mi mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>p</mi><mo is=&quot;true&quot;>&amp;#x2192;</mo></mover><mn mathvariant=&quot;bold-italic&quot; is=&quot;true&quot;>0</mn></msub></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="30.7ex" height="3.69ex" viewBox="0 -1164.2 13217.8 1588.9" role="img" focusable="false" style="vertical-align: -0.986ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><g is="true" transform="translate(199,0)"><use xlink:href="#MJMATHBI-70"></use></g><use xlink:href="#MJMAIN-2192" is="true" x="95" y="545"></use></g><g is="true" transform="translate(1096,-231)"><use transform="scale(0.707)" xlink:href="#MJMATHBI-6B"></use></g></g></g><g is="true" transform="translate(1901,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2957,0)"><g is="true"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(1079,362)"><use transform="scale(0.707)" xlink:href="#MJMATHI-54"></use></g></g><g is="true" transform="translate(4635,0)"><g is="true"><g is="true"><g is="true" transform="translate(199,0)"><use xlink:href="#MJMATHBI-70"></use></g><use xlink:href="#MJMAIN-2192" is="true" x="95" y="545"></use></g><g is="true" transform="translate(1096,-231)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHBI-6B"></use></g><g is="true" transform="translate(427,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(977,0)"><use transform="scale(0.707)" xlink:href="#MJMAINB-31"></use></g></g></g></g><g is="true" transform="translate(7494,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(8550,0)"><g is="true"><use xlink:href="#MJSZ1-28" is="true"></use><g is="true" transform="translate(458,0)"><g is="true"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(1079,362)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6B"></use></g></g><use xlink:href="#MJSZ1-29" is="true" x="2007" y="-1"></use></g><g is="true" transform="translate(2465,581)"><use transform="scale(0.707)" xlink:href="#MJMATHI-54"></use></g></g><g is="true" transform="translate(11614,0)"><g is="true"><g is="true"><g is="true" transform="translate(199,0)"><use xlink:href="#MJMATHBI-70"></use></g><use xlink:href="#MJMAIN-2192" is="true" x="95" y="545"></use></g><g is="true" transform="translate(1096,-231)"><use transform="scale(0.707)" xlink:href="#MJMAINB-30"></use></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mi mathvariant="bold-italic" is="true">k</mi></msub></mrow><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mi is="true">W</mi><mi is="true">T</mi></msup><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi><mo is="true">−</mo><mn mathvariant="bold-italic" is="true">1</mn></mrow></msub></mrow><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mrow is="true"><mo is="true">(</mo><msup is="true"><mi is="true">W</mi><mi is="true">k</mi></msup><mo is="true">)</mo></mrow><mi is="true">T</mi></msup><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mn mathvariant="bold-italic" is="true">0</mn></msub></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-11"><math><mrow is="true"><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mi mathvariant="bold-italic" is="true">k</mi></msub></mrow><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mi is="true">W</mi><mi is="true">T</mi></msup><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mrow is="true"><mi mathvariant="bold-italic" is="true">k</mi><mo is="true">−</mo><mn mathvariant="bold-italic" is="true">1</mn></mrow></msub></mrow><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mrow is="true"><mo is="true">(</mo><msup is="true"><mi is="true">W</mi><mi is="true">k</mi></msup><mo is="true">)</mo></mrow><mi is="true">T</mi></msup><mrow is="true"><msub is="true"><mover accent="true" is="true"><mi mathvariant="bold-italic" is="true">p</mi><mo is="true">→</mo></mover><mn mathvariant="bold-italic" is="true">0</mn></msub></mrow></mrow></math></script></span></span></span></p><p id="p0042">According to this computation, a word is important if it co-occurs with other important words. Nodes with higher eigenvector centrality are perceived as more important. Effectively, prestige of a word measures its influence over the entire document.</p></section><section id="sec0010"><h3 id="sctt0013" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.3. PageRank</h3><p id="p0043"><span>PageRank computes prestige in the context of ‘random surfer model’ of <a href="/topics/computer-science/world-wide-web-search" title="Learn more about Web search from ScienceDirect's AI-generated Topic Pages" class="topic-link">Web search</a>. A </span><span><em><a href="/topics/computer-science/damping-factor" title="Learn more about damping factor from ScienceDirect's AI-generated Topic Pages" class="topic-link">damping factor</a></em></span>, which is the probability of the surfer making <em>random jump</em>, is used here. In case of text documents, this can be interpreted as events like the change of discourse in the document, beginning of a new chapter in a book, etc. We adopt the computation of word score (<em>WS</em>) from TextRank algorithm&nbsp;(<a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea &amp; Tarau,&nbsp;2004</a>), as given below.<span class="display"><span id="ueq0003" class="formula"><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>W</mi><mi is=&quot;true&quot;>S</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mn is=&quot;true&quot;>1</mn><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>&amp;#x2212;</mo><mi is=&quot;true&quot;>d</mi><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>+</mo><mi is=&quot;true&quot;>d</mi><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>*</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>j</mi></msub><mo is=&quot;true&quot;>&amp;#x2208;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>i</mi></msub></mrow></munder><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mfrac is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>i</mi></mrow></msub><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>k</mi></msub><mo is=&quot;true&quot;>&amp;#x2208;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>j</mi></msub></mrow></msub><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>k</mi></mrow></msub></mrow></mfrac><mi is=&quot;true&quot;>W</mi><mi is=&quot;true&quot;>S</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>j</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="52.657ex" height="5.899ex" viewBox="0 -1534 22671.8 2539.8" role="img" focusable="false" style="vertical-align: -2.336ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(1048,0)"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(1860,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(485,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1219,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(3747,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4803,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMAIN-31"></use></g><g is="true" transform="translate(1112,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(2112,0)"><use xlink:href="#MJMATHI-64"></use></g><g is="true" transform="translate(2636,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(8051,0)"><use xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(9052,0)"><use xlink:href="#MJMATHI-64"></use></g><g is="true" transform="translate(9798,0)"><use xlink:href="#MJMAIN-2A"></use></g><g is="true" transform="translate(10520,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(343,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(620,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(1092,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g></g></g><g is="true" transform="translate(13747,0)"><use xlink:href="#MJSZ3-28" is="true"></use><g is="true" transform="translate(736,0)"><g transform="translate(120,0)"><rect stroke="none" width="3694" height="60" x="0" y="220"></rect><g is="true" transform="translate(1368,617)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(506,-107)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g></g><g is="true" transform="translate(60,-441)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(747,-203)"><g is="true"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(242,-176)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g><g is="true" transform="translate(553,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(887,0)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(401,-159)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g></g></g><g is="true" transform="translate(2529,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(506,-107)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g></g></g></g></g><g is="true" transform="translate(4670,0)"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(5719,0)"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(6531,0)"><use xlink:href="#MJMAIN-28" is="true"></use><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(485,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><use xlink:href="#MJMAIN-29" is="true" x="1266" y="0"></use></g><use xlink:href="#MJSZ3-29" is="true" x="8187" y="-1"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">W</mi><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">−</mo><mi is="true">d</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">+</mo><mi is="true">d</mi><mo linebreak="goodbreak" is="true">*</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mfrac is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">j</mi></msub></mrow></msub><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub></mrow></mfrac><mi is="true">W</mi><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-12"><math><mrow is="true"><mi is="true">W</mi><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">−</mo><mi is="true">d</mi><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">+</mo><mi is="true">d</mi><mo linebreak="goodbreak" is="true">*</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mfrac is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">j</mi></msub></mrow></msub><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub></mrow></mfrac><mi is="true">W</mi><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></script></span></span></span></p><p id="p0044">Here, damping factor <em>d</em> is set to 0.85 by the algorithm, which is the probability of random jump in context of the random surfing model. <em>N<sub>i</sub></em> and <em>N<sub>j</sub></em> are the sets of adjacent nodes of node <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em>, respectively. <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea&nbsp;and Tarau&nbsp;(2004)</a> expressed that the damping factor associated with random surfer model can be interpreted as text cohesion or “knitting” of discourse together.</p></section><section id="sec0011"><h3 id="sctt0014" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.4. PositionRank</h3><p id="p0045">PositionRank is an extension of PageRank that is based on the intuition that keywords are likely to occur towards the beginning of the text rather than towards the end. PositionRank favors words occurring at the beginning of the document as keywords by using a position-biased weight for each candidate&nbsp;(<a name="bbib0014" href="#bib0014" class="workspace-trigger">Florescu &amp; Caragea,&nbsp;2017</a>). Node <em>v<sub>i</sub></em> ∈ <em>V</em> is assigned a weight based on its positional information by taking the inverse of the sum of its positions of occurrences in the text. Subsequently, PageRank computation is performed on the weighted nodes of the network to yield PositionRank scores for the candidate words. Formally, the PositionRank score of a node <em>v<sub>i</sub></em> is computed as follows.<span class="display"><span id="ueq0004" class="formula"><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><mn is=&quot;true&quot;>1</mn><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>&amp;#x2212;</mo><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo is=&quot;true&quot;>)</mo></mrow><mo is=&quot;true&quot;>.</mo><mover accent=&quot;true&quot; is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>&amp;#x2DC;</mo></mover><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>+</mo><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo is=&quot;true&quot;>.</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>j</mi></msub><mo is=&quot;true&quot;>&amp;#x2208;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>i</mi></msub></mrow></munder><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mfrac is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>i</mi></mrow></msub><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>k</mi></msub><mo is=&quot;true&quot;>&amp;#x2208;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>j</mi></msub></mrow></msub><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>k</mi></mrow></msub></mrow></mfrac><mi is=&quot;true&quot;>S</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>j</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="50.914ex" height="5.899ex" viewBox="0 -1534 21921.3 2539.8" role="img" focusable="false" style="vertical-align: -2.336ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(812,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(485,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1219,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(2698,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3755,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMAIN-31"></use></g><g is="true" transform="translate(1112,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(2112,0)"><use xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(2753,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(7064,0)"><use xlink:href="#MJMAIN-2E"></use></g><g is="true" transform="translate(7509,0)"><g is="true" transform="translate(76,0)"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><use xlink:href="#MJSZ2-2DC" is="true" x="0" y="-87"></use></g><g is="true" transform="translate(8732,0)"><use xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(9733,0)"><use xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(10373,0)"><use xlink:href="#MJMAIN-2E"></use></g><g is="true" transform="translate(10818,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(343,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(620,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(1092,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g></g></g><g is="true" transform="translate(14045,0)"><use xlink:href="#MJSZ3-28" is="true"></use><g is="true" transform="translate(736,0)"><g transform="translate(120,0)"><rect stroke="none" width="3694" height="60" x="0" y="220"></rect><g is="true" transform="translate(1368,617)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(506,-107)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g></g><g is="true" transform="translate(60,-441)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(747,-203)"><g is="true"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(242,-176)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g><g is="true" transform="translate(553,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(887,0)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(401,-159)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g></g></g><g is="true" transform="translate(2529,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(506,-107)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g></g></g></g></g><g is="true" transform="translate(4670,0)"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(5482,0)"><use xlink:href="#MJMAIN-28" is="true"></use><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(485,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><use xlink:href="#MJMAIN-29" is="true" x="1266" y="0"></use></g><use xlink:href="#MJSZ3-29" is="true" x="7138" y="-1"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">−</mo><mi is="true">α</mi><mo is="true">)</mo></mrow><mo is="true">.</mo><mover accent="true" is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">˜</mo></mover><mo linebreak="goodbreak" is="true">+</mo><mi is="true">α</mi><mo is="true">.</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mfrac is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">j</mi></msub></mrow></msub><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub></mrow></mfrac><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-13"><math><mrow is="true"><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mrow is="true"><mo is="true">(</mo><mn is="true">1</mn><mo linebreak="badbreak" is="true">−</mo><mi is="true">α</mi><mo is="true">)</mo></mrow><mo is="true">.</mo><mover accent="true" is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">˜</mo></mover><mo linebreak="goodbreak" is="true">+</mo><mi is="true">α</mi><mo is="true">.</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mfrac is="true"><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">i</mi></mrow></msub><mrow is="true"><msub is="true"><mo is="true">∑</mo><mrow is="true"><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">j</mi></msub></mrow></msub><msub is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub></mrow></mfrac><mi is="true">S</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">)</mo></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></script></span></span></span></p><p id="p0046">Here, <em>α</em> is set as 0.85 by the algorithm, <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>&amp;#x2DC;</mo></mover><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>i</mi></msub><mrow is=&quot;true&quot;><msubsup is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>1</mn></mrow><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>|</mo><mi is=&quot;true&quot;>V</mi><mo is=&quot;true&quot;>|</mo></mrow></msubsup><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>j</mi></msub></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.979ex" height="4.672ex" viewBox="0 -952.9 5157.7 2011.5" role="img" focusable="false" style="vertical-align: -2.459ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true" transform="translate(76,0)"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><use xlink:href="#MJSZ2-2DC" is="true" x="0" y="-87"></use></g><g is="true" transform="translate(1278,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2056,0)"><g transform="translate(397,0)"><rect stroke="none" width="2583" height="60" x="0" y="220"></rect><g is="true" transform="translate(991,577)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(356,-163)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(60,-654)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(747,368)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMAIN-7C"></use></g><g is="true" transform="translate(139,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-56"></use></g><g is="true" transform="translate(524,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-7C"></use></g></g><g is="true" transform="translate(747,-218)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(595,0)"><use transform="scale(0.5)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(1830,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(356,-163)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mover accent="true" is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">˜</mo></mover><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mo is="true">|</mo><mi is="true">V</mi><mo is="true">|</mo></mrow></msubsup><msub is="true"><mi is="true">p</mi><mi is="true">j</mi></msub></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-14"><math><mrow is="true"><mover accent="true" is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">˜</mo></mover><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mrow is="true"><msubsup is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mo is="true">|</mo><mi is="true">V</mi><mo is="true">|</mo></mrow></msubsup><msub is="true"><mi is="true">p</mi><mi is="true">j</mi></msub></mrow></mfrac></mrow></math></script></span> is the normalized positional weight of <em>v<sub>i</sub>, N<sub>i</sub></em> is the set of adjacent nodes of <em>v<sub>i</sub></em>, and <em>w<sub>ji</sub></em> is the weight of edge <em>e<sub>ji</sub></em>. The bias vector <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>&amp;#x2DC;</mo></mover></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.324ex" height="2.341ex" viewBox="0 -741.6 1000.5 1007.7" role="img" focusable="false" style="vertical-align: -0.618ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true" transform="translate(76,0)"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><use xlink:href="#MJSZ2-2DC" is="true" x="0" y="-87"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true" is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">˜</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-15"><math><mover accent="true" is="true"><msub is="true"><mi is="true">p</mi><mi is="true">i</mi></msub><mo is="true">˜</mo></mover></math></script></span> ensures that words occurring towards the beginning are preferred as keywords by the system.</p></section><section id="sec0012"><h3 id="sctt0015" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.5. Coreness</h3><p id="p0047"><span>Coreness is a network <a href="/topics/engineering/degeneracy" title="Learn more about degeneracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">degeneracy</a> property that decomposes network </span><em>G</em> into a set of <em>maximal connected</em> subgraphs <em>G<sub>k</sub></em> (<em>k</em> denotes the core), such that nodes in <em>G<sub>k</sub></em> have degree at least <em>k</em> within the subgraph and <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>G</mi><mi is=&quot;true&quot;>k</mi></msub><mo is=&quot;true&quot;>&amp;#x2286;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>G</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>k</mi><mo is=&quot;true&quot;>+</mo><mn is=&quot;true&quot;>1</mn></mrow></msub></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.03ex" height="2.463ex" viewBox="0 -794.4 4749 1060.6" role="img" focusable="false" style="vertical-align: -0.618ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-47"></use></g><g is="true" transform="translate(786,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6B"></use></g></g><g is="true" transform="translate(1533,0)"><use xlink:href="#MJMAIN-2286"></use></g><g is="true" transform="translate(2589,0)"><g is="true"><use xlink:href="#MJMATHI-47"></use></g><g is="true" transform="translate(786,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6B"></use></g><g is="true" transform="translate(368,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(919,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">G</mi><mi is="true">k</mi></msub><mo is="true">⊆</mo><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">k</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math></span></span><script type="math/mml" id="MathJax-Element-16"><math><mrow is="true"><msub is="true"><mi is="true">G</mi><mi is="true">k</mi></msub><mo is="true">⊆</mo><msub is="true"><mi is="true">G</mi><mrow is="true"><mi is="true">k</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></msub></mrow></math></script></span>&nbsp;(<a name="bbib0032" href="#bib0032" class="workspace-trigger">Seidman,&nbsp;1983</a>). <em>Coreness</em> of a node is the highest core to which it belongs. <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau&nbsp;and Vazirgiannis&nbsp;(2015)</a> presume that words in the main (highest) core of the network are keywords due to their dense connections. Though our findings differ where we have empirically observed that main core typically consists of fewer keyword-quality nodes that results in increasing precision and dropping recall (<a name="bbib0013" href="#bib0013" class="workspace-trigger">Duari &amp; Bhatnagar,&nbsp;2019</a>), we are convinced that keywords tend to lie in higher cores. Hence, we choose to include <em>coreness</em> as a discriminating property.</p></section><section id="sec0013"><h3 id="sctt0016" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.6. Clustering Coefficient</h3><p id="p0048">Clustering Coefficient (CC) of a node indicates edge density in its neighbourhood. It is a local property and is computed for node <em>v<sub>i</sub></em> as the ratio of actual number of edges in the sub-graph induced by <em>v<sub>i</sub></em> (excluding itself) to the total number of possible edges in that subgraph&nbsp;(<a name="bbib0037" href="#bib0037" class="workspace-trigger">Zaki&nbsp;et&nbsp;al., 2014</a>). A node <em>v<sub>i</sub></em> having high clustering coefficient implies that the neighbors of <em>v<sub>i</sub></em> are densely connected to each other, and can convey a context effectively without involving node <em>v<sub>i</sub></em><span>. For an <a href="/topics/computer-science/directed-graphs" title="Learn more about undirected graph from ScienceDirect's AI-generated Topic Pages" class="topic-link">undirected graph</a> </span><em>G</em>, clustering coefficient of node <em>v<sub>i</sub></em> ∈ <em>G</em> is computed as below.<span class="display"><span id="ueq0005" class="formula"><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi><mi is=&quot;true&quot;>C</mi><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>2</mn><mo is=&quot;true&quot;>|</mo></mrow><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>e</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>k</mi></mrow></msub><mo is=&quot;true&quot;>:</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>j</mi></msub><mo is=&quot;true&quot;>,</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>k</mi></msub><mo is=&quot;true&quot;>&amp;#x2208;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>,</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>e</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>k</mi></mrow></msub><mo is=&quot;true&quot;>&amp;#x2208;</mo><mi is=&quot;true&quot;>E</mi></mrow><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>|</mo></mrow></mrow><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>|</mo></mrow><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>i</mi></msub><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>|</mo><mo is=&quot;true&quot;>(</mo><mo is=&quot;true&quot;>|</mo></mrow><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>i</mi></msub><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>|</mo><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn><mo is=&quot;true&quot;>)</mo></mrow></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="27.071ex" height="4.426ex" viewBox="0 -1217.1 11655.4 1905.8" role="img" focusable="false" style="vertical-align: -1.6ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g><g is="true" transform="translate(760,0)"><use xlink:href="#MJMATHI-43"></use></g><g is="true" transform="translate(1687,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(485,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1219,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(3574,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4352,0)"><g transform="translate(397,0)"><rect stroke="none" width="6784" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,617)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-32"></use></g><g is="true" transform="translate(353,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-7C"></use></g></g><g is="true" transform="translate(550,0)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-65"></use></g><g is="true" transform="translate(329,-107)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g></g><g is="true" transform="translate(867,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-3A"></use></g><g is="true" transform="translate(1064,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(343,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(1684,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(1881,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(343,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g><g is="true" transform="translate(2556,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(3028,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(3840,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(4037,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-65"></use></g><g is="true" transform="translate(329,-107)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g><g is="true" transform="translate(206,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6B"></use></g></g></g><g is="true" transform="translate(4904,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(5376,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-45"></use></g></g><g is="true" transform="translate(6467,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-7C" is="true"></use></g></g><g is="true" transform="translate(1459,-441)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-7C" is="true"></use></g><g is="true" transform="translate(196,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1008,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-7C" is="true"></use><g is="true" transform="translate(196,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-28"></use></g><use transform="scale(0.707)" xlink:href="#MJMAIN-7C" is="true" x="668" y="-1"></use></g><g is="true" transform="translate(1677,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(2489,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-7C" is="true"></use><g is="true" transform="translate(196,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(747,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g><g is="true" transform="translate(1101,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-29"></use></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">C</mi><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mn is="true">2</mn><mo is="true">|</mo></mrow><mrow is="true"><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub><mo is="true">:</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub><mo is="true">∈</mo><mi is="true">E</mi></mrow><mrow is="true"><mo is="true">|</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo><mo is="true">(</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-17"><math><mrow is="true"><mi is="true">C</mi><mi is="true">C</mi><mrow is="true"><mo is="true">(</mo><msub is="true"><mi is="true">v</mi><mi is="true">i</mi></msub><mo is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><mn is="true">2</mn><mo is="true">|</mo></mrow><mrow is="true"><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub><mo is="true">:</mo><msub is="true"><mi is="true">v</mi><mi is="true">j</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">v</mi><mi is="true">k</mi></msub><mo is="true">∈</mo><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub><mo is="true">,</mo><msub is="true"><mi is="true">e</mi><mrow is="true"><mi is="true">j</mi><mi is="true">k</mi></mrow></msub><mo is="true">∈</mo><mi is="true">E</mi></mrow><mrow is="true"><mo is="true">|</mo></mrow></mrow><mrow is="true"><mrow is="true"><mo is="true">|</mo></mrow><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo><mo is="true">(</mo><mo is="true">|</mo></mrow><msub is="true"><mi is="true">N</mi><mi is="true">i</mi></msub><mrow is="true"><mo is="true">|</mo><mo is="true">−</mo><mn is="true">1</mn><mo is="true">)</mo></mrow></mrow></mfrac></mrow></math></script></span></span></span></p><p id="p0049">Here, <em>N<sub>i</sub></em> is the set of nodes adjacent to <em>v<sub>i</sub></em>. We conjecture that nodes (words) with low clustering coefficient connect diverse contexts together, and thus are likely to be important words. We elaborate the idea below.</p><div><p id="p0050">A closed triad is formed in a graph of text when three words co-occur either in the same sentence or in adjacent sentences. Semantically, the words in the triad share a context. If a word <em>w</em> shares many unrelated contexts with several words (<a name="bfig0003" href="#fig0003" class="workspace-trigger">Fig.&nbsp;3</a>(a)), then <em>w</em> attains importance because it glues several independent contexts. On the other hand, if the contexts in which <em>w</em> participates are linked as shown in <a name="bfig0003" href="#fig0003" class="workspace-trigger">Fig.&nbsp;3</a>(b) (e.g., contexts formed by vertices 1,2,3 and vertices 1,4,5 are connected via an edge between vertices 2 and 4), then the word may not be important.</p><figure class="figure text-xs" id="fig0003"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr3.jpg" height="206" alt="Fig. 3" aria-describedby="cap0003"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (283KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (283KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cap0003"><p id="sp0007"><span class="label">Fig. 3</span>. Effect of semantically related and unrelated contexts on <a href="/topics/computer-science/clustering-coefficient" title="Learn more about Clustering Coefficient from ScienceDirect's AI-generated Topic Pages" class="topic-link">Clustering Coefficient</a>.</p></span></span></figure></div><p id="p0051"><span>For <a href="/topics/computer-science/unweighted-network" title="Learn more about unweighted networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">unweighted networks</a>, the above definition of topological clustering coefficient (CC) holds. However, for weighted networks, </span><a name="bbib0001" href="#bib0001" class="workspace-trigger">Barrat&nbsp;et&nbsp;al.&nbsp;(2004)</a> define weighted clustering coefficient (WCC) that incorporates edge weights into computation. Since our networks are weighted, we empirically evaluated the effect of WCC against CC in distinguishing keywords from non-keywords. However, though WCC is apparently a better option, in our case the performance of the models degraded when using WCC in place of CC. This is because incorporating edge weights sometimes increases the clustering coefficient for the node, which negatively correlates with the importance of the node. Thus, we decided to use topological CC instead of WCC as a network property in our experiments.</p><p id="p0052">Overlapping of densities of the six node property values in <a name="bfig0002" href="#fig0002" class="workspace-trigger">Fig.&nbsp;2</a> indicate high number of false positives and likely poor performance. However, an intricate coaction of all six properties produces desirable effect of improving prediction performance, which has been established in <a name="bsec0018" href="#sec0018" class="workspace-trigger">Section&nbsp;8</a>.</p></section></section><section id="sec0014"><h2 id="sctt0017" class="u-h3 u-margin-l-top u-margin-xs-bottom">6. Inducing the model</h2><p id="p0053">The construction of training set is guided by our conjecture that the distribution of network-centric properties of the keywords are similar irrespective of the language, domain, or collection of the document. Accordingly, we combine short scientific abstracts from Hulth2003 dataset and long scientific articles from SemEval2010 collection to create the training collection. The objective is to predict keywords from documents belonging to different collections of scientific papers, news articles, and non-English texts using the same <a href="/topics/computer-science/predictive-model" title="Learn more about predictive model from ScienceDirect's AI-generated Topic Pages" class="topic-link">predictive model</a>.</p><p id="p0054">For each document in the training collection, we consult the corresponding gold-standard keywords list and assign the class label as ‘positive’ or ‘negative’ to the candidate words depending on whether they are listed as a gold-standard keyword (as unigram) or not. Corresponding feature values for the candidate keywords, as described in <a name="bsec0007" href="#sec0007" class="workspace-trigger">Section&nbsp;5</a>, are range normalized to remove the bias due to document length. The feature set along with the labels constitute the training set for our empirical evaluations.</p><p id="p0055"><span>The curated training set naturally has high imbalance of class distribution because keywords are much lesser in number than other words. Longer documents in the training set contribute more to imbalance than shorter ones. Our pragmatism of using judicious mix of long and short text paid-off, resulting into the training set exhibiting an <a href="/topics/computer-science/imbalance-ratio" title="Learn more about imbalance ratio from ScienceDirect's AI-generated Topic Pages" class="topic-link">imbalance ratio</a> of 1:5 (keywords:non keywords). We use Weka implementation of SMOTE&nbsp;(</span><a name="bbib0009" href="#bib0009" class="workspace-trigger">Chawla,&nbsp;Bowyer, Hall, &amp; Kegelmeyer, 2002</a>) to balance<a name="bfn0004" href="#fn0004" class="workspace-trigger"><sup>4</sup></a> the training set.</p><p id="p0056"><span>We use Naïve Bayes (NB) and XGBoost <a href="/topics/computer-science/classification-machine-learning" title="Learn more about classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">classifiers</a> to train the model following their success as reported in earlier works. NB has been used for predicting keywords and keyphrases in various earlier studies (</span><a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0018" href="#bib0018" class="workspace-trigger">Kim, Medelyan, Kan, Baldwin, 2010</a>, <a name="bbib0025" href="#bib0025" class="workspace-trigger">Medelyan, Frank, Witten, 2009</a>, <a name="bbib0028" href="#bib0028" class="workspace-trigger">Nguyen, Kan, 2007</a><span><span><span>). We decided to use <a href="/topics/engineering/naive-bayes-classifier" title="Learn more about Naïve Bayes classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">Naïve Bayes classifier</a> because of its simplicity, </span><a href="/topics/computer-science/interpretability" title="Learn more about interpretability from ScienceDirect's AI-generated Topic Pages" class="topic-link">interpretability</a>, and fast execution time. We additionally use a gradient boosted decision tree implemented in the XGBoost package (XGBoost classifier) following its success in predicting keyphrases as reported by Sterckx et&nbsp;al., who note that XGBoost classifier outperforms NB and </span><a href="/topics/computer-science/linear-classifier" title="Learn more about linear classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">linear classifiers</a> in their study (</span><a name="bbib0033" href="#bib0033" class="workspace-trigger">Sterckx&nbsp;et&nbsp;al., 2016</a>).</p><p id="p0057"><span>We attempt to reduce the high bias factor of NB classifier by balancing the training set using artificially generated data to over-sample the minority class. We additionally experiment with Bagging and Boosting ensembles of NB classifier to inspect for improvement in performance due to <a href="/topics/computer-science/ensemble-learning" title="Learn more about ensemble learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">ensemble learning</a>. Use of classical classifiers shows a marked improvement in performance in our experiments. We envisage that the performance will be further boosted by use of </span><a href="/topics/computer-science/deep-learning-method" title="Learn more about deep learning methods from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning methods</a> if sufficiently large dataset and efficient computation power is available.</p></section><section id="sec0015"><h2 id="sctt0018" class="u-h3 u-margin-l-top u-margin-xs-bottom">7. Experimental setup and objectives</h2><p id="p0058">The proposed framework is implemented using R (version 3.3.1) and relevant packages<a name="bfn0005" href="#fn0005" class="workspace-trigger"><sup>5</sup></a> (<span class="monospace">igraph, tm, RWeka, caret and pROC</span><span>). We use six publicly available collections that have been used in similar studies. Each document in these collections is accompanied by an associated gold-standard keywords list, which is used as ground truth for testing the <a href="/topics/computer-science/classification-machine-learning" title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">classifier</a> performance. In the following sections, we briefly describe the datasets used in this study (</span><a name="bsec0016" href="#sec0016" class="workspace-trigger">Section&nbsp;7.1</a>) and the objective and design of our experiments (<a name="bsec0017" href="#sec0017" class="workspace-trigger">Section&nbsp;7.2</a>).</p><section id="sec0016"><h3 id="sctt0019" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.1. Datasets</h3><p id="p0059">We use six publicly available datasets in our experiments. The datasets are described in detail below.<dl class="list"><dt class="list-label">1.</dt><dd class="list-description"><p id="p0060">Hulth2003 (<a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth,&nbsp;2003</a>) consists of 2000 scientific abstracts from <em>Inspec</em> dataset, which are further divided into training (1000 articles), test (500 articles), and validation (500 articles). Each article in Hulth2003 dataset is accompanied with two gold-standard lists - one is controlled and uses a thesaurus, and the other is uncontrolled. We combine the training and test collections from Hulth2003 (a total of 1500 articles) to form a part of the training set for our experiments, and consult the uncontrolled keywords list as a gold-standard.</p></dd><dt class="list-label">2.</dt><dd class="list-description"><p id="p0061">WWW and KDD (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>) are two collections of abstracts from computer science articles published in the two well known conferences by the respective names. For both these collections, we consider only those articles which contains at least two sentences, and are accompanied by at least one gold-standard keyword.</p></dd><dt class="list-label">3.</dt><dd class="list-description"><p id="p0062"><span><span>Marujo2012 is a collection of 500 online news articles, which is grouped under training collection (450 articles) and test collection (50 articles). Each article is accompanied by a list of keywords assigned by human <a href="/topics/engineering/annotator" title="Learn more about annotators from ScienceDirect's AI-generated Topic Pages" class="topic-link">annotators</a> through a HIT in Amazon </span><a href="/topics/computer-science/mechanical-turk" title="Learn more about Mechanical Turk from ScienceDirect's AI-generated Topic Pages" class="topic-link">Mechanical Turk</a> (</span><a name="bbib0024" href="#bib0024" class="workspace-trigger">Marujo,&nbsp;Gershman, Carbonell, Frederking, &amp; Neto, 2012</a>). From this dataset, we use the articles under training collection (a set of 450 articles) as an <em>unseen test set</em> for our experiments.</p></dd><dt class="list-label">4.</dt><dd class="list-description"><p id="p0063">Krapivin2009 (<a name="bbib0019" href="#bib0019" class="workspace-trigger">Krapivin,&nbsp;Autaeu, &amp; Marchese, 2009</a>) and SemEval2010 (<a name="bbib0018" href="#bib0018" class="workspace-trigger">Kim&nbsp;et&nbsp;al., 2010</a>) are two datasets which contains full scientific articles from ACM. The Krapivin2009 dataset consists of 2304 articles and associated keywords lists. SemEval2010 consists of 284 articles, out of which 144 are grouped as training, 100 as test, and 40 as validation sets. Each document in SemEval2010 dataset is accompanied by three sets of keyword list - author-assigned, reader-assigned, and author-and-reader-assigned. We use the combined collection of 244 articles (training and test) as a part of the training set for our experiments, and we consult the author-and-reader-assigned keywords list as gold-standard.</p></dd></dl></p><div><p id="p0064"><a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a> presents the datasets along with relevant statistics about the data. As mentioned in <a name="bsec0014" href="#sec0014" class="workspace-trigger">Section&nbsp;6</a>, we create the training set for our experiments by combining the articles from Hulth2003 (1500 articles) and SemEval2010 (244 articles) datasets.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0001"><span class="captions"><span id="cap0006"><p id="sp0010"><span class="label">Table 1</span>. Overview of the experimental data collections. |<em>D</em>|: Number of docs, <em>L<sub>avg</sub></em>: average doc length, <em>N<sub>avg</sub></em>: average gold-standard keywords per doc, <em>K<sub>avg</sub></em>: average gold-standard keyphrases per doc, <em>KP<sub>avg</sub></em>: average percentage of keywords present in the text, <em>ngram</em>%: average %age of 1/2/3/3+ -gram distribution.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Collection</th><th scope="col" class="align-center valign-top">|D|</th><th scope="col" class="align-center valign-top">L<sub>avg</sub></th><th scope="col" class="align-center valign-top">N<sub>avg</sub></th><th scope="col" class="align-center valign-top">K<sub>avg</sub></th><th scope="col" class="align-center valign-top">KP<sub>avg</sub></th><th scope="col" class="align-center valign-top">ngram%</th></tr></thead><tbody><tr><td class="align-left valign-top">Hulth2003 (<a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth,&nbsp;2003</a>)</td><td class="align-left valign-top">1500</td><td class="align-left valign-top">129</td><td class="align-left valign-top">23</td><td class="align-left valign-top">10</td><td class="align-left valign-top">90.07</td><td class="align-left valign-top">16/52/24/8</td></tr><tr><td class="align-left valign-top">WWW (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>)</td><td class="align-left valign-top">1248</td><td class="align-left valign-top">174</td><td class="align-left valign-top">9</td><td class="align-left valign-top">5</td><td class="align-left valign-top">64.97</td><td class="align-left valign-top">31/51/16/1</td></tr><tr><td class="align-left valign-top">KDD (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>)</td><td class="align-left valign-top">704</td><td class="align-left valign-top">204</td><td class="align-left valign-top">8</td><td class="align-left valign-top">4</td><td class="align-left valign-top">68.12</td><td class="align-left valign-top">25/58/16/1</td></tr><tr><td class="align-left valign-top">Marujo2012 (<a name="bbib0024" href="#bib0024" class="workspace-trigger">Marujo&nbsp;et&nbsp;al., 2012</a>)</td><td class="align-left valign-top">450</td><td class="align-left valign-top">427</td><td class="align-left valign-top">69</td><td class="align-left valign-top">48</td><td class="align-left valign-top">99.31</td><td class="align-left valign-top">75/17/5/2</td></tr><tr><td class="align-left valign-top">Krapivin2009 (<a name="bbib0019" href="#bib0019" class="workspace-trigger">Krapivin&nbsp;et&nbsp;al., 2009</a>)</td><td class="align-left valign-top">2304</td><td class="align-left valign-top">7961</td><td class="align-left valign-top">11</td><td class="align-left valign-top">5</td><td class="align-left valign-top">96.91</td><td class="align-left valign-top">19/63/16/2</td></tr><tr><td class="align-left valign-top">SemEval2010 (<a name="bbib0018" href="#bib0018" class="workspace-trigger">Kim&nbsp;et&nbsp;al., 2010</a>)</td><td class="align-left valign-top">244</td><td class="align-left valign-top">8085</td><td class="align-left valign-top">34</td><td class="align-left valign-top">16</td><td class="align-left valign-top">95.89</td><td class="align-left valign-top">21/55/20/4</td></tr></tbody></table></div></div></div></section><section id="sec0017"><h3 id="sctt0020" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.2. Objectives and experimental design</h3><p id="p0065">We perform experimental evaluations to answer the following research questions.<dl class="list"><dt class="list-label">1.</dt><dd class="list-description"><p id="p0066"><em>Which model performs best for automatic keyword extraction task?</em></p><p id="p0067"><span>We perform 10-fold cross-validation on the training set using XGBoost, Naïve Bayes, and Bagging and <a href="/topics/engineering/adaboost" title="Learn more about Adaboost from ScienceDirect's AI-generated Topic Pages" class="topic-link">Adaboost</a> ensembles of Naïve Bayes. To reduce the bias, we balance the training set using Weka implementation of SMOTE filter with percentage parameter set to 200%. Details of experiment and results are discussed in </span><a name="bsec0019" href="#sec0019" class="workspace-trigger">Section&nbsp;8.1</a>.</p></dd><dt class="list-label">2.</dt><dd class="list-description"><p id="p0068"><em>How well do the graph-theoretic properties discriminate between the keywords and non-keywords over cross-collection and cross-domain datasets?</em></p><p id="p0069">We use the best model trained in experiment 1 for all subsequent experiments. We perform cross-collection and cross-domain analysis of the trained model using three scientific datasets and one news corpus. Experimental results are discussed in <a name="bsec0020" href="#sec0020" class="workspace-trigger">Section&nbsp;8.2</a>.</p></dd><dt class="list-label">3.</dt><dd class="list-description"><p id="p0070"><em>How does the quality of extracted keyphrases compare with those extracted by state-of-the-art supervised and unsupervised keyphrase extraction methods?</em></p><p id="p0071"><span>We generate keyphrases from predicted keywords as a <a href="/topics/computer-science/postprocessing-step" title="Learn more about post processing step from ScienceDirect's AI-generated Topic Pages" class="topic-link">post processing step</a>, and compare the quality with those extracted by state-of-the-art supervised and unsupervised keyphrase extraction methods. Comparative evaluation of the methods are presented for each dataset in </span><a name="bsec0023" href="#sec0023" class="workspace-trigger">Section&nbsp;8.3</a>.</p></dd><dt class="list-label">4.</dt><dd class="list-description"><p id="p0072"><em>How well does the model perform on cross-language documents?</em></p><p id="p0073">To substantiate our claim of the model being language-independent, we use the model trained in experiment 1 to extract keywords from documents written in Indian languages. <a name="bsec0030" href="#sec0030" class="workspace-trigger">Section&nbsp;8.4</a> presents a detailed discussion on this experiment.</p></dd></dl></p><div><p id="p0074"><span><em><a href="/topics/computer-science/evaluation-metric" title="Learn more about Evaluation Metrics from ScienceDirect's AI-generated Topic Pages" class="topic-link">Evaluation Metrics</a></em><em>.</em></span> We use precision, recall, and F1-score as performance evaluation metrics for all experiments pertaining to the above research questions. All three metrics are widely used in literature (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea, Bulgarov, Godea, Gollapalli, 2014</a>, <a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth, 2003</a>, <a name="bbib0025" href="#bib0025" class="workspace-trigger">Medelyan, Frank, Witten, 2009</a>, <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>, <a name="bbib0033" href="#bib0033" class="workspace-trigger">Sterckx, Demeester, Develder, Caragea, 2016</a>, <a name="bbib0039" href="#bib0039" class="workspace-trigger">Zhang, Chang, Liu, Gollapalli, Li, Xiao, 2017</a>). Except for 10-fold cross-validation results in <a name="btbl0002" href="#tbl0002" class="workspace-trigger">Table&nbsp;2</a>, all results presented in subsequent sections are macro-averaged at the dataset level.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0002"><span class="captions"><span id="cap0007"><p id="sp0011"><span class="label">Table 2</span>. <span>Cross-validated <a href="/topics/computer-science/classification-machine-learning" title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">classifier</a> performance. XGB: model trained using XGBoost classifier, NB: model trained using </span><a href="/topics/engineering/naive-bayes-classifier" title="Learn more about NB classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">NB classifier</a>.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Models</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB</td><td class="align-left valign-top"><strong>75.39</strong></td><td class="align-left valign-top"><strong>79.93</strong></td><td class="align-left valign-top"><strong>77.59</strong></td></tr><tr><td class="align-left valign-top">NB</td><td class="align-left valign-top">72.4</td><td class="align-left valign-top">49.71</td><td class="align-left valign-top">58.95</td></tr><tr><td class="align-left valign-top">NB+Bagging (NB-B)</td><td class="align-left valign-top"><em>72.41</em></td><td class="align-left valign-top">49.71</td><td class="align-left valign-top">58.95</td></tr><tr><td class="align-left valign-top">NB+Adaboost (NB-A)</td><td class="align-left valign-top">72.20</td><td class="align-left valign-top"><em>53.42</em></td><td class="align-left valign-top"><em>61.41</em></td></tr></tbody></table></div></div></div></section></section><section id="sec0018"><h2 id="sctt0021" class="u-h3 u-margin-l-top u-margin-xs-bottom">8. Results and discussion</h2><p id="p0075">In this section, we present the results for our experiments as highlighted in <a name="bsec0017" href="#sec0017" class="workspace-trigger">Section&nbsp;7.2</a>. Each subsection is devoted to one task, and we support our claim with empirical evidences.</p><section id="sec0019"><h3 id="sctt0022" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.1. Establishing the best performing model</h3><p id="p0076">We trained four models on the SMOTE balanced training set each, using XGBoost (<a name="bbib0010" href="#bib0010" class="workspace-trigger">Chen &amp; Guestrin,&nbsp;2016</a><span>), Naïve Bayes (NB), and Bagging and <a href="/topics/engineering/adaboost" title="Learn more about Adaboost from ScienceDirect's AI-generated Topic Pages" class="topic-link">Adaboost</a> ensembles of NB. We present 10-fold cross validation results in </span><a name="btbl0002" href="#tbl0002" class="workspace-trigger">Table&nbsp;2</a>. Bold values represent best performance across all models in terms of the ‘positive’ class,<a name="bfn0006" href="#fn0006" class="workspace-trigger"><sup>6</sup></a> and values in italics represent second-best results for the same. Among all models, XGBoost trained on the balanced training set turns out to be the best model and Adaboost ensemble of NB is the second best.</p><div><p id="p0077">Next, we test the performance of trained models on test sets from Hulth2003 and SemEval2010 collections. <a name="btbl0003" href="#tbl0003" class="workspace-trigger">Table&nbsp;3</a><span> shows macro-averaged results on the test sets. Bold values indicate best performance for corresponding test sets and values in italics indicate second-best results. We observe that XGB <a href="/topics/computer-science/classification-machine-learning" title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">classifier</a> performs best in terms of recall and F1-score for both test sets, whereas best performance in terms of precision is achieved by NB and NB-B classifiers. NB-A performs second-best in term of recall and F1-score, following XGB. Since the performance gap in precision between NB, NB-B, and NB-A are insignificant, we decided to retain NB-A model along with XGB for all further experiments.</span></p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0003"><span class="captions"><span id="cap0008"><p id="sp0012"><span class="label">Table 3</span>. Macro-averaged results of model performances on test sets. Models are trained on the SMOTE balanced training set.</p></span></span><div class="groups"><table><thead class="valign-top"><tr><th scope="col" class="align-left rowsep-1" rowspan="2">Models</th><th scope="col" class="align-center rowsep-1" colspan="3">Hulth2003 Test</th><th scope="col" class="align-center rowsep-1" colspan="3">Semeval2010</th></tr><tr class="rowsep-1"><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB</td><td class="align-left valign-top">49.8</td><td class="align-left valign-top"><strong>83.5</strong></td><td class="align-left valign-top"><strong>60.7</strong></td><td class="align-left valign-top"><em>46.2</em></td><td class="align-left valign-top"><strong>49</strong></td><td class="align-left valign-top"><strong>46.4</strong></td></tr><tr><td class="align-left valign-top">NB</td><td class="align-left valign-top"><strong>52.8</strong></td><td class="align-left valign-top">60.6</td><td class="align-left valign-top">50.1</td><td class="align-left valign-top"><strong>46.4</strong></td><td class="align-left valign-top">36.5</td><td class="align-left valign-top">39.5</td></tr><tr><td class="align-left valign-top">NB-B</td><td class="align-left valign-top"><strong>52.8</strong></td><td class="align-left valign-top">60.5</td><td class="align-left valign-top">50.1</td><td class="align-left valign-top"><strong>46.4</strong></td><td class="align-left valign-top">36.5</td><td class="align-left valign-top">39.5</td></tr><tr><td class="align-left valign-top">NB-A</td><td class="align-left valign-top"><em>52.4</em></td><td class="align-left valign-top"><em>63.5</em></td><td class="align-left valign-top"><em>51.8</em></td><td class="align-left valign-top">45.2</td><td class="align-left valign-top"><em>39.2</em></td><td class="align-left valign-top"><em>40.6</em></td></tr></tbody></table></div></div></div></section><section id="sec0020"><h3 id="sctt0023" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.2. Establishing domain and collection independence</h3><p id="p0078">With an aim to validate the claim of domain- and collection-independence, we evaluate XGB and NB-A on three unseen scientific datasets, i.e. Krapivin2009, KDD, and WWW, and one news corpus, i.e. Marujo2012. Recall that both models are trained on combined datasets from Hulth2003 and Semeval2010. <a name="bsec0021" href="#sec0021" class="workspace-trigger">Sections&nbsp;8.2.1</a> and <a name="bsec0022" href="#sec0022" class="workspace-trigger">8.2.2</a> present our results for establishing collection- and domain-independence, respectively.</p><section id="sec0021"><h4 id="sctt0024" class="u-margin-m-top u-margin-xs-bottom">8.2.1. Result on cross-collection datasets</h4><div><p id="p0079"><a name="btbl0004" href="#tbl0004" class="workspace-trigger">Table&nbsp;4</a> shows macro-averaged results for the models on the three cross-collection scientific datasets. We observe that the models are able to recall the keywords reasonably well from unseen documents across corpora. To the best of our knowledge, no earlier work on supervised KE has performed cross-collection investigation for keyword extraction. Hence we are unable to compare the performance.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0004"><span class="captions"><span id="cap0009"><p id="sp0013"><span class="label">Table 4</span>. Macro-averaged results for XGB and NB-A on unseen cross-collection datasets.</p></span></span><div class="groups"><table><thead><tr><th scope="col" class="align-left rowsep-1" rowspan="2">Test Sets</th><th scope="col" class="align-center rowsep-1" colspan="3">XGB</th><th scope="col" class="align-center rowsep-1" colspan="3">NB-A</th></tr><tr class="rowsep-1"><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">Krapivin2009</td><td class="align-left valign-top">21.6</td><td class="align-left valign-top"><strong>66.5</strong></td><td class="align-left valign-top">30.9</td><td class="align-left valign-top"><strong>26.2</strong></td><td class="align-left valign-top">61.7</td><td class="align-left valign-top"><strong>34.9</strong></td></tr><tr><td class="align-left valign-top">WWW</td><td class="align-left valign-top">14</td><td class="align-left valign-top"><strong>81.8</strong></td><td class="align-left valign-top">23.1</td><td class="align-left valign-top"><strong>24</strong></td><td class="align-left valign-top">66.3</td><td class="align-left valign-top"><strong>33.1</strong></td></tr><tr><td class="align-left valign-top">KDD</td><td class="align-left valign-top">13.6</td><td class="align-left valign-top"><strong>78.1</strong></td><td class="align-left valign-top">22.3</td><td class="align-left valign-top"><strong>24.3</strong></td><td class="align-left valign-top">70.6</td><td class="align-left valign-top"><strong>33.8</strong></td></tr></tbody></table></div></div></div><p id="p0080">Low precision for these datasets is due to the relatively less number of gold-standard keywords assigned per document (See column <em>N<sub>avg</sub></em> in <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>). The models recall most of these keywords along with some false positives, which drops precision. NB-A outperforms XGB for these three datasets in terms of precision and F1-score, however with lower values for recall.</p></section><section id="sec0022"><h4 id="sctt0025" class="u-margin-m-top u-margin-xs-bottom">8.2.2. Result on cross-domain dataset</h4><div><p id="p0081">We perform experiments to establish domain-independence of the trained models by evaluating their performance on an unseen, cross-domain dataset of news articles (Marujo2012). We present our empirical observations in <a name="btbl0005" href="#tbl0005" class="workspace-trigger">Table&nbsp;5</a>. It is evident from the results that the models are able to perform sufficiently on the cross-domain dataset, which establishes that the models are indeed applicable on documents from any domain. Again, we can’t compare with any baseline due to reason stated in <a name="bsec0021" href="#sec0021" class="workspace-trigger">Section&nbsp;8.2.1</a>.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0005"><span class="captions"><span id="cap0010"><p id="sp0014"><span class="label">Table 5</span>. Macro-averaged results for XGB and NB-A on unseen cross-domain datasets.</p></span></span><div class="groups"><table><thead><tr><th scope="col" class="align-left rowsep-1" rowspan="2">Test Sets</th><th scope="col" class="align-center rowsep-1" colspan="3">XGB</th><th scope="col" class="align-center rowsep-1" colspan="3">NB-A</th></tr><tr class="rowsep-1"><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">Marujo2012</td><td class="align-left valign-top">58.3</td><td class="align-left valign-top"><strong>42</strong></td><td class="align-left valign-top"><strong>45.2</strong></td><td class="align-left valign-top"><strong>67.4</strong></td><td class="align-left valign-top">29.8</td><td class="align-left valign-top">37</td></tr></tbody></table></div></div></div><p id="p0082">Interestingly, for the cross-domain dataset (i.e., Marujo2012), the models are able to extract keywords with high precision, albeit with a drop in recall. This is because of the relatively higher number of keywords assigned per document (<span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>a</mi><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>g</mi></mrow></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>69</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.978ex" height="2.581ex" viewBox="0 -740.1 4296 1111.1" role="img" focusable="false" style="vertical-align: -0.862ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(803,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-61"></use></g><g is="true" transform="translate(374,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(717,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-67"></use></g></g></g><g is="true" transform="translate(2238,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3295,0)"><use xlink:href="#MJMAIN-36"></use><use xlink:href="#MJMAIN-39" x="500" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">N</mi><mrow is="true"><mi is="true">a</mi><mi is="true">v</mi><mi is="true">g</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">69</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-18"><math><mrow is="true"><msub is="true"><mi is="true">N</mi><mrow is="true"><mi is="true">a</mi><mi is="true">v</mi><mi is="true">g</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">69</mn></mrow></math></script></span> in <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>) for this dataset. The models tend to extract fewer but correct keywords, thus increasing precision and lowering recall in this case. XGB outperforms NB-A for this dataset in terms of recall and F1-score, whereas NB-A reports better precision.</p></section></section><section id="sec0023"><h3 id="sctt0026" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.3. Comparison with keyphrase extraction algorithms</h3><p id="p0083">State-of-the-art supervised KE methods are <em>phrase</em>-based extractors, whereas the unsupervised graph-based methods are <em>word</em>-based extractors. Several earlier works suggest that keyphrase extraction should be treated as an extension of keyword extraction, and not as a separate task (<a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea, Tarau, 2004</a>, <a name="bbib0030" href="#bib0030" class="workspace-trigger">Papagiannopoulou, Tsoumakas, 2018</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>). Following this viewpoint, we generate candidate keyphrases from the text as a post-processing step considering only those words which are predicted as keywords by our model.</p><p id="p0084"><span>We pre-process the text to remove stopwords, and split at <a href="/topics/computer-science/punctuation-mark" title="Learn more about punctuation marks from ScienceDirect's AI-generated Topic Pages" class="topic-link">punctuation marks</a> to get phrases. All unique phrases that are not sub-strings of other phrases are extracted as keyphrases. We apply stemming</span><a name="bfn0007" href="#fn0007" class="workspace-trigger"><sup>7</sup></a> using Porter stemmer<a name="bfn0008" href="#fn0008" class="workspace-trigger"><sup>8</sup></a> to both the gold-standard keyphrases and the extracted keyphrases to improve performance of the keyphrase extractor.</p><p id="p0085">For all datasets except Marujo2012, we extract top-5, -10, and -15 keyphrases based on our observation in column <em>K<sub>avg</sub></em> in <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>. For Marujo2012, we extract top-5 to top-30 keyphrases (in increments of 5) to account for the higher number of average keywords assigned per document.</p><p id="p0086">We compare the performance of our models with the best in literature for each dataset. We report our observations for each dataset separately, because (i) results of all methods are not easily reproducible as their implementations are not publicly available, (ii) there is a diversity in choice of datasets for which the authors base their claims, and (iii) all state-of-the-art methods are not applicable across domain and corpora.</p><p id="p0087">We present our results in subsequent sections (<a name="bsec0024" href="#sec0024" class="workspace-trigger">8.3.1</a>–<a name="bsec0028" href="#sec0028" class="workspace-trigger">8.3.5</a>), comparing best performance of our models with select state-of-the-art methods evaluated on the datasets that we are using. We briefly explain these methods in subsequent sections and present comparative evaluation in the form of Tables. We also test the statistical significance of the improved performance of our algorithms over the baselines for each dataset (<a name="bsec0029" href="#sec0029" class="workspace-trigger">Section&nbsp;8.3.6</a>).</p><section id="sec0024"><h4 id="sctt0027" class="u-margin-m-top u-margin-xs-bottom">8.3.1. Result on Hulth2003 Test dataset</h4><p id="p0088">In this section, we evaluate the performance of our KE models on Hulth2003 dataset with the works of <a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth&nbsp;(2003)</a> and <a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea&nbsp;and Tarau&nbsp;(2004)</a>. Hulth2003 dataset was curated by <a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth&nbsp;(2003)</a><span> for her study of effect of <a href="/topics/computer-science/linguistic-property" title="Learn more about linguistic properties from ScienceDirect's AI-generated Topic Pages" class="topic-link">linguistic properties</a> in improving performance of keyword extractors. Later, this dataset has been mostly used by unsupervised keyword extraction methods (</span><a name="bbib0013" href="#bib0013" class="workspace-trigger">Duari, Bhatnagar, 2019</a>, <a name="bbib0031" href="#bib0031" class="workspace-trigger">Rousseau, Vazirgiannis, 2015</a>).</p><p id="p0089"><span>Hulth’s work is supervised <a href="/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning</a> based, which uses linguistic information to improve performance. The method explores three term selection strategies - </span><em>n</em>-gram, noun-phrase (NP) chunk, and POS tag sequence, and evaluates the model performance on feature sets with and without POS tag information. Best result is obtained on POS tag based feature sets in comparison to their counterparts, and best F1-score is obtained with <em>n</em>-gram approach with POS tags as features.</p><p id="p0090"><a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea&nbsp;and Tarau&nbsp;(2004)</a><span> proposed an unsupervised approach, called TextRank, to extract keywords. The method is based on <a href="/topics/computer-science/graph-representation" title="Learn more about graph representation from ScienceDirect's AI-generated Topic Pages" class="topic-link">graph representation</a> of text, where nouns and adjectives constitute the vertices set, and edges are formed between two vertices if they co-occur within a window of size </span><em>w</em><span>. Edges are undirected, and are weighted by the co-occurrence frequency of the <a href="/topics/computer-science/adjacent-vertex" title="Learn more about adjacent vertices from ScienceDirect's AI-generated Topic Pages" class="topic-link">adjacent vertices</a> (words). PageRank (</span><a name="bbib0006" href="#bib0006" class="workspace-trigger">Brin &amp; Page,&nbsp;1998</a>) computation is performed on the graph representation of text to rank the vertices in order of their keyword-ness, with high PageRank score associated with being more likely to be a keyword. The system then selects top one-third candidates as keywords.</p><div><p id="p0091">We report our results in <a name="btbl0006" href="#tbl0006" class="workspace-trigger">Table&nbsp;6</a><span>. It is clearly evident from the table that both XGB and NB-A outperform the <a href="/topics/computer-science/baseline-method" title="Learn more about baseline methods from ScienceDirect's AI-generated Topic Pages" class="topic-link">baseline methods</a> with large margin, with XGB leading in terms of precision, recall, and F1-score. Best result for both these models is obtained when we extract top-10 keyphrases, and XGB dominates NB-A on this dataset. It is noteworthy that the number of extracted keyphrases, i.e., 10 for Hulth2003 dataset, corresponds to the average number of keyphrases for the dataset as presented in Column </span><em>K<sub>avg</sub></em> of <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0006"><span class="captions"><span id="cap0011"><p id="sp0015"><span class="label">Table 6</span>. <span>Performance evaluation for Keyphrase Extraction on Hulth2003 <a href="/topics/engineering/test-dataset" title="Learn more about Test dataset from ScienceDirect's AI-generated Topic Pages" class="topic-link">Test dataset</a>. @</span><em>k</em>: evaluation results for top-<em>k</em> keyphrases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Model</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB@10</td><td class="align-left valign-top"><strong>52.5</strong></td><td class="align-left valign-top"><strong>65.1</strong></td><td class="align-left valign-top"><strong>54.7</strong></td></tr><tr><td class="align-left valign-top">NB-A@10</td><td class="align-left valign-top">49.3</td><td class="align-left valign-top">60.6</td><td class="align-left valign-top">51.1</td></tr><tr><td class="align-left valign-top"><em>n</em>-gram w. tag (<a name="bbib0017" href="#bib0017" class="workspace-trigger">Hulth,&nbsp;2003</a>)</td><td class="align-left valign-top">25.2</td><td class="align-left valign-top">51.7</td><td class="align-left valign-top">33.9</td></tr><tr><td class="align-left valign-top">TextRank (<a name="bbib0026" href="#bib0026" class="workspace-trigger">Mihalcea &amp; Tarau,&nbsp;2004</a>)</td><td class="align-left valign-top">31.2</td><td class="align-left valign-top">43.1</td><td class="align-left valign-top">36.2</td></tr></tbody></table></div></div></div></section><section id="sec0025"><h4 id="sctt0028" class="u-margin-m-top u-margin-xs-bottom">8.3.2. Result on KDD and WWW dataset</h4><p id="p0092">KDD and WWW datasets were curated by <a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al.&nbsp;(2014)</a> to study the effectiveness of citation information in improving the keyword extraction task. Since the study by <a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al.&nbsp;(2014)</a> uses citation information, the method is inefficacious for generic documents outside academic or scientific literature that do not have citation information. We evaluate the performance of XGB and NB-A models on KDD and WWW datasets, and compare them with two supervised baselines - CeKE (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>) and MIKE (<a name="bbib0039" href="#bib0039" class="workspace-trigger">Zhang&nbsp;et&nbsp;al., 2017</a>).</p><p id="p0093"><span>As mentioned above, CeKE enhances the feature set by using citation information along with statistical (tf-idf, position of occurrence, etc.) and linguistic (part-of-speech tags) information. The approach uses <a href="/topics/engineering/naive-bayes-classifier" title="Learn more about Naïve Bayes classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">Naïve Bayes classifier</a> to build a </span><a href="/topics/computer-science/predictive-model" title="Learn more about predictive model from ScienceDirect's AI-generated Topic Pages" class="topic-link">predictive model</a> to identify keywords. On the other hand, MIKE uses multidimensional information (e.g., topical information) to enhance the feature set. It uses gradient-descent algorithm to build the predictive model.</p><div><p id="p0094"><a name="btbl0007" href="#tbl0007" class="workspace-trigger">Table&nbsp;7</a> shows that XGB and NB-A outperform the two baselines with large margins in terms of precision, recall and F1-score. Performance of both XGB and NB-A models is comparable for the two datasets, with no (statistically) significant difference in performance of the models. Specifically, NB-A performs best for KDD dataset when we extract top-5 keyphrases, and XGB performs best on WWW dataset for the same number of keyphrases. The number of extracted keyphrases for both these models (i.e., 5 in this case) corresponds to the average number of keyphrases for both these datasets (column <em>K<sub>avg</sub></em> in <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>).</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0007"><span class="captions"><span id="cap0012"><p id="sp0016"><span class="label">Table 7</span>. Performance evaluation for Keyphrase Extraction on KDD and WWW datasets. @<em>k</em>: evaluation results for top-<em>k</em> keyphrases.</p></span></span><div class="groups"><table><thead class="valign-top"><tr><th scope="col" class="align-left rowsep-1" rowspan="2">Model</th><th scope="col" class="align-center rowsep-1" colspan="3">KDD</th><th scope="col" class="align-center rowsep-1" colspan="3">WWW</th></tr><tr class="rowsep-1"><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB@5</td><td class="align-left valign-top">26.9</td><td class="align-left valign-top">49.7</td><td class="align-left valign-top">33.3</td><td class="align-left valign-top"><strong>30.3</strong></td><td class="align-left valign-top"><strong>52.3</strong></td><td class="align-left valign-top"><strong>36.6</strong></td></tr><tr><td class="align-left valign-top">NB-A@5</td><td class="align-left valign-top"><strong>27.5</strong></td><td class="align-left valign-top"><strong>50.9</strong></td><td class="align-left valign-top"><strong>34.1</strong></td><td class="align-left valign-top">30.3</td><td class="align-left valign-top">52</td><td class="align-left valign-top">36.5</td></tr><tr><td class="align-left valign-top">MIKE@5 (<a name="bbib0039" href="#bib0039" class="workspace-trigger">Zhang&nbsp;et&nbsp;al., 2017</a>)</td><td class="align-left valign-top">14.01</td><td class="align-left valign-top">17.33</td><td class="align-left valign-top">15.49</td><td class="align-left valign-top">14.8</td><td class="align-left valign-top">15.05</td><td class="align-left valign-top">14.92</td></tr><tr><td class="align-left valign-top">CeKE* (<a name="bbib0008" href="#bib0008" class="workspace-trigger">Caragea&nbsp;et&nbsp;al., 2014</a>)</td><td class="align-left valign-top">21.3</td><td class="align-left valign-top">41.3</td><td class="align-left valign-top">28.0</td><td class="align-left valign-top">22.7</td><td class="align-left valign-top">38.6</td><td class="align-left valign-top">28.4</td></tr></tbody></table></div><p class="legend"><p id="sp0017">* Results are averaged at document-level for 10-fold cross validation.</p></p></div></div></section><section id="sec0026"><h4 id="sctt0029" class="u-margin-m-top u-margin-xs-bottom">8.3.3. Result on Krapivin2009 dataset</h4><p id="p0095">We evaluate the performance of XGB and NB-A models on Krapivin2009 dataset, and compare with one unsupervised baseline. The unsupervised baseline is a recent work by <a name="bbib0030" href="#bib0030" class="workspace-trigger">Papagiannopoulou&nbsp;and Tsoumakas&nbsp;(2018)</a><span>, which uses GloVe to encode local <a href="/topics/computer-science/word-embeddings" title="Learn more about word embeddings from ScienceDirect's AI-generated Topic Pages" class="topic-link">word embeddings</a> for the terms in title and abstract of a scientific publication. A mean </span><em>reference</em><span> vector is computed from the vectors trained from the full-text, and keyphrases are extracted by ranking all terms on the basis of their <a href="/topics/computer-science/cosine-similarity" title="Learn more about cosine similarity from ScienceDirect's AI-generated Topic Pages" class="topic-link">cosine similarity</a> to the reference vector. Reference vector represents the semantics of the complete document, and words closer to it are considered keyphrases. RVA (Reference Vector Algorithm from abstracts) with 50-dimensional vector representation reports best result in terms of F1-score.</span></p><div><p id="p0096">We present our experimental results on Krapivin2009 dataset in <a name="btbl0008" href="#tbl0008" class="workspace-trigger">Table&nbsp;8</a>. We observe that RVA performs best for this dataset in terms of F1-score. This shows the effectiveness of word embeddings in determining keyphrases. Blank entries (‘-’) in the table mean unavailability of results in relevant literature.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0008"><span class="captions"><span id="cap0013"><p id="sp0018"><span class="label">Table 8</span>. Performance evaluation for Keyphrase Extraction on Krapivin2009 dataset. @<em>k</em>: evaluation results for top-<em>k</em> keyphrases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Model</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB@5</td><td class="align-left valign-top">28.1</td><td class="align-left valign-top">29.8</td><td class="align-left valign-top">27.7</td></tr><tr><td class="align-left valign-top">NB-A@5</td><td class="align-left valign-top">27.2</td><td class="align-left valign-top">28.6</td><td class="align-left valign-top">26.7</td></tr><tr><td class="align-left valign-top">RVA* (<a name="bbib0030" href="#bib0030" class="workspace-trigger">Papagiannopoulou &amp; Tsoumakas,&nbsp;2018</a>)</td><td class="align-left valign-top">–</td><td class="align-left valign-top">–</td><td class="align-left valign-top"><strong>32.06</strong></td></tr></tbody></table></div><p class="legend"><p id="sp0019">* The algorithm is evaluated for top one-third keyphrases.</p></p></div></div><p id="p0097"><span>However, it is noteworthy that the evaluation of the baseline and our models is not same. The baseline is evaluated for top one-third keyphrases, whereas our models are evaluated for top-5 predicted keyphrases. This makes it difficult for us to perform an unbiased comparison of the methods. Among XGB and <a href="/topics/computer-science/naive-bayes-model" title="Learn more about NB models from ScienceDirect's AI-generated Topic Pages" class="topic-link">NB models</a>, XGB performs best when we extract top-5 keyphrases. The number of keywords extracted (i.e., 5 in this case) correlates with the average number of keyphrases per document for Krapivin2009 dataset (</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>K</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>a</mi><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>g</mi></mrow></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>5</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.922ex" height="2.581ex" viewBox="0 -740.1 3841.5 1111.1" role="img" focusable="false" style="vertical-align: -0.862ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-4B"></use></g><g is="true" transform="translate(849,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-61"></use></g><g is="true" transform="translate(374,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(717,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-67"></use></g></g></g><g is="true" transform="translate(2284,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3341,0)"><use xlink:href="#MJMAIN-35"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">K</mi><mrow is="true"><mi is="true">a</mi><mi is="true">v</mi><mi is="true">g</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">5</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-19"><math><mrow is="true"><msub is="true"><mi is="true">K</mi><mrow is="true"><mi is="true">a</mi><mi is="true">v</mi><mi is="true">g</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">5</mn></mrow></math></script></span> in <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>).</p></section><section id="sec0027"><h4 id="sctt0030" class="u-margin-m-top u-margin-xs-bottom">8.3.4. Result on SemEval2010 dataset</h4><p id="p0098"><span>SemEval2010 dataset was curated for Task 5 of the Workshop for <a href="/topics/computer-science/evaluation-semantics" title="Learn more about Semantic Evaluation from ScienceDirect's AI-generated Topic Pages" class="topic-link">Semantic Evaluation</a>, 2010. 21 teams participated in the task, and HUMB (</span><a name="bbib0022" href="#bib0022" class="workspace-trigger">Lopez &amp; Romary,&nbsp;2010b</a>) performed best for author-and-reader-assigned keywords (<a name="bbib0018" href="#bib0018" class="workspace-trigger">Kim&nbsp;et&nbsp;al., 2010</a>).</p><p id="p0099">We compare our XGB and NB-A models with HUMB (<a name="bbib0022" href="#bib0022" class="workspace-trigger">Lopez &amp; Romary,&nbsp;2010b</a>) and Boudin’s algorithm (<a name="bbib0005" href="#bib0005" class="workspace-trigger">Boudin,&nbsp;2018</a>) as baselines. HUMB is a supervised method that identifies keyphrases using a predictive model trained on a feature set of document structure (e.g. section and position), content (e.g. tf-idf), and external information (GRISP terminology and Wikipedia). The model is initially trained using a bagged decision tree, and candidates are further re-ranked using a probabilistic model to improve their ranking (<a name="bbib0022" href="#bib0022" class="workspace-trigger">Lopez &amp; Romary,&nbsp;2010b</a>). Boudin’s algorithm is unsupervised, which uses a multipartite graph representation of the text to encode keyphrase candidates and topics in a single graph. Candidates are ranked using TextRank computation for weighted graphs.</p><div><p id="p0100"><a name="btbl0009" href="#tbl0009" class="workspace-trigger">Table&nbsp;9</a> presents the experimental results for the proposed models and the two baselines. We observe that XGB model outperforms all models in terms of precision, recall, and F1-score when we extract top-10 keyphrases. We also show the results of our models for top-15 keyphrases (<span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>K</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>a</mi><mi is=&quot;true&quot;>v</mi><mi is=&quot;true&quot;>g</mi></mrow></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>16</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.085ex" height="2.581ex" viewBox="0 -740.1 4342 1111.1" role="img" focusable="false" style="vertical-align: -0.862ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-4B"></use></g><g is="true" transform="translate(849,-150)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-61"></use></g><g is="true" transform="translate(374,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-76"></use></g><g is="true" transform="translate(717,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-67"></use></g></g></g><g is="true" transform="translate(2284,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3341,0)"><use xlink:href="#MJMAIN-31"></use><use xlink:href="#MJMAIN-36" x="500" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">K</mi><mrow is="true"><mi is="true">a</mi><mi is="true">v</mi><mi is="true">g</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">16</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-20"><math><mrow is="true"><msub is="true"><mi is="true">K</mi><mrow is="true"><mi is="true">a</mi><mi is="true">v</mi><mi is="true">g</mi></mrow></msub><mo linebreak="goodbreak" is="true">=</mo><mn is="true">16</mn></mrow></math></script></span> in <a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a> for SemEval2010 dataset). However, we only show results of one baseline, HUMB, as Boudin’s algorithm do not report results for top-15 keyphrases. The difference in performance of our models (i.e., XGB and NB) for top-10 and top-15 keyphrases is insignificant, with a slightly better performance for top-10 keyphrases.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0009"><span class="captions"><span id="cap0014"><p id="sp0020"><span class="label">Table 9</span>. Performance evaluation for Keyphrase Extraction on SemEval2010 dataset. @<em>k</em>: evaluation results for top-<em>k</em> keyphrases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Model</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB@10</td><td class="align-left valign-top"><strong>38.5</strong></td><td class="align-left valign-top"><strong>25.6</strong></td><td class="align-left valign-top"><strong>30.3</strong></td></tr><tr><td class="align-left valign-top">NB-A@10</td><td class="align-left valign-top">36</td><td class="align-left valign-top">24</td><td class="align-left valign-top">28.3</td></tr><tr><td class="align-left valign-top">HUMB@10 (<a name="bbib0022" href="#bib0022" class="workspace-trigger">Lopez &amp; Romary,&nbsp;2010b</a>)</td><td class="align-left valign-top">32.0</td><td class="align-left valign-top">21.8</td><td class="align-left valign-top">26.0</td></tr><tr><td class="align-left valign-top">Boudin@10 (<a name="bbib0005" href="#bib0005" class="workspace-trigger">Boudin,&nbsp;2018</a>)</td><td class="align-left valign-top">–</td><td class="align-left valign-top">–</td><td class="align-left valign-top">14.5</td></tr><tr><td class="align-left valign-top">XGB@15</td><td class="align-left valign-top">30</td><td class="align-left valign-top">29.9</td><td class="align-left valign-top">29.5</td></tr><tr><td class="align-left valign-top">NB-A@15</td><td class="align-left valign-top">28.6</td><td class="align-left valign-top">28.4</td><td class="align-left valign-top">28.1</td></tr><tr><td class="align-left valign-top">HUMB@15</td><td class="align-left valign-top">27.2</td><td class="align-left valign-top">27.8</td><td class="align-left valign-top">27.5</td></tr></tbody></table></div></div></div></section><section id="sec0028"><h4 id="sctt0031" class="u-margin-m-top u-margin-xs-bottom">8.3.5. Result on Marujo2012 dataset</h4><p id="p0101">Marujo2012 dataset (<a name="bbib0024" href="#bib0024" class="workspace-trigger">Marujo&nbsp;et&nbsp;al., 2012</a>) is a cross-domain dataset that we adopted to establish domain-independence of our proposed method. The dataset consists of news articles. To compare the performance of XGB and NB-A models, we consider as baseline Boudin’s algorithm (<a name="bbib0005" href="#bib0005" class="workspace-trigger">Boudin,&nbsp;2018</a>), which has already been briefed in <a name="bsec0027" href="#sec0027" class="workspace-trigger">Section&nbsp;8.3.4</a>.</p><div><p id="p0102">We present our experimental results in <a name="btbl0010" href="#tbl0010" class="workspace-trigger">Table&nbsp;10</a>. We observe that our models outperformed the baseline by a huge margin and shows impressive performance for a cross-domain keyphrase extraction model. Specifically, best precision is achieved when we extract top-10 keyphrases using NB-A model, and best recall and F1-score is achieved when we extract top-30 keyphrases using the XGB model. High precision and comparatively low recall is due to the high number of gold-standard keyphrases assigned for this dataset (<a name="btbl0001" href="#tbl0001" class="workspace-trigger">Table&nbsp;1</a>, column <em>K<sub>avg</sub></em>). Our models predicted lesser number of keyphrases as in the gold-standard list, out of which most are correctly extracted (high precision) but a few correct keyphrases are missed (low recall).</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0010"><span class="captions"><span id="cap0015"><p id="sp0021"><span class="label">Table 10</span>. Performance evaluation for Keyphrase Extraction on Marujo2010 dataset. @<em>k</em>: evaluation results for top-<em>k</em> keyphrases. *: Evaluated only for top-5 and top-10 keyphrases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Model</th><th scope="col" class="align-center valign-top">P</th><th scope="col" class="align-center valign-top">R</th><th scope="col" class="align-center valign-top">F1</th></tr></thead><tbody><tr><td class="align-left valign-top">XGB@30</td><td class="align-left valign-top">83.4</td><td class="align-left valign-top"><strong>43.1</strong></td><td class="align-left valign-top"><strong>53.8</strong></td></tr><tr><td class="align-left valign-top">NB-A@30</td><td class="align-left valign-top">80.81</td><td class="align-left valign-top">33.36</td><td class="align-left valign-top">44.64</td></tr><tr><td class="align-left valign-top">XGB@10</td><td class="align-left valign-top">92.86</td><td class="align-left valign-top">25.62</td><td class="align-left valign-top">38.33</td></tr><tr><td class="align-left valign-top">NB-A@10</td><td class="align-left valign-top"><strong>92.91</strong></td><td class="align-left valign-top">25.55</td><td class="align-left valign-top">38.21</td></tr><tr><td class="align-left valign-top">Boudin@10* (<a name="bbib0005" href="#bib0005" class="workspace-trigger">Boudin,&nbsp;2018</a>)</td><td class="align-left valign-top">–</td><td class="align-left valign-top">–</td><td class="align-left valign-top">18.2</td></tr></tbody></table></div></div></div></section><section id="sec0029"><h4 id="sctt0032" class="u-margin-m-top u-margin-xs-bottom">8.3.6. Statistical significance testing</h4><p id="p0103">Our next goal is to examine if the performance of our algorithm is (statistically) significantly better than that of the corresponding baselines for each dataset. Since we know only the macro-averaged metrics for the baselines, we can’t use traditional statistical significance testing approaches. Therefore, we follow the approach recommended by <a name="bbib0002" href="#bib0002" class="workspace-trigger">Berg-Kirkpatrick,&nbsp;Burkett, and Klein&nbsp;(2012)</a> and <a name="bbib0012" href="#bib0012" class="workspace-trigger">Dror,&nbsp;Baumer, Shlomov, and Reichart&nbsp;(2018)</a>.</p><p id="p0104">Let <em>O</em> be our algorithm and <em>B</em> be the baseline algorithm. We test the null hypothesis, H0: the performance of <em>O</em> is no better than the performance of <em>B</em>, against the alternative, H1: the performance of <em>O</em> is significantly better than <em>B</em>. We compare our method with the corresponding baselines for each dataset. The performance difference, <em>δ</em>(<em>x</em>), is the difference in performance metric of <em>O</em> minus <em>B</em> for the dataset <em>x</em>.</p><div><p id="p0105"><span>For each dataset, we generate one million <a href="/topics/computer-science/bootstrap-sample" title="Learn more about bootstrap samples from ScienceDirect's AI-generated Topic Pages" class="topic-link">bootstrap samples</a> from the document-level F1-score vectors for our algorithm.</span><a name="bfn0009" href="#fn0009" class="workspace-trigger"><sup>9</sup></a> Following the algorithm recommended by <a name="bbib0002" href="#bib0002" class="workspace-trigger">Berg-Kirkpatrick&nbsp;et&nbsp;al.&nbsp;(2012)</a> (<a name="bfig0004" href="#fig0004" class="workspace-trigger">Fig.&nbsp;4</a>), we estimate the p-value as the ratio of number of times our algorithm beats the baseline by twice the margin<a name="bfn0010" href="#fn0010" class="workspace-trigger"><sup>10</sup></a>(2<em>δ</em>(<em>x</em>)) on the bootstrap samples, to the total number of samples. For p-value  &lt; 0.05, we reject the null hypothesis.</p><figure class="figure text-xs" id="fig0004"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr4.jpg" height="150" alt="Fig. 4" aria-describedby="cap0004"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (171KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (171KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cap0004"><p id="sp0008"><span class="label">Fig. 4</span>. <span><a href="/topics/engineering/pseudocode" title="Learn more about Pseudocode from ScienceDirect's AI-generated Topic Pages" class="topic-link">Pseudocode</a> for estimating p-value (</span><a name="bbib0002" href="#bib0002" class="workspace-trigger">Berg-Kirkpatrick&nbsp;et&nbsp;al., 2012</a>).</p></span></span></figure><figure class="figure text-xs" id="fig0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr5.jpg" height="827" alt="Fig. 5" aria-describedby="cap0005"><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (774KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (774KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span id="cap0005"><p id="sp0009"><span class="label">Fig. 5</span>. Distribution of average F1-scores for <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>R</mi><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><msup is=&quot;true&quot;><mn is=&quot;true&quot;>10</mn><mn is=&quot;true&quot;>6</mn></msup></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.242ex" height="2.498ex" viewBox="0 -954.4 3548.5 1075.4" role="img" focusable="false" style="vertical-align: -0.281ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-52"></use></g><g is="true" transform="translate(1037,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2093,0)"><g is="true"><use xlink:href="#MJMAIN-31"></use><use xlink:href="#MJMAIN-30" x="500" y="0"></use></g><g is="true" transform="translate(1001,393)"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">R</mi><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mn is="true">10</mn><mn is="true">6</mn></msup></mrow></math></span></span><script type="math/mml" id="MathJax-Element-21"><math><mrow is="true"><mi is="true">R</mi><mo linebreak="goodbreak" is="true">=</mo><msup is="true"><mn is="true">10</mn><mn is="true">6</mn></msup></mrow></math></script></span><span> <a href="/topics/computer-science/bootstrap-sample" title="Learn more about bootstrap samples from ScienceDirect's AI-generated Topic Pages" class="topic-link">bootstrap samples</a><span> drawn from each dataset, along with corresponding quantiles of <a href="/topics/engineering/standard-normal-distribution" title="Learn more about standard normal distribution from ScienceDirect's AI-generated Topic Pages" class="topic-link">standard normal distribution</a>. </span></span><em>t</em>: Mean F1-score for bootstrap sample.</p></span></span></figure></div><div><p id="p0106">For all datasets except Krapivin2009, low p-value ( &lt; 0.05) led to rejection of H0. This is a strong evidence that the superior performance of the proposed method is not due to chance. As evident in <a name="btbl0008" href="#tbl0008" class="workspace-trigger">Table&nbsp;8</a>, performance of our method is weaker than the competing method for Krapivin2009 corpus. The same is confirmed by the statistical test. We show the distribution of F1-scores for one million bootstrap samples for each dataset (for XGB model) in <a name="bfig0005" href="#fig0005" class="workspace-trigger">Fig.&nbsp;5</a><span>. Each plot is paired with the corresponding quantiles of <a href="/topics/engineering/standard-normal-distribution" title="Learn more about standard normal distribution from ScienceDirect's AI-generated Topic Pages" class="topic-link">standard normal distribution</a>. Distribution of F1-scores is found to be good normal fit (</span><a name="bfig0005" href="#fig0005" class="workspace-trigger">Fig.&nbsp;5</a>(a)–(f)) for all datasets including Krapivin2009. The mean and standard deviation for each of these distributions is shown in <a name="btbl0011" href="#tbl0011" class="workspace-trigger">Table&nbsp;11</a>. Low standard deviation values establish consistency of the proposed method.</p><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0011"><span class="captions"><span id="cap0016"><p id="sp0022"><span class="label">Table 11</span>. Mean (<em>μ</em>) and standard deviation (<em>σ</em>) of distributions shown in <a name="bfig0005" href="#fig0005" class="workspace-trigger">Fig.&nbsp;5</a>.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top">Datasets</th><th scope="col" class="align-center valign-top">Mean</th><th scope="col" class="align-center valign-top">Standard deviation</th></tr></thead><tbody><tr><td class="align-left valign-top">Hulth2003</td><td class="align-center valign-top">0.547325</td><td class="align-center valign-top">0.007697</td></tr><tr><td class="align-left valign-top">WWW</td><td class="align-center valign-top">0.365530</td><td class="align-center valign-top">0.006432</td></tr><tr><td class="align-left valign-top">KDD</td><td class="align-center valign-top">0.332905</td><td class="align-center valign-top">0.008523</td></tr><tr><td class="align-left valign-top">SemEval2010</td><td class="align-center valign-top">0.303430</td><td class="align-center valign-top">0.008322</td></tr><tr><td class="align-left valign-top">Krapivin2009</td><td class="align-center valign-top">0.276609</td><td class="align-center valign-top">0.004156</td></tr><tr><td class="align-left valign-top">Marujo2012</td><td class="align-center valign-top">0.595079</td><td class="align-center valign-top">0.008096</td></tr></tbody></table></div></div></div><p id="p0107">At this point in time, we are unable to explain consistently low performance of our method on Krapivin2009 dataset for keyphrase extraction (lower by  ≈ 5%). Deeper investigation about the nature of Krapivin documents is pending for future.</p></section></section><section id="sec0030"><h3 id="sctt0033" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.4. Keyword extraction from Indian language documents</h3><p id="p0108">India is a country with 23 official languages, including English. According to Census of India of 2011, India has 121 major languages with more than 10,000 speakers for each language.<a name="bfn0011" href="#fn0011" class="workspace-trigger"><sup>11</sup></a> With such a wide variety of written and spoken languages, there is a huge collection of literature available in the country. Since digital texts are increasing day by day, automatic analysis of such documents needs to be addressed. However, due to unavailability of sophisticated NLP tools, documents written in Indian regional languages, which are grossly under-resourced, remain poorly analyzed.</p><p id="p0109">We demonstrate the language-agnostic character of the proposed method by using the XGB model trained on English language documents to predict keywords from text documents written in two Indian languages. We establish the effectiveness of the proposed method in two phases. In the first phase, we choose an English document and predict the keywords. We Google translate<a name="bfn0012" href="#fn0012" class="workspace-trigger"><sup>12</sup></a> the same document to Hindi and compare the keywords predicted from the translation with keywords predicted from the English document. We choose to translate an English document to Hindi over an article originally written in Hindi so that the quality of predicted Hindi keywords can be compared with the English gold-standard. In the second phase, we apply the same XGB model on five Assamese language documents. Below, we describe in detail the experiments and the observations.</p><div><p id="p0110">The sample English text is a randomly chosen document from Marujo2012 dataset (id “art_and_culture-20925876.txt”), which is translated to Hindi. We combine<a name="bfn0013" href="#fn0013" class="workspace-trigger"><sup>13</sup></a> two publicly available Hindi stopwords lists<a name="bfn0014" href="#fn0014" class="workspace-trigger"><sup>14</sup></a> to create an expanded stop-list. <a name="btbl0012" href="#tbl0012" class="workspace-trigger">Table&nbsp;12</a> presents a detailed analysis of the results. The columns correspond to results for Hindi and English text, respectively. First row of the table lists predicted keywords using the XGB model. Based on the English gold-standard keywords list, we highlight each recalled keyword in bold. For Hindi keywords, we highlight the words whose English translation is present in the gold-standard list. Next row presents English translation for every Hindi keyword predicted by the model. The ‘-’ in the translation denotes that the corresponding word is semantically a Hindi stopword but is not included in the stop-list. Third row lists twenty keywords that are predicted from both Hindi and English versions, with the translations given in parenthesis. Out of twenty-nine total predicted keywords (last row), twenty common keywords indicate fairly good performance of the model on the Hindi document although it was trained on English corpora. We are confident that human translated Hindi text of the English document will yield improved performance. We clarify here that the same number of Hindi and English keywords matching with the gold-standard is incidental.</p><div class="tables rowsep-0 colsep-0 frame-none" id="tbl0012"><span class="captions"><span id="cap0017"><p id="sp0023"><span class="label">Table 12</span>. Keywords predicted from the original English and translated Hindi text using the pre-trained XGB model. ‘-’ in translated keywords mean the corresponding Hindi word should be a stopword.</p></span></span><div class="groups"><table><tbody><tr><th class="align-left valign-top" scope="row"><figure class="inline-figure"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-fx1.jpg" height="362" alt="Image 1"></figure></th></tr></tbody></table></div></div></div><div><p id="p0111">In the second phase, we experiment to evaluate the model for Assamese<a name="bfn0015" href="#fn0015" class="workspace-trigger"><sup>15</sup></a> language texts. To perform this experiment, we collected five Assamese articles from Assamese Wikipedia.<a name="bfn0016" href="#fn0016" class="workspace-trigger"><sup>16</sup></a> The topics of the documents and the keywords predicted from each of the documents are shown in <a name="btbl0013" href="#tbl0013" class="workspace-trigger">Table&nbsp;13</a>. We are unable to objectively assess the performance of our method due to unavailability of gold-standard keywords for these documents. We provide English translation for the corresponding predicted keywords to enable rational assessment of the performance of our method. Keywords relevant to the topic are marked in bold. Last column shows the ratio of the relevant keywords to the total number of predicted keywords (barring “-”).</p><div class="tables rowsep-0 colsep-0 frame-none" id="tbl0013"><span class="captions"><span id="cap0018"><p id="sp0024"><span class="label">Table 13</span>. Keywords predicted from the Assamese documents using the pre-trained XGB model. ‘-’ in translated keywords mean the corresponding Assamese word is semantically a stopword. R/E: Number of relevant keywords/number of predicted keywords excluding ‘-’.</p></span></span><div class="groups"><table><tbody><tr><th class="align-left valign-top" scope="row"><figure class="inline-figure"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741741930586X-fx2.jpg" height="469" alt="Image 2"></figure></th></tr></tbody></table></div></div></div><p id="p0112">Some noise is evident in the predicted keywords (e.g. ‘help’ in <em>Animation</em>, ‘several’ in <em>Capitalism</em> and <em>Solar system</em>, ‘part in <em>Computer</em>, ‘see’ in <em>Movie</em>). Interestingly, the term ‘english’ occurs in 3/5 topics. This is because English translations of some words are preceded by the term ‘english’ in the Assamese text. Morphologically inflected words with different endings (translated with semantics/context in parenthesis) manifest as repetitions. For example, in <em>Animation</em>, words ‘image’, ‘picture’, ‘(of) picture’, indicate to an Assamese reader that <em>image</em> and <em>picture</em> are keywords.</p><p id="p0113">This substantiates our claim that the proposed method is applicable to any language outside the training corpus, and can perform reasonably well without using any linguistic tools. However, morphological idiosyncrasies of languages in general may have somewhat blunting effect on the potential of the proposed method. Introducing a <a href="/topics/computer-science/human-in-the-loop" title="Learn more about human in the loop from ScienceDirect's AI-generated Topic Pages" class="topic-link">human in the loop</a> can quickly resolve such issues to aid automatic indexing of documents in language specific digital libraries and repositories.</p></section></section><section id="sec0031"><h2 id="sctt0034" class="u-h3 u-margin-l-top u-margin-xs-bottom">9. Conclusion</h2><p id="p0114">We presented a supervised framework for automatic keyword extraction using graph-theoretic properties of words in text. The framework is domain-, collection, and language-independent. We explored six <a href="/topics/engineering/graph-node" title="Learn more about graph node from ScienceDirect's AI-generated Topic Pages" class="topic-link">graph node</a><span><span><span> properties to distinguish keywords from non-keywords - <a href="/topics/computer-science/degree-centrality" title="Learn more about degree centrality from ScienceDirect's AI-generated Topic Pages" class="topic-link">degree centrality</a> (strength of a node), </span><a href="/topics/computer-science/eigenvector-centrality" title="Learn more about eigenvector centrality from ScienceDirect's AI-generated Topic Pages" class="topic-link">eigenvector centrality</a>, PageRank, PositionRank, coreness, and </span><a href="/topics/computer-science/clustering-coefficient" title="Learn more about clustering coefficient from ScienceDirect's AI-generated Topic Pages" class="topic-link">clustering coefficient</a><span>. Using training set from a mixed collection of short and long scientific texts, we trained <a href="/topics/computer-science/classification-models" title="Learn more about classification models from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification models</a> on SMOTE-balanced training set using XGBoost, Naïve Bayes, and bagging and boosting ensembles of Naïve Bayes. The induced models are then tested on four unseen collections, out of which one is from a different domain. Experimental results show that XGBoost (XGB) outperforms others in terms of F1-score, while Adabbost ensemble of Naïve Bayes (NB-A) closely follows. We also empirically affirm that our approach is domain- and collection-independent. Furthermore, to validate the claim of language-independence, we evaluated our models on unseen Indian language texts (Hindi and Assamese). Experimental results for keyphrase extraction show that the proposed models (XGB and NB-A) are able to outperform established keyphrase extraction models for all datasets except Krapivin2009.</span></span></p><p id="p0115">Top-5 keyphrases extracted from this paper<a name="bfn0017" href="#fn0017" class="workspace-trigger"><sup>17</sup></a> using XGB model are - “supervised keyword extraction”, “complex network”, “extract node properties”, “graph-based node properties”, and “keyword extraction techniques”, which basically sums up the work presented here.</p><p id="p0116">In future, we plan to apply the proposed approach over documents written in various Indian languages. We also intend to make our model a benchmark for cross-lingual studies, on the basis of which future keyword extraction algorithms for Indian languages could be evaluated.</p></section><section id="sec0031a"><h2 id="sctt0034a" class="u-h3 u-margin-l-top u-margin-xs-bottom">CRediT authorship contribution statement</h2><p id="p0116a"><strong>Swagata Duari:</strong><span> Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, <a href="/topics/computer-science/data-curation" title="Learn more about Data curation from ScienceDirect's AI-generated Topic Pages" class="topic-link">Data curation</a>, Writing - original draft, Visualization. </span><strong>Vasudha Bhatnagar:</strong> Conceptualization, Methodology, Validation, Formal analysis, Investigation, Writing - review &amp; editing, Supervision.</p></section></div><section id="sec0032"><h2 id="sctt0035" class="u-h3 u-margin-l-top u-margin-xs-bottom">Declaration of Competing Interest</h2><p id="p0117">The authors certify that they have NO affiliations with or involvement in any organization or entity with any financial interest (such as honoraria; educational grants; participation in speakers’ bureaus; membership, employment, consultancies, stock ownership, or other equity interest; and expert testimony or patent-licensing arrangements), or non-financial interest (such as personal or professional relationships, affiliations, knowledge or beliefs) in the subject matter or materials discussed in this manuscript.</p></section></div>
