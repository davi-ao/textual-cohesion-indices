<div class="Body u-font-serif" id="body"><div><section id="s0005"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><p id="p0005">Language production and comprehension provide a window into the cognitive and neural architecture underlying complex information processing in the brain (<a name="bbb0205" href="#bb0205" class="workspace-trigger">Pinker, 2000</a><span>). They are high-level cognitive functions that reflect the state of numerous cognitive processes. The pattern and content of the communication can be traced back to individuals' cognitive abilities, knowledge, affective state and consequently their overall mental state. Disturbances in the domain of language, especially in speech, occur in a variety of psychiatric and neurological conditions, and their <a href="/topics/medicine-and-dentistry/neural-substrate" title="Learn more about neural substrates from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural substrates</a><span> are likely to be related to the <a href="/topics/medicine-and-dentistry/pathophysiology" title="Learn more about pathophysiology from ScienceDirect's AI-generated Topic Pages" class="topic-link">pathophysiology</a> of the disorder (</span></span><a name="bbb0055" href="#bb0055" class="workspace-trigger">DeLisi, 2001</a>), and hence are a fundamental aspect in diagnosis and assessing treatment responsiveness and prognosis (<a name="bbb0020" href="#bb0020" class="workspace-trigger">Andreasen and Grove, 1986</a>, <a name="bbb0015" href="#bb0015" class="workspace-trigger">Andreasen and Black, 2005</a>, <a name="bbb0170" href="#bb0170" class="workspace-trigger">McKenna and Oh, 2005</a>).</p><p id="p0010">Indexing language comprehension and production disturbances has been conducted using a variety of neuropsychological measures and tests (<a name="bbb0115" href="#bb0115" class="workspace-trigger">Hodges et al., 1992</a>, <a name="bbb0230" href="#bb0230" class="workspace-trigger">Tamlyn et al., 1992</a>, <a name="bbb0175" href="#bb0175" class="workspace-trigger">McKenna et al., 1994</a>). We focus on speech, which traditionally has been quantified for predictability and variability using a variety of manual (and labor-intensive) techniques, such as cloze analysis, type:token ratios, analysis of lexical and syntactic structure, and also discourse structure using cohesion analysis (for a review, <a name="bbb0155" href="#bb0155" class="workspace-trigger">Kuperberg, 2010</a>). There are a variety of fine-grained rating scales of the coherence of speech and communication, such as the Scale for the Assessment of Thought, Language and Communication (TLC; <a name="bbb0010" href="#bb0010" class="workspace-trigger">Andreasen, 1986</a>), the Communication Disturbances Index (<a name="bbb0060" href="#bb0060" class="workspace-trigger">Docherty, 2005</a>), and the Thought Disorder Index (TDI) (<a name="bbb0215" href="#bb0215" class="workspace-trigger">Solovay et al., 1987</a>, <a name="bbb0195" href="#bb0195" class="workspace-trigger">Niznikiewicz et al., 2002</a><span>), use of which requires extensive training but nonetheless remains open to variance across raters. In some sense these are probing “communication efficiency”, which can be assessed by a range of <a href="/topics/neuroscience/computational-linguistics" title="Learn more about computational linguistic from ScienceDirect's AI-generated Topic Pages" class="topic-link">computational linguistic</a> techniques (</span><a name="bbb0145" href="#bb0145" class="workspace-trigger">Jurafsky and Martin, 2000</a>). Indeed, such studies – using Latent Semantic Analysis (LSA) which models and matches discourse content (<a name="bbb0160" href="#bb0160" class="workspace-trigger">Landauer and Dumais, 1997</a>, <a name="bbb0165" href="#bb0165" class="workspace-trigger">Landauer et al., 2007</a><span>) – have demonstrated that it is possible to evaluate patients with <a href="/topics/neuroscience/dementia-praecox" title="Learn more about schizophrenia from ScienceDirect's AI-generated Topic Pages" class="topic-link">schizophrenia</a><span> based on open-ended <a href="/topics/medicine-and-dentistry/verbalization" title="Learn more about verbalizations from ScienceDirect's AI-generated Topic Pages" class="topic-link">verbalizations</a>. These automatically derived language scores have distinguished patients from controls accurately (and patients from other patients, and also from their family members), using both large discourse samples as well as responses consisting of only a few words (</span></span><a name="bbb0080" href="#bb0080" class="workspace-trigger">Elvevåg et al., 2007</a>, <a name="bbb0085" href="#bb0085" class="workspace-trigger">Elvevåg et al., 2010</a>).</p><p id="p0015">Our goal here is to present some tools derived from recent developments in network theory and information science that enable one to capture and index “meaning” in a quantifiable and biologically relevant manner. This is because there are statistical properties in expressed language that provide a rich source of information regarding “meaningful communication”. Specifically, we present measurements of disorganization of discourse based on topic randomness and semantic graph measures. Thus, in the next section we describe methods based on information science and complex network approaches to language, and we introduce a particular representation of discourse, and present ways to measure its disorganization. Then we apply our framework to speech samples from patients with schizophrenia and a healthy participant to illustrate the potential of the method.</p></section><section id="s0010"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">2. Methods</h2><section id="s0015"><h3 class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Semantic graphs and complex networks</h3><p id="p0020">Our method requires a simple but rich representation of meaning. One approach to achieve this is graph representation, with roots in semantic network theory (<a name="bbb0040" href="#bb0040" class="workspace-trigger">Collins and Quillian, 1969</a>, <a name="bbb0035" href="#bb0035" class="workspace-trigger">Collins and Loftus, 1975</a>, <a name="bbb0220" href="#bb0220" class="workspace-trigger">Steyvers and Tenenbaum, 2005</a><span>). Graphs are <a href="/topics/psychology/mathematical-object" title="Learn more about mathematical objects from ScienceDirect's AI-generated Topic Pages" class="topic-link">mathematical objects</a> consisting of sets of nodes and sets of edges connecting the nodes. Traditional semantic networks are “graphs” with labeled connections that instantiate different relationships between entities (e.g., “a robin is a bird” is represented by a particular type of link (the IS-A link) between the “robin” node and the “bird” node, or the HAS linking “a bird has feathers”, that together support the inference that “a robin has feathers” (</span><a name="bbb0210" href="#bb0210" class="workspace-trigger">Quillian, 1968</a>)). Semantic graphs (i.e., “stripped down” versions of semantic networks) can be used to capture associative and conceptual relationships by automatically analyzing large portions of text, usually linking together nodes that represent words that co-occur within a small range in a large corpus (graphs built this way are referred to here as “lexical graphs”; <a name="bbb0095" href="#bb0095" class="workspace-trigger">Ferrer i Cancho and Solé, 2001</a>, <a name="bbb0065" href="#bb0065" class="workspace-trigger">Dorogovtsev and Mendes, 2001</a>, <a name="bbb0220" href="#bb0220" class="workspace-trigger">Steyvers and Tenenbaum, 2005</a>).</p><p id="p0025">Recent developments in graph theory applied to the study of complex systems have shown that many natural and artificial complex networks show the small world and scale free properties (<a name="bbb0005" href="#bb0005" class="workspace-trigger">Albert and Barabási, 2002</a>). The former means that networks tend to have high clustering coefficients<a name="bfn0005" href="#fn0005" class="workspace-trigger"><sup>1</sup></a> while keeping low path lengths (<a name="bbb0250" href="#bb0250" class="workspace-trigger">Watts and Strogatz, 1998</a>), and the latter implies that link distribution is frequently characterized by a power law, enabling highly connected nodes to appear relatively often. These two characteristics confer interesting properties to networks, like fast transmission and failure tolerance (<a name="bbb0190" href="#bb0190" class="workspace-trigger">Motter et al., 2002</a>, <a name="bbb0100" href="#bb0100" class="workspace-trigger">Ferrer i Cancho et al., 2005</a>, <a name="bbb0220" href="#bb0220" class="workspace-trigger">Steyvers and Tenenbaum, 2005</a>). In order to represent meaning, we use graphs that turn out to satisfy some of these properties.</p><p id="p0030">As our goal here is to capture the thematic structure of a single instance of a linguistic expression (a relatively small sample of text, discourse or dialog) we will represent words as nodes in a semantic graph, and consider discourse as a trajectory in such a graph. “Goal directed” discourse would show an ordered and organized trajectory, whereas thought disordered discourse would appear as a disordered trajectory due to the disorganization of the semantic structure, or of the mechanism that searches through it. We propose that to measure this disorganization, semantic structures be represented using networks and characterized using measures inspired by information theory. However, in order to derive useful tools two methodological challenges have to be addressed: First, find a suitable representation of discourse, including a topic graph and a trajectory. Second, devise measures of disorganization, sensitive enough to detect subtle deviations.</p></section><section id="s0020"><h3 class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. The representation of discourse</h3><p id="p0035">To represent discourse we analyzed textual transcriptions of speech samples. The texts were subjectively evaluated by delimiting small blocks of text of just one theme or idea, and labeling each block in the text with a set of words representing that theme (<a name="bbb0025" href="#bb0025" class="workspace-trigger">Cabana, 2009</a>).</p><div><p id="p0040"><span>As a calibration procedure we analyzed the first two chapters of “A study in scarlet” by Arthur <a href="/topics/medicine-and-dentistry/quinapril" title="Learn more about Conan from ScienceDirect's AI-generated Topic Pages" class="topic-link">Conan</a> Doyle, as the descriptive nature of the text is devoid of complex metaphors or other literary devices that could complicate analysis. For this large text sample, we added the additional criteria that block size should be between two sentences and four paragraphs and that the selected theme be distinguishable from the previous and next blocks. We show this calibration example in </span><a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>. As the discourse advances, consecutive paragraphs share some labels, allowing the construction of a graph, whereby each label is a node and each pair of labels that co-occur in one block of text is linked by an edge (<a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>B). The resulting graph is the topic graph (<a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>C).</p><figure class="figure text-xs" id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr1.jpg" height="497" alt=""><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (1MB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (1MB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span><p id="sp0005"><span class="label">Fig.&nbsp;1</span>. <span>To illustrate our approach, we analyzed the first two chapters of “A study in scarlet” by Arthur <a href="/topics/medicine-and-dentistry/quinapril" title="Learn more about Conan from ScienceDirect's AI-generated Topic Pages" class="topic-link">Conan</a> Doyle, which features the first appearance of detective Sherlock Holmes (</span><a name="bbb0070" href="#bb0070" class="workspace-trigger">Conan Doyle, 2005</a>). The first paragraphs and an illustration of the labeling process are shown. A) Four thematic blocks with labels are delimited. B) The resulting graph after assigning a node to each label, and linking labels that occur together in a block assignment, called topic graph. C) The resulting topic graph of the two chapters. D) The discourse trajectory is drawn over the topic graph.</p></span></span></figure></div><div><p id="p0045">When the topic graph is displayed bi-dimensionally, the discourse trajectory can be represented as a line drawn over each block in the order of their appearance in the discourse (<a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>B, D). If this trajectory is drawn over the whole topic graph, the line appears convoluted and folded, as a result of the text “re-visiting” central topics of the story (<a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>D). These “long range interactions” are what we seek to capture by measuring the entropies (see <a name="bs0025" href="#s0025" class="workspace-trigger">Section&nbsp;2.3</a>). The rationale for this <em>a priori</em> expectation is that sentence sequence in discourse is not random when language is organized (see <a name="bbb0150" href="#bb0150" class="workspace-trigger">Kintsch, 1988</a>, <a name="bbb0105" href="#bb0105" class="workspace-trigger">Foltz, 2007</a>), and loss of this higher-level order would result in disorganization. From visual inspection of the topic graph and the trajectory line, at least five major sets of labels can be identified that delimit five major topics in the graph (<a name="bt0005" href="#t0005" class="workspace-trigger">Table&nbsp;1</a>). In a larger graph, this delimitation could be automatically performed by identifying connected components (sets of connected nodes disconnected from others) or communities (sets of nodes statistically more connected to each other) as representing major topics (<a name="bbb0200" href="#bb0200" class="workspace-trigger">Palla et al., 2005</a>; see <a name="bs0075" href="#s0075" class="workspace-trigger">Section&nbsp;4</a>).</p><div class="tables frame-topbot colsep-0 rowsep-0" id="t0005"><span class="captions"><span><p id="sp0030"><span class="label">Table&nbsp;1</span>. Representative topic labels used to categorize each sentence of the text.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col" class="align-left">Topic 1</th><th scope="col" class="align-left">Topic 2</th><th scope="col" class="align-left">Topic 3</th><th scope="col" class="align-left">Topic 4</th><th scope="col" class="align-left">Topic 5</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">Afghanistan</td><td class="align-left">return</td><td class="align-left">crime</td><td class="align-left">personality</td><td class="align-left">mystery</td></tr><tr class="valign-top"><td class="align-left">war</td><td class="align-left">search</td><td class="align-left">cases</td><td class="align-left">strangeness</td><td class="align-left">deduce</td></tr><tr class="valign-top"><td class="align-left">wound</td><td class="align-left">lodgings</td><td class="align-left">blood</td><td class="align-left">methods</td><td class="align-left">evidence</td></tr><tr class="valign-top"><td class="align-left">sickness</td><td class="align-left">move</td><td class="align-left">laboratory</td><td class="align-left">occupation</td><td class="align-left">analysis</td></tr><tr class="valign-top"><td class="align-left">suffering</td><td class="align-left">coexistence</td><td class="align-left">substances</td><td class="align-left">knowledge</td><td class="align-left">detective</td></tr></tbody></table></div></div></div></section><section id="s0025"><h3 class="u-h4 u-margin-m-top u-margin-xs-bottom">2.3. Measures of disorganization</h3><p id="p0050">The central hypothesis of this work is that loss of goal, tangentiality and incoherence frequently observed in <a href="/topics/neuroscience/dementia-praecox" title="Learn more about schizophrenia from ScienceDirect's AI-generated Topic Pages" class="topic-link">schizophrenia</a> are based in part on problems “following” an ordered trajectory among different topics. In coherent discourse adjacent words refer to connected topics. In contrast, in incoherent discourse a certain degree of “shuffling” of the topics occurs such that adjacent words may belong to different topics. This does not imply a “word salad” as the discourse may respect syntax, word order and even word similarity, but nevertheless reveal a high degree of disorder (i.e., semantic shuffling) in terms of meaning. To detect and measure this disorganization, we developed topic and transition entropy measures (closely related to that used in statistical mechanics).</p><section id="s0030"><h4 class="u-margin-m-top u-margin-xs-bottom">2.3.1. Topic entropy</h4><div><p id="p0055">Topic entropy S(α), is defined as<span class="display"><span id="fo0005" class="formula"><span class="label">(1)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo><mo is=&quot;true&quot;>=</mo><mo is=&quot;true&quot;>&amp;#x2212;</mo><munderover is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>1</mn></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></munderover><mi is=&quot;true&quot;>p</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mi is=&quot;true&quot;>i</mi></msub><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo><mi is=&quot;true&quot;>l</mi><mi is=&quot;true&quot;>o</mi><mi is=&quot;true&quot;>g</mi><mi is=&quot;true&quot;>p</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mi is=&quot;true&quot;>i</mi></msub><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="32.848ex" height="4.549ex" viewBox="0 -1217.1 14142.9 1958.7" role="img" focusable="false" style="vertical-align: -1.722ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-53"></use></g><use xlink:href="#MJSZ2-28" is="true" x="645" y="-1"></use><g is="true" transform="translate(1243,0)"><use xlink:href="#MJMATHI-3B1"></use></g><use xlink:href="#MJSZ2-29" is="true" x="1883" y="-1"></use><g is="true" transform="translate(2758,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3815,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(4760,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,521)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(424,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(700,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(1152,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(1056,-308)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(794,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(7511,0)"><use xlink:href="#MJMATHI-70"></use></g><use xlink:href="#MJSZ2-28" is="true" x="8015" y="-1"></use><g is="true" transform="translate(8612,0)"><g is="true"><use xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(640,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><use xlink:href="#MJSZ2-29" is="true" x="9597" y="-1"></use><g is="true" transform="translate(10195,0)"><use xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(10493,0)"><use xlink:href="#MJMATHI-6F"></use></g><g is="true" transform="translate(10979,0)"><use xlink:href="#MJMATHI-67"></use></g><g is="true" transform="translate(11459,0)"><use xlink:href="#MJMATHI-70"></use></g><use xlink:href="#MJSZ2-28" is="true" x="11963" y="-1"></use><g is="true" transform="translate(12560,0)"><g is="true"><use xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(640,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><use xlink:href="#MJSZ2-29" is="true" x="13545" y="-1"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">S</mi><mo stretchy="true" is="true">(</mo><mi is="true">α</mi><mo stretchy="true" is="true">)</mo><mo is="true">=</mo><mo is="true">−</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi><mo stretchy="true" is="true">(</mo><mi is="true">α</mi><mo stretchy="true" is="true">)</mo></mrow></munderover><mi is="true">p</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">α</mi><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><mi is="true">p</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">α</mi><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-6"><math><mrow is="true"><mi is="true">S</mi><mo stretchy="true" is="true">(</mo><mi is="true">α</mi><mo stretchy="true" is="true">)</mo><mo is="true">=</mo><mo is="true">−</mo><munderover is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi><mo stretchy="true" is="true">(</mo><mi is="true">α</mi><mo stretchy="true" is="true">)</mo></mrow></munderover><mi is="true">p</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">α</mi><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><mi is="true">p</mi><mo stretchy="true" is="true">(</mo><msub is="true"><mi is="true">α</mi><mi is="true">i</mi></msub><mo stretchy="true" is="true">)</mo></mrow></math></script></span></span></span><em>α</em> being a particular topic in the text, <em>n</em>(<em>α</em>) the number of continuous stretches of text attributable to this topic and <em>p</em>(<em>α</em><sub><em>i</em></sub>) the ratio of length of the stretch <em>i</em> to total number of words attributable to this topic. This Eq.&nbsp;<a name="bfo0005" href="#fo0005" class="workspace-trigger">(1)</a> measures the level of discontinuity of words belonging to the same topic. When discourse is organized in a perfect sequence of topics each consisting of an uninterrupted stretch of text, all topic entropies will be zero, and will grow in those cases where topics are more interspersed. To illustrate, we shuffled the text of our example, “A study in scarlet”, and present a visual representation of this shuffling (<a name="bf0010" href="#f0010" class="workspace-trigger">Fig.&nbsp;2</a>).</p><figure class="figure text-xs" id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr2.jpg" height="214" alt=""><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (170KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (170KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span><p id="sp0010"><span class="label">Fig.&nbsp;2</span>. A) Schematic diagram of the shuffling procedure. Two “cutting points” are randomly assigned in the text, and then the remaining three portions of text are randomly permuted. B) Visualization of the effectiveness of the shuffling procedure on the topic assignment of the first two chapters of “A study in scarlet”, based on the 5 topics identified in the thematic graph (<a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>D). Note how the mixture develops as the shuffling is iterated.</p></span></span></figure></div><div><p id="p0060"><a name="bf0015" href="#f0015" class="workspace-trigger">Fig.&nbsp;3</a>A shows how the entropy of each of the topics increases as a result of the shuffling. The first instances of shuffling disrupt the original order only moderately, and a great deal of shuffling has to be imposed to result in some “randomness”, but the greatest increases in entropy occur in the first shuffling instances. Given that every topic's entropy increases, but to different degrees, a good measure of disorganization is mean entropy across topics, estimated as<span class="display"><span id="fo0010" class="formula"><span class="label">(2)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>D</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>&amp;#xAF;</mo></mover><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mn is=&quot;true&quot;>1</mn><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>T</mi></msub></mfrac><munderover is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mi is=&quot;true&quot;>&amp;#x3BA;</mi><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>T</mi></msub></mrow></munderover><mi is=&quot;true&quot;>S</mi><mfenced open=&quot;(&quot; close=&quot;)&quot; is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BA;</mi></mfenced></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.042ex" height="3.813ex" viewBox="0 -1058.6 9490.1 1641.7" role="img" focusable="false" style="vertical-align: -1.354ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true" transform="translate(35,0)"><g is="true"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(645,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(1035,0)"><use xlink:href="#MJMATHI-44"></use></g><g is="true" transform="translate(1863,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(0,287)"><use xlink:href="#MJMAIN-AF" x="-70" y="0"></use><g transform="translate(66.25069252077566,0) scale(4.240997229916897,1)"><use xlink:href="#MJMAIN-AF"></use></g><use xlink:href="#MJMAIN-AF" x="1822" y="0"></use></g></g><g is="true" transform="translate(2600,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3379,0)"><g transform="translate(397,0)"><rect stroke="none" width="1111" height="60" x="0" y="220"></rect><g is="true" transform="translate(378,409)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g><g is="true" transform="translate(60,-393)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-54"></use></g></g></g></g><g is="true" transform="translate(5174,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,477)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-54"></use></g></g></g><g is="true" transform="translate(1056,-287)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3BA"></use></g></g><g is="true" transform="translate(7489,0)"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(8134,0)"><use xlink:href="#MJMAIN-28" x="0" y="0"></use><g is="true" transform="translate(389,0)"><use xlink:href="#MJMATHI-3BA"></use></g><use xlink:href="#MJMAIN-29" x="966" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">S</mi><mo stretchy="true" is="true">(</mo><mi is="true">D</mi><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">¯</mo></mover><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mfrac><munderover is="true"><mo is="true">∑</mo><mi is="true">κ</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mrow></munderover><mi is="true">S</mi><mfenced open="(" close=")" is="true"><mi is="true">κ</mi></mfenced></mrow></math></span></span><script type="math/mml" id="MathJax-Element-7"><math><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">S</mi><mo stretchy="true" is="true">(</mo><mi is="true">D</mi><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">¯</mo></mover><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mfrac><munderover is="true"><mo is="true">∑</mo><mi is="true">κ</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mrow></munderover><mi is="true">S</mi><mfenced open="(" close=")" is="true"><mi is="true">κ</mi></mfenced></mrow></math></script></span></span></span>where <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>D</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>&amp;#xAF;</mo></mover></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.395ex" height="2.954ex" viewBox="0 -952.9 2323 1271.9" role="img" focusable="false" style="vertical-align: -0.741ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true" transform="translate(35,0)"><g is="true"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(645,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(1035,0)"><use xlink:href="#MJMATHI-44"></use></g><g is="true" transform="translate(1863,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(0,287)"><use xlink:href="#MJMAIN-AF" x="-70" y="0"></use><g transform="translate(66.25069252077566,0) scale(4.240997229916897,1)"><use xlink:href="#MJMAIN-AF"></use></g><use xlink:href="#MJMAIN-AF" x="1822" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true" is="true"><mrow is="true"><mi is="true">S</mi><mo stretchy="true" is="true">(</mo><mi is="true">D</mi><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">¯</mo></mover></math></span></span><script type="math/mml" id="MathJax-Element-8"><math><mover accent="true" is="true"><mrow is="true"><mi is="true">S</mi><mo stretchy="true" is="true">(</mo><mi is="true">D</mi><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">¯</mo></mover></math></script></span> refers to the entropy of the discourse and <em>N</em><sub><em>T</em></sub> is the number of topics that are expressed.</p><figure class="figure text-xs" id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr3.jpg" height="292" alt=""><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (513KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (513KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span><p id="sp0015"><span class="label">Fig.&nbsp;3</span>. Topic and transition entropy increase when the text of the first two paragraphs of “A study in scarlet” is shuffled. A) The topic entropies, calculated using Eq.&nbsp;<a name="bfo0005" href="#fo0005" class="workspace-trigger">(1)</a>. B) Transition entropy of each topic, according to Eq.&nbsp;<a name="bfo0015" href="#fo0015" class="workspace-trigger">(3)</a>.</p></span></span></figure></div></section><section id="s0035"><h4 class="u-margin-m-top u-margin-xs-bottom">2.3.2. Transition entropy</h4><p id="p0065">Transition entropy, is defined as<span class="display"><span id="fo0015" class="formula"><span class="label">(3)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>T</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo><mo is=&quot;true&quot;>=</mo><mo is=&quot;true&quot;>&amp;#x2212;</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C4;</mi><mo is=&quot;true&quot;>&amp;#x2260;</mo><mi is=&quot;true&quot;>&amp;#x3B1;</mi></mrow></munder><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>&amp;#x3B1;</mi></msub><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>&amp;#x3C4;</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo><mi is=&quot;true&quot;>l</mi><mi is=&quot;true&quot;>o</mi><mi is=&quot;true&quot;>g</mi><msub is=&quot;true&quot;><mi is=&quot;true&quot;>p</mi><mi is=&quot;true&quot;>&amp;#x3B1;</mi></msub><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>&amp;#x3C4;</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="33.246ex" height="4.549ex" viewBox="0 -1217.1 14314 1958.7" role="img" focusable="false" style="vertical-align: -1.722ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-54"></use></g><use xlink:href="#MJSZ2-28" is="true" x="704" y="-1"></use><g is="true" transform="translate(1302,0)"><use xlink:href="#MJMATHI-3B1"></use></g><use xlink:href="#MJSZ2-29" is="true" x="1942" y="-1"></use><g is="true" transform="translate(2817,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3874,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(4819,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-3C4"></use></g><g is="true" transform="translate(365,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2260"></use></g><g is="true" transform="translate(916,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3B1"></use></g></g></g><g is="true" transform="translate(7511,0)"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3B1"></use></g></g><use xlink:href="#MJSZ2-28" is="true" x="8568" y="-1"></use><g is="true" transform="translate(9165,0)"><use xlink:href="#MJMATHI-3C4"></use></g><use xlink:href="#MJSZ2-29" is="true" x="9683" y="-1"></use><g is="true" transform="translate(10280,0)"><use xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(10579,0)"><use xlink:href="#MJMATHI-6F"></use></g><g is="true" transform="translate(11064,0)"><use xlink:href="#MJMATHI-67"></use></g><g is="true" transform="translate(11545,0)"><g is="true"><use xlink:href="#MJMATHI-70"></use></g><g is="true" transform="translate(503,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3B1"></use></g></g><use xlink:href="#MJSZ2-28" is="true" x="12601" y="-1"></use><g is="true" transform="translate(13199,0)"><use xlink:href="#MJMATHI-3C4"></use></g><use xlink:href="#MJSZ2-29" is="true" x="13716" y="-1"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">T</mi><mo stretchy="true" is="true">(</mo><mi is="true">α</mi><mo stretchy="true" is="true">)</mo><mo is="true">=</mo><mo is="true">−</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">τ</mi><mo is="true">≠</mo><mi is="true">α</mi></mrow></munder><msub is="true"><mi is="true">p</mi><mi is="true">α</mi></msub><mo stretchy="true" is="true">(</mo><mi is="true">τ</mi><mo stretchy="true" is="true">)</mo><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><msub is="true"><mi is="true">p</mi><mi is="true">α</mi></msub><mo stretchy="true" is="true">(</mo><mi is="true">τ</mi><mo stretchy="true" is="true">)</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-9"><math><mrow is="true"><mi is="true">T</mi><mo stretchy="true" is="true">(</mo><mi is="true">α</mi><mo stretchy="true" is="true">)</mo><mo is="true">=</mo><mo is="true">−</mo><munder is="true"><mo is="true">∑</mo><mrow is="true"><mi is="true">τ</mi><mo is="true">≠</mo><mi is="true">α</mi></mrow></munder><msub is="true"><mi is="true">p</mi><mi is="true">α</mi></msub><mo stretchy="true" is="true">(</mo><mi is="true">τ</mi><mo stretchy="true" is="true">)</mo><mi is="true">l</mi><mi is="true">o</mi><mi is="true">g</mi><msub is="true"><mi is="true">p</mi><mi is="true">α</mi></msub><mo stretchy="true" is="true">(</mo><mi is="true">τ</mi><mo stretchy="true" is="true">)</mo></mrow></math></script></span></span></span><em>p</em><sub><em>α</em></sub>(<em>τ</em>) is the fraction of transitions from topic <em>α</em> to topic <em>τ</em>. Discourse can have a large topic entropy (calculated with Eq.&nbsp;<a name="bfo0005" href="#fo0005" class="workspace-trigger">(1)</a>) but zero transition entropy (calculated with Eq.&nbsp;<a name="bfo0015" href="#fo0015" class="workspace-trigger">(3)</a>). If the discourse were perfectly periodic (e.g., a repetition of sequence <em>αβγαβγ</em>…), then transition frequencies would be <em>p</em><sub><em>α</em></sub>(<em>β</em>)&nbsp;=&nbsp;1 and <em>p</em><sub><em>α</em></sub>(<em>γ</em>)&nbsp;=&nbsp;0, and the entropy defined by Eq.&nbsp;<a name="bfo0015" href="#fo0015" class="workspace-trigger">(3)</a> would be equal to 0. With reference to <a name="bf0015" href="#f0015" class="workspace-trigger">Fig.&nbsp;3</a>B illustrating transition entropy for the original and shuffled versions of “A study in scarlet”, the increase in transition entropy is apparent, and it is more subtle than with topic entropy. This is probably because in the original text the entropy is already high since topics are relatively independent, or because the small sample of transitions cannot be used to detect inter-topic structure. As with topic entropy, mean transition entropy can be defined as:<span class="display"><span id="fo0020" class="formula"><span class="label">(4)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mover accent=&quot;true&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>T</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>D</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>&amp;#xAF;</mo></mover><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mn is=&quot;true&quot;>1</mn><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>T</mi></msub></mfrac><munderover is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mi is=&quot;true&quot;>&amp;#x3BA;</mi><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>N</mi><mi is=&quot;true&quot;>T</mi></msub></mrow></munderover><mi is=&quot;true&quot;>T</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>&amp;#x3BA;</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.282ex" height="4.549ex" viewBox="0 -1217.1 10024.1 1958.7" role="img" focusable="false" style="vertical-align: -1.722ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true" transform="translate(35,0)"><g is="true"><use xlink:href="#MJMATHI-54"></use></g><g is="true" transform="translate(704,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(1094,0)"><use xlink:href="#MJMATHI-44"></use></g><g is="true" transform="translate(1922,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(0,287)"><use xlink:href="#MJMAIN-AF" x="-70" y="0"></use><g transform="translate(54.89196675900274,0) scale(4.404432132963989,1)"><use xlink:href="#MJMAIN-AF"></use></g><use xlink:href="#MJMAIN-AF" x="1881" y="0"></use></g></g><g is="true" transform="translate(2659,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3438,0)"><g transform="translate(397,0)"><rect stroke="none" width="1111" height="60" x="0" y="220"></rect><g is="true" transform="translate(378,409)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g><g is="true" transform="translate(60,-393)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-54"></use></g></g></g></g><g is="true" transform="translate(5233,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,477)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(568,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-54"></use></g></g></g><g is="true" transform="translate(1056,-287)"><use transform="scale(0.707)" xlink:href="#MJMATHI-3BA"></use></g></g><g is="true" transform="translate(7548,0)"><use xlink:href="#MJMATHI-54"></use></g><use xlink:href="#MJSZ2-28" is="true" x="8252" y="-1"></use><g is="true" transform="translate(8850,0)"><use xlink:href="#MJMATHI-3BA"></use></g><use xlink:href="#MJSZ2-29" is="true" x="9426" y="-1"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">T</mi><mo stretchy="true" is="true">(</mo><mi is="true">D</mi><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">¯</mo></mover><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mfrac><munderover is="true"><mo is="true">∑</mo><mi is="true">κ</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mrow></munderover><mi is="true">T</mi><mo stretchy="true" is="true">(</mo><mi is="true">κ</mi><mo stretchy="true" is="true">)</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-10"><math><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">T</mi><mo stretchy="true" is="true">(</mo><mi is="true">D</mi><mo stretchy="true" is="true">)</mo></mrow><mo stretchy="true" is="true">¯</mo></mover><mo is="true">=</mo><mfrac is="true"><mn is="true">1</mn><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mfrac><munderover is="true"><mo is="true">∑</mo><mi is="true">κ</mi><mrow is="true"><msub is="true"><mi is="true">N</mi><mi is="true">T</mi></msub></mrow></munderover><mi is="true">T</mi><mo stretchy="true" is="true">(</mo><mi is="true">κ</mi><mo stretchy="true" is="true">)</mo></mrow></math></script></span></span></span>where the sum is performed over all topics.</p><p id="p0070">It remains to be established whether subtle disorganization in semantic structure of discourse can be detected reliably and reproducibly using this approach. We present below examples of its potential usefulness.</p></section></section></section><section id="s0040"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">3. The topology of speech in schizophrenia</h2><p id="p0075">Clearly our method would benefit from further refinements, but we nonetheless illustrate the potential usefulness of the whole approach and demonstrate its “proof of concept”.</p><p id="p0080"><span>One important difference between short speech transcriptions and the example we used to calibrate the procedure concerns size; to evaluate the effect of size we analyzed a paragraph of “A study in scarlet” (Text example), and compared it with a somewhat incoherent speech sample from a patient with <a href="/topics/neuroscience/dementia-praecox" title="Learn more about schizophrenia from ScienceDirect's AI-generated Topic Pages" class="topic-link">schizophrenia</a> (Sample 1). We selected the text example which has evident metaphorical character in stark contrast to Sample 1. In order to establish an even better comparison and analyze further, we examined speech samples generated in response to the question “What activities do people generally do during the course of the day?”, from a healthy participant (Sample 2), and three patients with schizophrenia (Samples 3 to 5) (from </span><a name="bbb0080" href="#bb0080" class="workspace-trigger">Elvevåg et al., 2007</a>). The responses were rated by two human raters for coherence (a score of 1&nbsp;=&nbsp;very coherent versus 7&nbsp;=&nbsp;very incoherent) and tangentiality (a score of 1&nbsp;=&nbsp;very incisively related to question versus 7&nbsp;=&nbsp;completely unrelated to question).</p><p id="p0085">For all samples we built lexical graphs (see below) and calculated the topological graph parameters (see <a name="bs0015" href="#s0015" class="workspace-trigger">Section&nbsp;2.1</a>). We also built topic graphs and calculated the topic and transition entropies (topological graph parameters were not estimated for these graphs because of their small size). Stuttering and repetitions were omitted from speech transcriptions.</p><div><p id="p0135">We constructed lexical graphs by linking together words co-occurring in a text at a distance of three words (<a name="bbb0090" href="#bb0090" class="workspace-trigger">Ferrer i Cancho, 2005</a>), but removing all function words (e.g., articles, prepositions). We calculated the main graph parameters (clustering coefficient, characteristic path length) of the lexical graphs (see <a name="bt0010" href="#t0010" class="workspace-trigger">Table&nbsp;2</a>). Notice that all samples show similar measures, but the smallness of the graphs precludes us from concluding anything further. As discussed below, future studies using this approach should employ bigger speech samples. Next we focus on the topic graphs.</p><div class="tables frame-topbot colsep-0 rowsep-0" id="t0010"><span class="captions"><span><p id="sp0035"><span class="label">Table&nbsp;2</span>. Characteristics of lexical graphs obtained from the samples.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col" class="align-left">Network</th><th scope="col" class="align-left">Clustering coefficient</th><th scope="col" class="align-left">Path length</th><th scope="col" class="align-left">No. of nodes</th><th scope="col" class="align-left">Links</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">Text example (Holmes)</td><td class="align-char">0.55</td><td class="align-char">3.63</td><td class="align-left">49</td><td class="align-char">109</td></tr><tr class="valign-top"><td class="align-left">Sample 1 (Patient)</td><td class="align-char">0.51</td><td class="align-char">4.04</td><td class="align-left">45</td><td class="align-char">95</td></tr><tr class="valign-top"><td class="align-left">Sample 2 (Control)</td><td class="align-char">0.54</td><td class="align-char">4.39</td><td class="align-left">56</td><td class="align-char">121</td></tr><tr class="valign-top"><td class="align-left">Sample 3 (Patient)</td><td class="align-char">0.50</td><td class="align-char">3.02</td><td class="align-left">49</td><td class="align-char">124</td></tr><tr class="valign-top"><td class="align-left">Sample 4 (Patient)</td><td class="align-char">0.52</td><td class="align-char">3.34</td><td class="align-left">34</td><td class="align-char">72</td></tr><tr class="valign-top"><td class="align-left">Sample 5 (Patient)</td><td class="align-char">0.51</td><td class="align-char">4.41</td><td class="align-left">61</td><td class="align-char">128</td></tr></tbody></table></div></div></div><div><p id="p0140">To obtain the topic graph, we performed a manual labeling procedure (as in the topic graph of “A study in scarlet”), selecting blocks of about one sentence in length. Once the topic graph was built (<a name="bf0020" href="#f0020" class="workspace-trigger">Fig.&nbsp;4</a>), each connected component was assigned a different topic, enabling the calculation of the topic and transition entropies.</p><figure class="figure text-xs" id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr4.jpg" height="589" alt=""><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (951KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (951KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span><p id="sp0020"><span class="label">Fig.&nbsp;4</span>. Topic graphs obtained from manual label assignment for A) Sherlock Holmes' “speech” , B) a patient with <a href="/topics/neuroscience/dementia-praecox" title="Learn more about schizophrenia from ScienceDirect's AI-generated Topic Pages" class="topic-link">schizophrenia</a> (Sample 1), C) a healthy participant (Sample 2), D, E and F) patients with schizophrenia (Samples 3 to 5). Discourse trajectories are shown as lines over the graphs.</p></span></span></figure></div><div><p id="p0145">After calculating the entropies, we detected important differences between the patient samples and the controls of comparable length. With reference to <a name="bt0015" href="#t0015" class="workspace-trigger">Table&nbsp;3</a>, the patient<em>'</em>s discourse (Sample 1) results in higher topic and transition entropies than the text example (Holmes). Regarding the responses to the question “What activities do people generally do during the course of the day?” (Samples 2 to 5), it can be seen that the healthy participant's response (Sample 2) results in lower topic entropy than the patients' responses. Within the patients' responses, the one with the lowest coherence (Sample 5) has much higher topic entropy than the others. However, transition entropy was lowest in the healthy participant (Sample 2) and in one of the responses from a patient (Sample 4).</p><div class="tables frame-topbot colsep-0 rowsep-0" id="t0015"><span class="captions"><span><p id="sp0040"><span class="label">Table&nbsp;3</span>. Comparison of topic and transition entropy for the samples. In each case the sum of the entropy for each topic is computed. Normalizing by the number of topics we obtain the mean topic and transition entropy, displayed in columns 5 and 6 respectively. Entropies for the text example are zero because only one topic was detected.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"></th><th scope="col" class="align-left">No. of topics</th><th scope="col" class="align-left">Sum of topic entropy</th><th scope="col" class="align-left">Sum of transition entropy</th><th scope="col" class="align-left">Mean topic entropy</th><th scope="col" class="align-left">Mean transition entropy</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">Text (Holmes)</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><td class="align-left">Sample 1 (Patient)</td><td class="align-char">4</td><td class="align-char">1.94</td><td class="align-char">1.79</td><td class="align-char">0.38</td><td class="align-char">0.35</td></tr><tr class="valign-top"><td class="align-left">Sample 2 (Control)</td><td class="align-char">7</td><td class="align-char">0.30</td><td class="align-char">0.69</td><td class="align-char">0.043</td><td class="align-char">0.099</td></tr><tr class="valign-top"><td class="align-left">Sample 3 (Patient)</td><td class="align-char">10</td><td class="align-char">1.08</td><td class="align-char">1.10</td><td class="align-char">0.11</td><td class="align-char">0.11</td></tr><tr class="valign-top"><td class="align-left">Sample 4 (Patient)</td><td class="align-char">6</td><td class="align-char">0.67</td><td class="align-char">0.69</td><td class="align-char">0.11</td><td class="align-char">0.12</td></tr><tr class="valign-top"><td class="align-left">Sample 5 (Patient)</td><td class="align-char">8</td><td class="align-char">2.05</td><td class="align-char">1.39</td><td class="align-char">0.26</td><td class="align-char">0.17</td></tr></tbody></table></div></div></div><p id="p0150">Although preliminary, these results clearly demonstrate the possibility of applying this novel methodological framework to assay the nature of the disorder that is readily apparent in this discourse.</p></section><section id="s0075"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Prospects for an automated topic graph construction</h2><div><p id="p0155">One promising direction to automatically obtain the topic graphs is to employ multidimensional semantic spaces. In these, each concept is associated with a vector, a set of concepts is represented as a vector space, and semantic relatedness is gauged as the proximity of the corresponding vectors (e.g., Latent Semantic Analysis (LSA, <a name="bbb0050" href="#bb0050" class="workspace-trigger">Deerwester et al., 1990</a>), BEAGLE (<a name="bbb0140" href="#bb0140" class="workspace-trigger">Jones et al., 2006</a>)). Semantic spaces are usually built using information on how words co-occur with different frequencies in different contexts. If a large enough corpus (on the order of thousands of documents, each having hundreds of terms) is used, the resulting space can simulate human behavior on a variety of tasks (<a name="bbb0160" href="#bb0160" class="workspace-trigger">Landauer and Dumais, 1997</a>, <a name="bbb0140" href="#bb0140" class="workspace-trigger">Jones et al., 2006</a>). In order to devise an illustrative automatic procedure, we built an LSA space using 53,956 documents and 56,108 terms obtained from Wikipedia<a name="bfn0010" href="#fn0010" class="workspace-trigger"><sup>2</sup></a>, applying standard methods (<a name="bbb0165" href="#bb0165" class="workspace-trigger">Landauer et al., 2007</a>). The 390-dimensional semantic space performed comparably well on the TOEFL synonym test (64.65%, versus the ‘gold-standard’ of 64.38% (<a name="bbb0160" href="#bb0160" class="workspace-trigger">Landauer and Dumais, 1997</a>))<a name="bfn0015" href="#fn0015" class="workspace-trigger"><sup>3</sup></a>. The automatic labeling procedure was as follows: First, we projected each paragraph of text onto the semantic space, generating paragraph vectors representing their semantic content. Since each of the terms used to build the word-document matrix can also be represented as a vector in that space, we selected the three terms that were closest to each paragraph and used them as “automatic labels”, to build a thematic graph (similar to <a name="bf0005" href="#f0005" class="workspace-trigger">Fig.&nbsp;1</a>C). We performed the dot product between every word vector and each paragraph vector to determine which word vectors were closest to each paragraph. A pre-selection of words was made by projecting “windows of words” of length 8, and selecting 3 labels for each. Then, when computing dot products for the whole paragraphs, the words were chosen from the previously obtained set of labels, not from the full 56,108 terms (<a name="bf0025" href="#f0025" class="workspace-trigger">Fig.&nbsp;5</a>).</p><figure class="figure text-xs" id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr5.jpg" height="350" alt=""><ol class="links-for-figure"><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (482KB)"><span class="anchor-text">Download : <span class="download-link-title">Download high-res image (482KB)</span></span></a></li><li><a class="anchor download-link u-font-sans" href="https://ars.els-cdn.com/content/image/1-s2.0-S0920996411002337-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text">Download : <span class="download-link-title">Download full-size image</span></span></a></li></ol></span><span class="captions"><span><p id="sp0025"><span class="label">Fig.&nbsp;5</span>. Topic graph obtained by applying the automatic procedure to the first two chapters of “A study in scarlet”. Although the resulting labeling is not optimal, this automatically generated graph has a central component and several topics that can readily be detected, enabling the computation of entropies. Topic entropy was 5.13 and transition entropy was 4.38, values that compare well with entropies generated via the manually labeled topic graph (4.84 and 4.76, respectively).</p></span></span></figure></div><p id="p0160">Although the semantic space method produced noisy labels (<a name="bf0025" href="#f0025" class="workspace-trigger">Fig.&nbsp;5</a>), the results are nonetheless encouraging at least when applied to large portions of text. We discuss the potential of this and other methods in <a name="bs0080" href="#s0080" class="workspace-trigger">Section&nbsp;5</a>.</p></section><section id="s0080"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">5. Discussion and future challenges</h2><p id="p0165"><span>Communication patterns change across the lifespan, and in illness (e.g., <a href="/topics/psychology/psychopathology" title="Learn more about psychopathology from ScienceDirect's AI-generated Topic Pages" class="topic-link">psychopathology</a> and dementia). The convergence of methods from theoretical physics, network theory (</span><a name="bbb0005" href="#bb0005" class="workspace-trigger">Albert and Barabási, 2002</a>), information sciences (<a name="bbb0050" href="#bb0050" class="workspace-trigger">Deerwester et al., 1990</a>, <a name="bbb0240" href="#bb0240" class="workspace-trigger">Valle-Lisboa and Mizraji, 2007</a>) and cognitive neuropsychiatry (<a name="bbb0110" href="#bb0110" class="workspace-trigger">Halligan and David, 2001</a><span>), presents an opportunity for new frameworks within which to study how humans communicate effectively, and how many <a href="/topics/medicine-and-dentistry/pathological-process" title="Learn more about pathological processes from ScienceDirect's AI-generated Topic Pages" class="topic-link">pathological processes</a> rob humanity of this most central aspect, namely communicating effectively and meaningfully.</span></p><p id="p0170"><span>The models and procedures we have presented may be valuable modeling tools to assay the underlying structure of discourse disorganization. We have presented a set of techniques to analyze text and demonstrate a “proof of concept” of our approach. We believe that if a good topic classification of the speech of patients is achieved, our representations and measures can be valuable tools with which to study <a href="/topics/neuroscience/dementia-praecox" title="Learn more about schizophrenia from ScienceDirect's AI-generated Topic Pages" class="topic-link">schizophrenia</a>. Although promising, the results thus far require subjective judgments to determine the topics a discourse “visits”. Also, the graphs and topic classification were generated manually, yet our goal is to devise an automatic method to generate the semantic graph and segment the graph in topics. Ideally our procedure should yield an automatic characterization of incoherent discourse, based on disorganization of the topic graph. Also, the availability of automatically generated large topic graphs would allow a reliable comparison of complex network parameters for normal and pathological samples. We introduced a promising albeit preliminary method based upon LSA (</span><a name="bs0075" href="#s0075" class="workspace-trigger">Section&nbsp;4</a>). Although LSA is a “bag-of-words” approach (as it ignores word-order and syntactic information) it is used in cognitive computational models (<a name="bbb0235" href="#bb0235" class="workspace-trigger">Utsumi, 2011</a>) and might have a biological basis (see <a name="bbb0185" href="#bb0185" class="workspace-trigger">Mizraji et al., 2009</a><span>). Alternatively, classification and labeling procedures could rely on <a href="/topics/psychology/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> models (</span><a name="bbb0045" href="#bb0045" class="workspace-trigger">Dayan and Abbott, 2001</a>, <a name="bbb0185" href="#bb0185" class="workspace-trigger">Mizraji et al., 2009</a>). Of note, <a name="bbb0120" href="#bb0120" class="workspace-trigger">Hoffman (1987)</a><span> provided an early neural network model to illustrate the putative differences between speech generated from patients with schizophrenia versus those with mania. This model was heuristic by providing a mechanism to understand and visualize specific characteristics of speech, such as perseverative speech versus the seemingly random and rapid associations in mania. Similarly, our work exploits recent developments in network theory and information sciences, as well as the vast computational power available today to construct models of coherent and incoherent discourse. These new technological advances additionally afford the modeling of real data (which is computationally intense), allowing the time-course of discourse to be examined, and the results displayed in a visually rich and informative manner. Moreover, these models open up the possibility of building better neural models of the <a href="/topics/medicine-and-dentistry/pathophysiology" title="Learn more about pathophysiology from ScienceDirect's AI-generated Topic Pages" class="topic-link">pathophysiology</a> of schizophrenia (</span><a name="bbb0030" href="#bb0030" class="workspace-trigger">Chen, 1994</a>, <a name="bbb0135" href="#bb0135" class="workspace-trigger">Hoffman et al., 1995</a>, <a name="bbb0125" href="#bb0125" class="workspace-trigger">Hoffman and McGlashan, 1997</a>, <a name="bbb0130" href="#bb0130" class="workspace-trigger">Hoffman and McGlashan, 1998</a>, <a name="bbb0225" href="#bb0225" class="workspace-trigger">Talamini et al., 2005</a>). Previously, we (<a name="bbb0245" href="#bb0245" class="workspace-trigger">Valle-Lisboa et al., 2005</a>) replicated the results of Hoffman, McGlashan and coworkers (<a name="bbb0135" href="#bb0135" class="workspace-trigger">Hoffman et al., 1995</a>, <a name="bbb0130" href="#bb0130" class="workspace-trigger">Hoffman and McGlashan, 1998</a><span>) concerning verbal <a href="/topics/psychology/hallucinations" title="Learn more about hallucinations from ScienceDirect's AI-generated Topic Pages" class="topic-link">hallucinations</a>, using different models of neural networks (</span><a name="bbb0180" href="#bb0180" class="workspace-trigger">Mizraji, 1989</a>). Our long term goal is to apply neural models to the production of incoherent discourse. If the measurements presented here can be applied generally, and the translation of these procedures to neural models can be achieved, the modeling of language production deviances on a large scale will be possible, and thus provide much needed insight into the neural and cognitive processes underlying speech production in schizophrenia.</p></section><section id="s0085"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">Role of funding source</h2><p id="p0175">EM and JCVL were supported by PEDECIBA and CSIC-UDELAR. AC was supported by PEDECIBA. BE was supported by the Northern Norwegian Regional Health Authority (Helse Nord RHF). None of the funding agencies (PEDECIBA, CSIC or Helse Nord RHF) had any further role in study design, data analysis and interpretation, or in the writing of the report and decision to submit for publication.</p></section><section id="s0090"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">Contributors</h2><p id="p0180">EM and JVL conceived the original ideas and theoretical framework. AC designed the procedures to create and analyze the lexical and topic graphs, corrected and improved the LSA-Wikipedia previously created by JVL and wrote all the programs for text and network analysis. BE motivated the clinical application of the theoretical framework and tools. All authors discussed the methods and results and contributed to the writing of the manuscript. All authors have read and approved the final manuscript.</p></section><section id="s0095"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">Conflict of interest</h2><p id="p0185">None of the authors have any potential conflicts of interest or biomedical financial interests.</p></section></div><section id="aep-acknowledgment-id13"><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">Acknowledgements</h2><p id="p0200">We thank Dr. Andrés Pomi for the valuable discussions and encouragement.</p></section></div>